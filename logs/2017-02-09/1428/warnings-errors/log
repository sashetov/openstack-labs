/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 07:43:45.498 14586 WARNING neutron.agent.securitygroups_rpc [-] Driver configuration doesn't match with enable_security_group
/root/openstack/logs/2017-02-09/1428/nova-novncproxy.log:2017-02-09 07:43:45.528 14581 WARNING oslo_reports.guru_meditation_report [-] Guru meditation now registers SIGUSR1 and SIGUSR2 by default for backward compatibility. SIGUSR1 will no longer be registered in a future release, so please use SIGUSR2 to generate reports.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 07:43:45.632 14608 WARNING stevedore.named [req-d074e04a-1d0e-41b9-9727-097849c76ac2 - - - - -] Could not load neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 07:43:45.634 14586 WARNING oslo_db.sqlalchemy.engines [-] SQL connection failed. 10 attempts left.
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 07:43:46 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 07:43:46 15093 [Warning] Buffered warning: Changed limits: max_connections: 214 (requested 500)
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 07:43:46 15093 [Warning] Buffered warning: Changed limits: max_open_files: 1024 (requested 5000)
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 07:43:46 15093 [Warning] Buffered warning: Changed limits: table_open_cache: 400 (requested 2000)
/root/openstack/logs/2017-02-09/1428/nova-consoleauth.log:2017-02-09 07:43:46.616 14575 WARNING oslo_reports.guru_meditation_report [-] Guru meditation now registers SIGUSR1 and SIGUSR2 by default for backward compatibility. SIGUSR1 will no longer be registered in a future release, so please use SIGUSR2 to generate reports.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 07:43:46.643 14579 WARNING oslo_reports.guru_meditation_report [-] Guru meditation now registers SIGUSR1 and SIGUSR2 by default for backward compatibility. SIGUSR1 will no longer be registered in a future release, so please use SIGUSR2 to generate reports.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 07:43:46.683 14572 WARNING oslo_reports.guru_meditation_report [-] Guru meditation now registers SIGUSR1 and SIGUSR2 by default for backward compatibility. SIGUSR1 will no longer be registered in a future release, so please use SIGUSR2 to generate reports.
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 07:43:46.966 14631 WARNING oslo_reports.guru_meditation_report [-] Guru meditation now registers SIGUSR1 and SIGUSR2 by default for backward compatibility. SIGUSR1 will no longer be registered in a future release, so please use SIGUSR2 to generate reports.
/root/openstack/logs/2017-02-09/1428/nova-scheduler.log:2017-02-09 07:43:47.125 14577 WARNING oslo_reports.guru_meditation_report [-] Guru meditation now registers SIGUSR1 and SIGUSR2 by default for backward compatibility. SIGUSR1 will no longer be registered in a future release, so please use SIGUSR2 to generate reports.
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 07:43:47.270 14631 WARNING os_brick.initiator.connectors.remotefs [req-c3ddc3d7-bc27-4274-9d1c-b93fbb208d83 - - - - -] Connection details not present. RemoteFsClient may not initialize properly.
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 07:43:48.695 14631 WARNING nova.compute.monitors [req-5611e8e2-ac56-4ff4-a941-7d01a2a097d6 - - - - -] Excluding nova.compute.monitors.cpu monitor virt_driver. Not in the list of enabled monitors (CONF.compute_monitors).
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 07:43:49.244 14631 WARNING nova.scheduler.client.report [req-5611e8e2-ac56-4ff4-a941-7d01a2a097d6 - - - - -] No authentication information found for placement API. Optional use of placement API for reporting is now disabled.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 07:43:55.850 14586 WARNING neutron.agent.securitygroups_rpc [req-ce6d7af0-53bc-4199-8423-ff06965c026a - - - - -] Driver configuration doesn't match with enable_security_group
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 07:43:55.910 14586 WARNING neutron.api.extensions [req-ce6d7af0-53bc-4199-8423-ff06965c026a - - - - -] Extension dns-integration not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 07:43:55.917 14586 WARNING neutron.api.extensions [req-ce6d7af0-53bc-4199-8423-ff06965c026a - - - - -] Extension ip_allocation not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 07:43:55.917 14586 WARNING neutron.api.extensions [req-ce6d7af0-53bc-4199-8423-ff06965c026a - - - - -] Extension l2_adjacency not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 07:43:55.923 14586 WARNING neutron.api.extensions [req-ce6d7af0-53bc-4199-8423-ff06965c026a - - - - -] Extension metering not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 07:43:55.933 14586 WARNING neutron.api.extensions [req-ce6d7af0-53bc-4199-8423-ff06965c026a - - - - -] Extension qos not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 07:43:55.937 14586 WARNING neutron.api.extensions [req-ce6d7af0-53bc-4199-8423-ff06965c026a - - - - -] Extension router-service-type not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 07:43:55.940 14586 WARNING neutron.api.extensions [req-ce6d7af0-53bc-4199-8423-ff06965c026a - - - - -] Extension segment not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 07:43:55.946 14586 WARNING neutron.api.extensions [req-ce6d7af0-53bc-4199-8423-ff06965c026a - - - - -] Extension trunk not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 07:43:55.947 14586 WARNING neutron.api.extensions [req-ce6d7af0-53bc-4199-8423-ff06965c026a - - - - -] Extension trunk-details not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 07:43:55.948 14586 WARNING neutron.api.extensions [req-ce6d7af0-53bc-4199-8423-ff06965c026a - - - - -] Extension vlan-transparent not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 07:44:07.862 14631 WARNING nova.compute.manager [-] [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b] Instance shutdown by itself. Calling the stop API. Current vm_state: active, current task_state: None, original DB power_state: 1, current VM power_state: 4
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 07:44:44.206 15469 WARNING neutron.db.agentschedulers_db [req-09254073-ffe7-42cb-bed8-ee044a757931 - - - - -] No DHCP agents available, skipping rescheduling
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 07:44:45.129 14588 ERROR neutron.agent.metadata.agent 
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 07:44:45.129 14588 ERROR neutron.agent.metadata.agent [-] Failed reporting state!
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 07:44:45.129 14588 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/metadata/agent.py", line 262, in _report_state
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 07:44:45.129 14588 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/rpc.py", line 88, in report_state
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 07:44:45.129 14588 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 238, in get
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 07:44:45.129 14588 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 336, in wait
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 07:44:45.129 14588 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 453, in _send
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 07:44:45.129 14588 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 464, in send
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 07:44:45.129 14588 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py", line 169, in call
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 07:44:45.129 14588 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/transport.py", line 97, in _send
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 07:44:45.129 14588 ERROR neutron.agent.metadata.agent     message = self.waiters.get(msg_id, timeout=timeout)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 07:44:45.129 14588 ERROR neutron.agent.metadata.agent MessagingTimeout: Timed out waiting for a reply to message ID d35e35b3430f4e38a0e3112782d07965
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 07:44:45.129 14588 ERROR neutron.agent.metadata.agent     result = self._waiter.wait(msg_id, timeout)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 07:44:45.129 14588 ERROR neutron.agent.metadata.agent     retry=retry)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 07:44:45.129 14588 ERROR neutron.agent.metadata.agent     retry=self.retry)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 07:44:45.129 14588 ERROR neutron.agent.metadata.agent     return method(context, 'report_state', **kwargs)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 07:44:45.129 14588 ERROR neutron.agent.metadata.agent     timeout=timeout, retry=retry)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 07:44:45.129 14588 ERROR neutron.agent.metadata.agent     'to message ID %s' % msg_id)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 07:44:45.129 14588 ERROR neutron.agent.metadata.agent Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 07:44:45.129 14588 ERROR neutron.agent.metadata.agent     use_call=self.agent_state.get('start_flag'))
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 07:44:45.132 14588 WARNING oslo.service.loopingcall [-] Function 'neutron.agent.metadata.agent.UnixDomainMetadataProxy._report_state' run outlasted interval by 30.06 sec
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:45.137 14587 ERROR neutron.agent.dhcp.agent 
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:45.137 14587 ERROR neutron.agent.dhcp.agent     ctx, self.agent_state, True)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:45.137 14587 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/dhcp/agent.py", line 687, in _report_state
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:45.137 14587 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/rpc.py", line 88, in report_state
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:45.137 14587 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 238, in get
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:45.137 14587 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 336, in wait
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:45.137 14587 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 453, in _send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:45.137 14587 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 464, in send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:45.137 14587 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py", line 169, in call
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:45.137 14587 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/transport.py", line 97, in _send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:45.137 14587 ERROR neutron.agent.dhcp.agent     message = self.waiters.get(msg_id, timeout=timeout)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:45.137 14587 ERROR neutron.agent.dhcp.agent MessagingTimeout: Timed out waiting for a reply to message ID 5c627ed063ca4df9a50c2d306349f97d
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:45.137 14587 ERROR neutron.agent.dhcp.agent [req-5c959a0b-e58b-467d-b7fe-be3732bcc6d7 - - - - -] Failed reporting state!
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:45.137 14587 ERROR neutron.agent.dhcp.agent     result = self._waiter.wait(msg_id, timeout)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:45.137 14587 ERROR neutron.agent.dhcp.agent     retry=retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:45.137 14587 ERROR neutron.agent.dhcp.agent     retry=self.retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:45.137 14587 ERROR neutron.agent.dhcp.agent     return method(context, 'report_state', **kwargs)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:45.137 14587 ERROR neutron.agent.dhcp.agent     timeout=timeout, retry=retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:45.137 14587 ERROR neutron.agent.dhcp.agent     'to message ID %s' % msg_id)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:45.137 14587 ERROR neutron.agent.dhcp.agent Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:45.142 14587 WARNING oslo.service.loopingcall [req-5c959a0b-e58b-467d-b7fe-be3732bcc6d7 - - - - -] Function 'neutron.agent.dhcp.agent.DhcpAgentWithStateReport._report_state' run outlasted interval by 30.06 sec
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:45.143 14587 ERROR neutron.common.rpc [req-82cfc849-e45e-43dd-88b0-a69bfeca176e - - - - -] Timeout in RPC method get_active_networks_info. Waiting for 7 seconds before next attempt. If the server is not down, consider increasing the rpc_response_timeout option as Neutron server(s) may be overloaded and unable to respond quickly enough.
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:45.144 14587 WARNING neutron.common.rpc [req-82cfc849-e45e-43dd-88b0-a69bfeca176e - - - - -] Increasing timeout for get_active_networks_info calls to 120 seconds. Restart the agent to restore it to the default value.
/root/openstack/logs/2017-02-09/1428/l3-agent.log:2017-02-09 07:44:45.170 14591 ERROR neutron.common.rpc [req-ca70f015-d67d-4c43-9782-8d633bdb5368 - - - - -] Timeout in RPC method get_service_plugin_list. Waiting for 5 seconds before next attempt. If the server is not down, consider increasing the rpc_response_timeout option as Neutron server(s) may be overloaded and unable to respond quickly enough.
/root/openstack/logs/2017-02-09/1428/l3-agent.log:2017-02-09 07:44:45.172 14591 WARNING neutron.common.rpc [req-ca70f015-d67d-4c43-9782-8d633bdb5368 - - - - -] Increasing timeout for get_service_plugin_list calls to 120 seconds. Restart the agent to restore it to the default value.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 07:44:45.712 14608 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent 
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 07:44:45.712 14608 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Failed reporting state!
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 07:44:45.712 14608 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/neutron/agent/rpc.py", line 88, in report_state
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 07:44:45.712 14608 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/neutron/plugins/ml2/drivers/openvswitch/agent/ovs_neutron_agent.py", line 320, in _report_state
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 07:44:45.712 14608 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 238, in get
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 07:44:45.712 14608 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 336, in wait
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 07:44:45.712 14608 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 453, in _send
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 07:44:45.712 14608 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 464, in send
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 07:44:45.712 14608 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py", line 169, in call
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 07:44:45.712 14608 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/transport.py", line 97, in _send
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 07:44:45.712 14608 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     message = self.waiters.get(msg_id, timeout=timeout)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 07:44:45.712 14608 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent MessagingTimeout: Timed out waiting for a reply to message ID 87480171b61f4f74915423eda938b6a0
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 07:44:45.712 14608 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     result = self._waiter.wait(msg_id, timeout)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 07:44:45.712 14608 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     retry=retry)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 07:44:45.712 14608 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     retry=self.retry)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 07:44:45.712 14608 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     return method(context, 'report_state', **kwargs)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 07:44:45.712 14608 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     timeout=timeout, retry=retry)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 07:44:45.712 14608 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     'to message ID %s' % msg_id)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 07:44:45.712 14608 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 07:44:45.712 14608 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     True)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 07:44:45.715 14608 WARNING oslo.service.loopingcall [-] Function 'neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent.OVSNeutronAgent._report_state' run outlasted interval by 30.05 sec
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 07:44:46.093 14608 ERROR neutron.common.rpc [req-f1dfd205-9f42-4aa6-9478-1c04ad5f4dd7 - - - - -] Timeout in RPC method tunnel_sync. Waiting for 50 seconds before next attempt. If the server is not down, consider increasing the rpc_response_timeout option as Neutron server(s) may be overloaded and unable to respond quickly enough.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 07:44:46.094 14608 WARNING neutron.common.rpc [req-f1dfd205-9f42-4aa6-9478-1c04ad5f4dd7 - - - - -] Increasing timeout for tunnel_sync calls to 120 seconds. Restart the agent to restore it to the default value.
/root/openstack/logs/2017-02-09/1428/l3-agent.log:2017-02-09 07:44:50.440 14591 WARNING neutron.agent.l3.agent [req-ca70f015-d67d-4c43-9782-8d633bdb5368 - - - - -] l3-agent cannot contact neutron server to retrieve service plugins enabled. Check connectivity to neutron server. Retrying... Detailed message: Timed out waiting for a reply to message ID dc24125235c2408b9922844ab139c257.
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:51.690 14587 ERROR neutron.agent.dhcp.agent 
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:51.690 14587 ERROR neutron.agent.dhcp.agent     active_networks = self.plugin_rpc.get_active_networks_info()
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:51.690 14587 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/dhcp/agent.py", line 159, in sync_state
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:51.690 14587 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/dhcp/agent.py", line 522, in get_active_networks_info
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:51.690 14587 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/common/rpc.py", line 138, in call
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:51.690 14587 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/common/rpc.py", line 157, in call
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:51.690 14587 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 238, in get
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:51.690 14587 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 336, in wait
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:51.690 14587 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 453, in _send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:51.690 14587 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 464, in send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:51.690 14587 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py", line 169, in call
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:51.690 14587 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/transport.py", line 97, in _send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:51.690 14587 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 196, in force_reraise
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:51.690 14587 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 220, in __exit__
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:51.690 14587 ERROR neutron.agent.dhcp.agent     host=self.host)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:51.690 14587 ERROR neutron.agent.dhcp.agent     message = self.waiters.get(msg_id, timeout=timeout)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:51.690 14587 ERROR neutron.agent.dhcp.agent MessagingTimeout: Timed out waiting for a reply to message ID 270d2e0cc9084e7e9f38361576883063
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:51.690 14587 ERROR neutron.agent.dhcp.agent [req-82cfc849-e45e-43dd-88b0-a69bfeca176e - - - - -] Unable to sync network state.
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:51.690 14587 ERROR neutron.agent.dhcp.agent     result = self._waiter.wait(msg_id, timeout)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:51.690 14587 ERROR neutron.agent.dhcp.agent     retry=retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:51.690 14587 ERROR neutron.agent.dhcp.agent     retry=self.retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:51.690 14587 ERROR neutron.agent.dhcp.agent     return self._original_context.call(ctxt, method, **kwargs)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:51.690 14587 ERROR neutron.agent.dhcp.agent     self.force_reraise()
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:51.690 14587 ERROR neutron.agent.dhcp.agent     six.reraise(self.type_, self.value, self.tb)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:51.690 14587 ERROR neutron.agent.dhcp.agent     timeout=timeout, retry=retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:51.690 14587 ERROR neutron.agent.dhcp.agent     time.sleep(wait)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:51.690 14587 ERROR neutron.agent.dhcp.agent     'to message ID %s' % msg_id)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 07:44:51.690 14587 ERROR neutron.agent.dhcp.agent Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 07:45:38.048 14608 WARNING neutron.plugins.ml2.drivers.openvswitch.agent.openflow.ovs_ofctl.ofswitch [req-f1dfd205-9f42-4aa6-9478-1c04ad5f4dd7 - - - - -] Deleting flow  cookie=0xbd1e6c3356ec869d, duration=2624.457s, table=0, n_packets=0, n_bytes=0, idle_age=2624, priority=10,icmp6,in_port=32,icmp_type=136 actions=resubmit(,24)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 07:45:38.058 14608 WARNING neutron.plugins.ml2.drivers.openvswitch.agent.openflow.ovs_ofctl.ofswitch [req-f1dfd205-9f42-4aa6-9478-1c04ad5f4dd7 - - - - -] Deleting flow  cookie=0xbd1e6c3356ec869d, duration=2624.442s, table=0, n_packets=3, n_bytes=126, idle_age=2549, priority=10,arp,in_port=32 actions=resubmit(,24)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 07:45:38.066 14608 WARNING neutron.plugins.ml2.drivers.openvswitch.agent.openflow.ovs_ofctl.ofswitch [req-f1dfd205-9f42-4aa6-9478-1c04ad5f4dd7 - - - - -] Deleting flow  cookie=0xbd1e6c3356ec869d, duration=2624.474s, table=0, n_packets=135, n_bytes=13312, idle_age=2554, priority=9,in_port=32 actions=resubmit(,25)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 07:45:38.075 14608 WARNING neutron.plugins.ml2.drivers.openvswitch.agent.openflow.ovs_ofctl.ofswitch [req-f1dfd205-9f42-4aa6-9478-1c04ad5f4dd7 - - - - -] Deleting flow  cookie=0xbd1e6c3356ec869d, duration=2624.465s, table=24, n_packets=0, n_bytes=0, idle_age=2624, priority=2,icmp6,in_port=32,icmp_type=136,nd_target=fe80::f816:3eff:fe10:a760 actions=NORMAL
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 07:45:38.084 14608 WARNING neutron.plugins.ml2.drivers.openvswitch.agent.openflow.ovs_ofctl.ofswitch [req-f1dfd205-9f42-4aa6-9478-1c04ad5f4dd7 - - - - -] Deleting flow  cookie=0xbd1e6c3356ec869d, duration=2624.450s, table=24, n_packets=3, n_bytes=126, idle_age=2549, priority=2,arp,in_port=32,arp_spa=192.168.2.6 actions=resubmit(,25)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 07:45:38.093 14608 WARNING neutron.plugins.ml2.drivers.openvswitch.agent.openflow.ovs_ofctl.ofswitch [req-f1dfd205-9f42-4aa6-9478-1c04ad5f4dd7 - - - - -] Deleting flow  cookie=0xbd1e6c3356ec869d, duration=2624.488s, table=25, n_packets=138, n_bytes=13438, idle_age=2549, priority=2,in_port=32,dl_src=fa:16:3e:10:a7:60 actions=NORMAL
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 07:45:38.147 15467 WARNING neutron.plugins.ml2.drivers.type_tunnel [req-d074e04a-1d0e-41b9-9727-097849c76ac2 - - - - -] Endpoint with ip x.x.x.2 already exists
/root/openstack/logs/2017-02-09/1428/api.log:2017-02-09 08:00:38.803 14941 WARNING oslo_config.cfg [req-1c28e0d5-e42e-40f1-8d8b-381945a1d872 fbf650853680448684c3eea8d2a6d0b7 e9f7777a80214091867a0e8bca2d8a22 - default default] Option "sql_connection" from group "DEFAULT" is deprecated. Use option "connection" from group "database".
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:21:22.669 14608 ERROR neutron.agent.linux.async_process [-] Error received from [ovsdb-client monitor Interface name,ofport,external_ids --format=json]: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:21:22.669 14608 ERROR neutron.agent.linux.async_process [-] Process [ovsdb-client monitor Interface name,ofport,external_ids --format=json] dies due to the error: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:21:26.228 21021 WARNING stevedore.named [req-0015a0e9-629a-42e6-80da-9a2f61c91a47 - - - - -] Could not load neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:22:23.040 14631 ERROR oslo_service.periodic_task 
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:22:23.040 14631 ERROR oslo_service.periodic_task     args=args, kwargs=kwargs)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:22:23.040 14631 ERROR oslo_service.periodic_task     args, kwargs)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:22:23.040 14631 ERROR oslo_service.periodic_task     context, self.host, expected_attrs=[], use_slave=True)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:22:23.040 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 5712, in _heal_instance_info_cache
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:22:23.040 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/nova/conductor/rpcapi.py", line 236, in object_class_action_versions
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:22:23.040 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 238, in get
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:22:23.040 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 336, in wait
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:22:23.040 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 453, in _send
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:22:23.040 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 464, in send
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:22:23.040 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py", line 169, in call
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:22:23.040 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_messaging/transport.py", line 97, in _send
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:22:23.040 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_service/periodic_task.py", line 220, in run_periodic_tasks
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:22:23.040 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_versionedobjects/base.py", line 177, in wrapper
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:22:23.040 14631 ERROR oslo_service.periodic_task     message = self.waiters.get(msg_id, timeout=timeout)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:22:23.040 14631 ERROR oslo_service.periodic_task MessagingTimeout: Timed out waiting for a reply to message ID 2ec4574de0a34924a7076e7b64d5f18d
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:22:23.040 14631 ERROR oslo_service.periodic_task [req-f520423f-ddd8-43ed-a44f-35c0f686bfbf - - - - -] Error during ComputeManager._heal_instance_info_cache
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:22:23.040 14631 ERROR oslo_service.periodic_task     result = self._waiter.wait(msg_id, timeout)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:22:23.040 14631 ERROR oslo_service.periodic_task     retry=retry)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:22:23.040 14631 ERROR oslo_service.periodic_task     retry=self.retry)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:22:23.040 14631 ERROR oslo_service.periodic_task     task(self, context)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:22:23.040 14631 ERROR oslo_service.periodic_task     timeout=timeout, retry=retry)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:22:23.040 14631 ERROR oslo_service.periodic_task     'to message ID %s' % msg_id)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:22:23.040 14631 ERROR oslo_service.periodic_task Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:22:24.409 14631 ERROR oslo.messaging._drivers.impl_rabbit [-] [405667e5-dad5-470d-9197-2aa0bf8b2446] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: timed out. Trying again in 1 seconds. Client port: 33940
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:22:26.325 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent 
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:22:26.325 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Failed reporting state!
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:22:26.325 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/neutron/agent/rpc.py", line 88, in report_state
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:22:26.325 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/neutron/plugins/ml2/drivers/openvswitch/agent/ovs_neutron_agent.py", line 320, in _report_state
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:22:26.325 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 238, in get
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:22:26.325 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 336, in wait
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:22:26.325 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 453, in _send
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:22:26.325 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 464, in send
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:22:26.325 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py", line 169, in call
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:22:26.325 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/transport.py", line 97, in _send
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:22:26.325 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     message = self.waiters.get(msg_id, timeout=timeout)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:22:26.325 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent MessagingTimeout: Timed out waiting for a reply to message ID 49859733d7c14513aee820b68496cd87
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:22:26.325 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     result = self._waiter.wait(msg_id, timeout)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:22:26.325 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     retry=retry)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:22:26.325 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     retry=self.retry)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:22:26.325 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     return method(context, 'report_state', **kwargs)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:22:26.325 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     timeout=timeout, retry=retry)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:22:26.325 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     'to message ID %s' % msg_id)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:22:26.325 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:22:26.325 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     True)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:22:26.329 21021 WARNING oslo.service.loopingcall [-] Function 'neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent.OVSNeutronAgent._report_state' run outlasted interval by 30.07 sec
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:22:26.813 21021 ERROR neutron.common.rpc [req-50676fd7-3ed3-46a4-a5ac-c28c13e66081 - - - - -] Timeout in RPC method tunnel_sync. Waiting for 58 seconds before next attempt. If the server is not down, consider increasing the rpc_response_timeout option as Neutron server(s) may be overloaded and unable to respond quickly enough.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:22:26.814 21021 WARNING neutron.common.rpc [req-50676fd7-3ed3-46a4-a5ac-c28c13e66081 - - - - -] Increasing timeout for tunnel_sync calls to 120 seconds. Restart the agent to restore it to the default value.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:22:41.181 15467 ERROR oslo.messaging._drivers.impl_rabbit [-] [8f0c1896-f37e-4792-943a-9d35fe514202] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 34144
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:22:41.187 15469 ERROR oslo.messaging._drivers.impl_rabbit [-] [2d523404-7195-403a-bad2-582eda68f740] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 34148
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:22:41.209 15467 ERROR oslo.messaging._drivers.impl_rabbit [-] [3cfc65d6-24ba-46c6-8e89-f03676147362] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 34150
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:22:41.240 15461 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:22:41.249 15467 ERROR oslo.messaging._drivers.impl_rabbit [-] [b7e6dce8-9cf5-4b35-9ab5-4b3d788bbf9b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 34154
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:22:41.320 15461 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:22:45.138 14587 ERROR oslo.messaging._drivers.impl_rabbit [req-12da1fef-b4b8-43d3-af47-8235f237da73 - - - - -] [3b6a4cfa-1075-4927-9dce-36f8b3daabcd] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: timed out. Trying again in 1 seconds. Client port: 33740
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 08:22:45.295 14588 ERROR oslo.messaging._drivers.impl_rabbit [-] [6c718410-b662-4952-8482-deef1061f2b0] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: timed out. Trying again in 1 seconds. Client port: 33736
/root/openstack/logs/2017-02-09/1428/l3-agent.log:2017-02-09 08:22:50.589 14591 ERROR oslo.messaging._drivers.impl_rabbit [-] [82be8eb9-eee4-40ee-9625-c555942e1ff2] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: timed out. Trying again in 1 seconds. Client port: 33744
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:22:53.135 15164 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:22:54.425 15135 ERROR oslo.messaging._drivers.impl_rabbit [-] [c9428b15-be20-43c5-bbb3-8c283d9c4672] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33874
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:22:58.161 15469 WARNING neutron.db.agentschedulers_db [req-09254073-ffe7-42cb-bed8-ee044a757931 - - - - -] No DHCP agents available, skipping rescheduling
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:22:59.889 15200 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 08:23:00.191 14588 ERROR oslo.messaging._drivers.impl_rabbit [-] [b3f7d479-dd85-4e8d-9b17-819531292cac] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33732
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:23:00.392 14587 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:02.188 15143 ERROR oslo.messaging._drivers.impl_rabbit [-] [e028f275-61ce-452e-aa1d-01bc87864d7e] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33858
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:02.251 15153 ERROR oslo.messaging._drivers.impl_rabbit [-] [dc2f041b-0156-48df-babf-47c9d08542ce] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33892
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:02.264 15134 ERROR oslo.messaging._drivers.impl_rabbit [-] [8051641b-d4ea-4299-b15e-74f09872adc9] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33898
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:02.266 15149 ERROR oslo.messaging._drivers.impl_rabbit [-] [0c43d075-48f7-4ee5-9d21-69054518b1bd] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33900
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:02.267 15166 ERROR oslo.messaging._drivers.impl_rabbit [-] [1da74124-d952-44df-8b55-29ff40a41271] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33902
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:02.275 15150 ERROR oslo.messaging._drivers.impl_rabbit [-] [31aed8b3-1ee6-4ff8-b965-1b2cb6c5897e] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33904
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:02.278 15182 ERROR oslo.messaging._drivers.impl_rabbit [-] [23230e4a-307a-4911-b86e-0c209db0b76b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33906
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:02.279 15138 ERROR oslo.messaging._drivers.impl_rabbit [-] [c72f80bd-6eee-4976-bc8e-5f4374ef6303] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33908
/root/openstack/logs/2017-02-09/1428/nova-consoleauth.log:2017-02-09 08:23:02.279 14575 ERROR oslo.messaging._drivers.impl_rabbit [-] [57cddd80-98e9-4488-b232-bdfb177a2642] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33862
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:02.327 15161 ERROR oslo.messaging._drivers.impl_rabbit [-] [4e2076a3-402c-4b4a-a550-84624b455289] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33928
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:02.327 15167 ERROR oslo.messaging._drivers.impl_rabbit [-] [39c4b8bf-737f-4e00-861d-270db813308d] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33926
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:02.335 15176 ERROR oslo.messaging._drivers.impl_rabbit [-] [fa483a25-d75d-4315-b120-48e602136e2b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33930
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:02.355 15152 ERROR oslo.messaging._drivers.impl_rabbit [-] [444c58a0-24a7-4477-91a2-742facad4004] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33932
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:02.360 15173 ERROR oslo.messaging._drivers.impl_rabbit [-] [9b3e5b9b-6529-4d55-8972-4ee9fbf680f7] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33934
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:02.370 15178 ERROR oslo.messaging._drivers.impl_rabbit [-] [8fb90a7d-2344-44dd-bd02-cf8e9688e033] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33938
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:02.432 15200 ERROR oslo.messaging._drivers.impl_rabbit [-] [a1b7c211-0b0d-4a23-9d14-46a2e20e48b0] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33944
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:02.597 15143 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:03.405 15167 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:03.483 15161 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:03.584 15176 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:03.642 15152 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:04.423 15133 ERROR oslo.messaging._drivers.impl_rabbit [-] [11cc8d55-1dc2-4a70-81f5-2a60b9483b29] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33876
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:04.523 15135 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:23:04.588 14631 ERROR oslo.messaging._drivers.impl_rabbit [-] [74acb386-5ca9-48ba-ae1d-6ec4207d86e3] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 34138
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:04.608 15133 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/l3-agent.log:2017-02-09 08:23:05.636 14591 ERROR oslo.messaging._drivers.impl_rabbit [-] [05692361-ac32-4e81-bde0-8bb66c31a368] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 34220
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:23:05.770 15469 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:09.851 15173 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:09.857 15180 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:09.891 15140 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:11.476 15178 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:14.045 15130 ERROR oslo.messaging._drivers.impl_rabbit [-] [4deda250-2e09-490d-a793-99b9484a48d3] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33882
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:14.422 15142 ERROR oslo.messaging._drivers.impl_rabbit [-] [b3fe5f2f-c011-47ff-8322-992c687ade5a] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33884
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:14.728 15174 ERROR oslo.messaging._drivers.impl_rabbit [-] [66b09f28-652b-4ed4-bb66-0df619217812] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33886
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:14.767 15127 ERROR oslo.messaging._drivers.impl_rabbit [-] [ec4bb420-5d73-4024-a60d-aab0ad24c92f] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33880
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:14.839 15139 ERROR oslo.messaging._drivers.impl_rabbit [-] [a1908a35-9740-4bf1-84c2-d3ae836ce27f] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33888
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:14.923 15141 ERROR oslo.messaging._drivers.impl_rabbit [-] [00e7658d-d30a-4495-a47a-2ff54c3fedbf] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33890
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:23:14.958 14631 ERROR oslo.messaging._drivers.impl_rabbit [-] [deb7688b-13f8-45ba-8218-74c26d7746e3] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33936
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:23:15.159 14587 ERROR oslo.messaging._drivers.impl_rabbit [-] [9ca423ef-f3af-4c3c-bc9e-082d99089ca3] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33734
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:23:15.818 15467 ERROR oslo.messaging._drivers.impl_rabbit [-] [91cc356d-0a2f-467f-85e9-b817d111bf9e] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 34152
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:19.686 15130 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:19.769 15142 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:19.856 15197 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:19.866 15127 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:23:20.599 15468 ERROR oslo.messaging._drivers.impl_rabbit [-] [56dbf09b-c7c2-4824-ae0b-d6b4ee763f91] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 34146
/root/openstack/logs/2017-02-09/1428/l3-agent.log:2017-02-09 08:23:20.638 14591 ERROR oslo.messaging._drivers.impl_rabbit [-] [8d3db2ce-d48e-4f7f-bdfd-d4bd2056444a] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33742
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:22.889 15139 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:23.028 15141 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:23.050 15164 ERROR oslo.messaging._drivers.impl_rabbit [-] [053041ee-0710-4b34-b165-018215984421] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33896
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:23.277 15153 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:23.332 15149 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:23.401 15166 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:23.474 15134 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:23.592 15150 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:23.801 15138 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:23.842 15182 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:24.867 15174 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:23:26.335 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent 
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:23:26.335 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Failed reporting state!
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:23:26.335 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/neutron/agent/rpc.py", line 88, in report_state
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:23:26.335 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/neutron/plugins/ml2/drivers/openvswitch/agent/ovs_neutron_agent.py", line 320, in _report_state
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:23:26.335 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 238, in get
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:23:26.335 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 336, in wait
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:23:26.335 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 453, in _send
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:23:26.335 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 464, in send
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:23:26.335 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py", line 169, in call
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:23:26.335 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/transport.py", line 97, in _send
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:23:26.335 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     message = self.waiters.get(msg_id, timeout=timeout)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:23:26.335 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent MessagingTimeout: Timed out waiting for a reply to message ID 961ff7a332b7464093d8ae1cfbc70265
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:23:26.335 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     result = self._waiter.wait(msg_id, timeout)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:23:26.335 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     retry=retry)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:23:26.335 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     retry=self.retry)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:23:26.335 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     return method(context, 'report_state', **kwargs)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:23:26.335 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     timeout=timeout, retry=retry)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:23:26.335 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     'to message ID %s' % msg_id)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:23:26.335 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:23:26.335 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     True)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:23:26.338 21021 WARNING oslo.service.loopingcall [-] Function 'neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent.OVSNeutronAgent._report_state' run outlasted interval by 30.01 sec
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:23:27.074 14631 ERROR oslo_service.periodic_task 
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:23:27.074 14631 ERROR oslo_service.periodic_task     args=args, kwargs=kwargs)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:23:27.074 14631 ERROR oslo_service.periodic_task     args, kwargs)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:23:27.074 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 6445, in update_available_resource
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:23:27.074 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 6463, in _get_compute_nodes_in_db
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:23:27.074 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/nova/conductor/rpcapi.py", line 236, in object_class_action_versions
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:23:27.074 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 238, in get
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:23:27.074 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 336, in wait
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:23:27.074 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 453, in _send
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:23:27.074 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 464, in send
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:23:27.074 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py", line 169, in call
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:23:27.074 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_messaging/transport.py", line 97, in _send
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:23:27.074 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_service/periodic_task.py", line 220, in run_periodic_tasks
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:23:27.074 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_versionedobjects/base.py", line 177, in wrapper
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:23:27.074 14631 ERROR oslo_service.periodic_task     message = self.waiters.get(msg_id, timeout=timeout)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:23:27.074 14631 ERROR oslo_service.periodic_task MessagingTimeout: Timed out waiting for a reply to message ID b2cea105d77d4ba4bf03ae3deab7f266
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:23:27.074 14631 ERROR oslo_service.periodic_task [req-f520423f-ddd8-43ed-a44f-35c0f686bfbf - - - - -] Error during ComputeManager.update_available_resource
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:23:27.074 14631 ERROR oslo_service.periodic_task     result = self._waiter.wait(msg_id, timeout)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:23:27.074 14631 ERROR oslo_service.periodic_task     retry=retry)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:23:27.074 14631 ERROR oslo_service.periodic_task     retry=self.retry)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:23:27.074 14631 ERROR oslo_service.periodic_task     task(self, context)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:23:27.074 14631 ERROR oslo_service.periodic_task     timeout=timeout, retry=retry)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:23:27.074 14631 ERROR oslo_service.periodic_task     'to message ID %s' % msg_id)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:23:27.074 14631 ERROR oslo_service.periodic_task Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:23:27.074 14631 ERROR oslo_service.periodic_task     use_slave=True)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:23:27.074 14631 ERROR oslo_service.periodic_task     use_slave=use_slave)
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:29.871 15187 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:29.883 15177 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:23:30.479 15467 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:23:30.603 15468 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:32.203 15145 ERROR oslo.messaging._drivers.impl_rabbit [-] [be0f3dfd-d23f-4812-beb5-abe5ebfe5d02] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33860
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:32.210 15137 ERROR oslo.messaging._drivers.impl_rabbit [-] [ba41a281-d6b2-4930-b5e1-b73b56cb3d40] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33866
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:32.214 15131 ERROR oslo.messaging._drivers.impl_rabbit [-] [d06d27ae-76bf-4da2-945f-f8571bba1e30] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33864
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:32.222 15157 ERROR oslo.messaging._drivers.impl_rabbit [-] [e3b85723-d3a3-4ce9-a578-3cbdbaad4da4] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33868
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:32.224 15132 ERROR oslo.messaging._drivers.impl_rabbit [-] [29738324-fb71-4670-855e-39879faf84ec] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33870
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:32.228 15128 ERROR oslo.messaging._drivers.impl_rabbit [-] [eed24f44-683d-40e3-a877-c42bf76806b1] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33872
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:32.243 15136 ERROR oslo.messaging._drivers.impl_rabbit [-] [63098a60-acde-4538-8807-f92eb40dd669] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33878
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:32.284 15177 ERROR oslo.messaging._drivers.impl_rabbit [-] [a39e12a4-3d89-4f40-b8af-62102286d2a4] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33910
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:32.291 15187 ERROR oslo.messaging._drivers.impl_rabbit [-] [9679f8a1-4278-4a91-97ad-eff93b134f93] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33916
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:32.293 15197 ERROR oslo.messaging._drivers.impl_rabbit [-] [16119cd9-e7f0-4e73-a6bd-ec75e4a81833] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33914
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:32.294 15189 ERROR oslo.messaging._drivers.impl_rabbit [-] [03e3da9b-0807-4d83-b178-79c02542f72c] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33918
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:32.303 15140 ERROR oslo.messaging._drivers.impl_rabbit [-] [a185e798-f7c1-4095-86bb-054e70178fe0] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33912
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:32.318 15190 ERROR oslo.messaging._drivers.impl_rabbit [-] [82bd5492-fb1c-4272-a780-f1d76bccea39] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33920
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:32.318 15192 ERROR oslo.messaging._drivers.impl_rabbit [-] [267833d4-5661-4570-8e91-0b88116e1d43] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33921
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:32.330 15195 ERROR oslo.messaging._drivers.impl_rabbit [-] [3f93489f-169c-4893-8767-7ec0bc1642ff] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33924
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:32.377 15189 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:32.462 15180 ERROR oslo.messaging._drivers.impl_rabbit [-] [860c1855-a165-46f0-ae68-eacca588b577] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33946
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:32.685 15145 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-scheduler.log:2017-02-09 08:23:32.692 14577 ERROR oslo.messaging._drivers.impl_rabbit [-] [8b6d4351-e4c7-4ab2-a70c-8d76015a2f8d] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 33956
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:32.796 15137 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:32.840 15132 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:33.154 15190 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:33.240 15192 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:33.322 15195 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:33.941 15131 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:33.975 15157 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:34.422 15128 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:23:34.455 15136 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:23:35.179 15469 WARNING neutron.db.agentschedulers_db [req-09254073-ffe7-42cb-bed8-ee044a757931 - - - - -] No DHCP agents available, skipping rescheduling
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:23:36.805 14587 ERROR oslo.messaging._drivers.impl_rabbit [-] [c6bf7f05-8e17-43f7-a8dc-8aebdb7886a0] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 34228
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 08:23:46.320 14588 WARNING oslo.service.loopingcall [-] Function 'neutron.agent.metadata.agent.UnixDomainMetadataProxy._report_state' run outlasted interval by 91.03 sec
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:24:12.206 15469 WARNING neutron.db.agentschedulers_db [req-09254073-ffe7-42cb-bed8-ee044a757931 - - - - -] No DHCP agents available, skipping rescheduling
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:24:15.986 14631 WARNING oslo.service.loopingcall [-] Function 'nova.servicegroup.drivers.db.DbDriver._report_state' run outlasted interval by 161.58 sec
/root/openstack/logs/2017-02-09/1428/l3-agent.log:2017-02-09 08:24:21.663 14591 WARNING oslo.service.loopingcall [-] Function 'neutron.agent.l3.agent.L3NATAgentWithStateReport._report_state' run outlasted interval by 121.08 sec
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:24:24.582 21021 ERROR neutron.common.rpc [req-50676fd7-3ed3-46a4-a5ac-c28c13e66081 - - - - -] Timeout in RPC method get_devices_details_list_and_failed_devices. Waiting for 57 seconds before next attempt. If the server is not down, consider increasing the rpc_response_timeout option as Neutron server(s) may be overloaded and unable to respond quickly enough.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:24:24.583 21021 WARNING neutron.common.rpc [req-50676fd7-3ed3-46a4-a5ac-c28c13e66081 - - - - -] Increasing timeout for get_devices_details_list_and_failed_devices calls to 120 seconds. Restart the agent to restore it to the default value.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:24:26.343 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent 
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:24:26.343 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Failed reporting state!
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:24:26.343 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/neutron/agent/rpc.py", line 88, in report_state
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:24:26.343 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/neutron/plugins/ml2/drivers/openvswitch/agent/ovs_neutron_agent.py", line 320, in _report_state
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:24:26.343 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 238, in get
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:24:26.343 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 336, in wait
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:24:26.343 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 453, in _send
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:24:26.343 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 464, in send
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:24:26.343 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py", line 169, in call
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:24:26.343 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/transport.py", line 97, in _send
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:24:26.343 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     message = self.waiters.get(msg_id, timeout=timeout)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:24:26.343 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent MessagingTimeout: Timed out waiting for a reply to message ID 2561c1b610dd4852be92ea6cdf2d89f9
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:24:26.343 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     result = self._waiter.wait(msg_id, timeout)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:24:26.343 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     retry=retry)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:24:26.343 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     retry=self.retry)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:24:26.343 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     return method(context, 'report_state', **kwargs)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:24:26.343 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     timeout=timeout, retry=retry)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:24:26.343 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     'to message ID %s' % msg_id)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:24:26.343 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:24:26.343 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     True)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:24:26.345 21021 WARNING oslo.service.loopingcall [-] Function 'neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent.OVSNeutronAgent._report_state' run outlasted interval by 30.01 sec
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:24:27.081 14631 ERROR oslo_service.periodic_task 
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:24:27.081 14631 ERROR oslo_service.periodic_task     args=args, kwargs=kwargs)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:24:27.081 14631 ERROR oslo_service.periodic_task     args, kwargs)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:24:27.081 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 1637, in _sync_scheduler_instance_info
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:24:27.081 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/nova/conductor/rpcapi.py", line 236, in object_class_action_versions
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:24:27.081 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 238, in get
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:24:27.081 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 336, in wait
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:24:27.081 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 453, in _send
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:24:27.081 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 464, in send
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:24:27.081 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py", line 169, in call
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:24:27.081 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_messaging/transport.py", line 97, in _send
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:24:27.081 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_service/periodic_task.py", line 220, in run_periodic_tasks
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:24:27.081 14631 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_versionedobjects/base.py", line 177, in wrapper
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:24:27.081 14631 ERROR oslo_service.periodic_task     message = self.waiters.get(msg_id, timeout=timeout)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:24:27.081 14631 ERROR oslo_service.periodic_task MessagingTimeout: Timed out waiting for a reply to message ID 9f7a4bce6a3f4dafbe85e49a5f6cdde6
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:24:27.081 14631 ERROR oslo_service.periodic_task [req-f520423f-ddd8-43ed-a44f-35c0f686bfbf - - - - -] Error during ComputeManager._sync_scheduler_instance_info
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:24:27.081 14631 ERROR oslo_service.periodic_task     result = self._waiter.wait(msg_id, timeout)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:24:27.081 14631 ERROR oslo_service.periodic_task     retry=retry)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:24:27.081 14631 ERROR oslo_service.periodic_task     retry=self.retry)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:24:27.081 14631 ERROR oslo_service.periodic_task     task(self, context)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:24:27.081 14631 ERROR oslo_service.periodic_task     timeout=timeout, retry=retry)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:24:27.081 14631 ERROR oslo_service.periodic_task     'to message ID %s' % msg_id)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:24:27.081 14631 ERROR oslo_service.periodic_task Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:24:27.081 14631 ERROR oslo_service.periodic_task     use_slave=True)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:24:41.313 15461 ERROR oslo.messaging._drivers.impl_rabbit [-] [717d7d1d-fdf1-4f6c-97f0-3f52ca0de415] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: timed out. Trying again in 1 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:24:41.323 15461 ERROR oslo.messaging._drivers.impl_rabbit [-] [29ed4bd4-275f-49db-9ef6-84eda48adaff] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: timed out. Trying again in 1 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:24:46.168 14587 ERROR neutron.agent.dhcp.agent 
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:24:46.168 14587 ERROR neutron.agent.dhcp.agent     ctx, self.agent_state, True)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:24:46.168 14587 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/dhcp/agent.py", line 687, in _report_state
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:24:46.168 14587 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/rpc.py", line 88, in report_state
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:24:46.168 14587 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 238, in get
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:24:46.168 14587 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 336, in wait
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:24:46.168 14587 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 453, in _send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:24:46.168 14587 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 464, in send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:24:46.168 14587 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py", line 169, in call
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:24:46.168 14587 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/transport.py", line 97, in _send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:24:46.168 14587 ERROR neutron.agent.dhcp.agent     message = self.waiters.get(msg_id, timeout=timeout)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:24:46.168 14587 ERROR neutron.agent.dhcp.agent MessagingTimeout: Timed out waiting for a reply to message ID 1946f268bb0542d9a1e64b4c1a98245f
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:24:46.168 14587 ERROR neutron.agent.dhcp.agent [req-12da1fef-b4b8-43d3-af47-8235f237da73 - - - - -] Failed reporting state!
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:24:46.168 14587 ERROR neutron.agent.dhcp.agent     result = self._waiter.wait(msg_id, timeout)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:24:46.168 14587 ERROR neutron.agent.dhcp.agent     retry=retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:24:46.168 14587 ERROR neutron.agent.dhcp.agent     retry=self.retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:24:46.168 14587 ERROR neutron.agent.dhcp.agent     return method(context, 'report_state', **kwargs)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:24:46.168 14587 ERROR neutron.agent.dhcp.agent     timeout=timeout, retry=retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:24:46.168 14587 ERROR neutron.agent.dhcp.agent     'to message ID %s' % msg_id)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:24:46.168 14587 ERROR neutron.agent.dhcp.agent Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:24:46.171 14587 WARNING oslo.service.loopingcall [req-12da1fef-b4b8-43d3-af47-8235f237da73 - - - - -] Function 'neutron.agent.dhcp.agent.DhcpAgentWithStateReport._report_state' run outlasted interval by 151.04 sec
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:24:53.141 15164 ERROR oslo.messaging._drivers.impl_rabbit [-] [58e84f85-411f-4409-98d0-f32c49a24207] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: timed out. Trying again in 1 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.532 21182 WARNING oslo_db.sqlalchemy.engines [req-be7d0d67-8b08-4db1-aa80-ab0227bd840f - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.536 21175 WARNING oslo_db.sqlalchemy.engines [req-7b6ebb5e-8908-4497-ae98-1d298ca7e576 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.536 21185 WARNING oslo_db.sqlalchemy.engines [req-66a4c3ff-ec32-498e-8ee5-86ee40907dc0 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.541 21174 WARNING oslo_db.sqlalchemy.engines [req-3fa8572a-c451-495f-a294-86da56cce3d6 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.542 21178 WARNING oslo_db.sqlalchemy.engines [req-91e439fd-82de-4d27-b2e6-22962e0179e7 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.543 21186 WARNING oslo_db.sqlalchemy.engines [req-cab7961e-203a-4922-8aa8-371217de8875 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.546 21183 WARNING oslo_db.sqlalchemy.engines [req-c709648c-2520-4ffd-b689-48b99624f305 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.550 21176 WARNING oslo_db.sqlalchemy.engines [req-0c35a556-0af6-4607-880d-08a36d426d8e - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.566 21190 WARNING oslo_db.sqlalchemy.engines [req-123d50c7-282f-4f78-8338-8cee365d059c - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.566 21191 WARNING oslo_db.sqlalchemy.engines [req-49aad111-4888-4870-9845-6867f3db852c - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.566 21192 WARNING oslo_db.sqlalchemy.engines [req-d7aefa95-79d8-4f8d-9d8f-7b5ded8cebc2 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.574 21184 WARNING oslo_db.sqlalchemy.engines [req-8e849d3e-cecd-4a76-a2f1-8cc36ee8b36f - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.574 21187 WARNING oslo_db.sqlalchemy.engines [req-f1a2c179-8ea1-4244-a0c0-c1cd7c4d5924 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.574 21188 WARNING oslo_db.sqlalchemy.engines [req-52ca9adb-d5ad-4ac1-a372-f0081d079eaa - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.574 21189 WARNING oslo_db.sqlalchemy.engines [req-3666a11a-c856-4153-846f-979de7c58be6 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.576 21193 WARNING oslo_db.sqlalchemy.engines [req-ddae784b-10c8-499c-b8fb-485df1389ab9 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.609 21194 WARNING oslo_db.sqlalchemy.engines [req-af38c267-44b3-4e0c-9cab-a21c3b518a61 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.609 21195 WARNING oslo_db.sqlalchemy.engines [req-e325995c-6747-4ee8-a4d9-a1a3af83fd07 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.609 21196 WARNING oslo_db.sqlalchemy.engines [req-0e5c0ba6-392e-4d96-8c82-2a118520d5b3 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.609 21197 WARNING oslo_db.sqlalchemy.engines [req-262a0e69-fb60-4e77-8ce4-e36f23140db0 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.609 21198 WARNING oslo_db.sqlalchemy.engines [req-c22459aa-2ed0-42ee-8b12-fe82a939069d - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.615 21199 WARNING oslo_db.sqlalchemy.engines [req-d80733d4-ca5e-4c03-bde2-d4f0ad1df93a - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.622 21203 WARNING oslo_db.sqlalchemy.engines [req-d0df6ad9-1a37-426f-951d-e43b4bfc6c12 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.627 21204 WARNING oslo_db.sqlalchemy.engines [req-23e4c8f7-effb-41ed-8f50-51f42666a8ec - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.634 21205 WARNING oslo_db.sqlalchemy.engines [req-5071a69b-f20d-4407-bea2-34b2fb632130 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.641 21206 WARNING oslo_db.sqlalchemy.engines [req-2f68d1a5-6c63-45bd-994a-fc916fc8903d - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.647 21207 WARNING oslo_db.sqlalchemy.engines [req-d74077aa-dd14-4930-a437-d14a21279d11 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.653 21208 WARNING oslo_db.sqlalchemy.engines [req-77a162a0-9a60-41a2-b518-929e711ba40a - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.660 21210 WARNING oslo_db.sqlalchemy.engines [req-2ff89724-8867-46ba-87b7-97932df32e8e - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.664 21211 WARNING oslo_db.sqlalchemy.engines [req-218e40eb-1171-41d5-9a3f-9bcda6e70dee - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.672 21212 WARNING oslo_db.sqlalchemy.engines [req-67bfbfd8-5ce3-40a6-b544-ad5f8abc0702 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.677 21213 WARNING oslo_db.sqlalchemy.engines [req-ef252c0e-38f9-4b9a-9399-2d4d7a0e7d8c - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.683 21214 WARNING oslo_db.sqlalchemy.engines [req-3c06b116-d444-4dfb-bcf5-5015d2d1688c - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.691 21215 WARNING oslo_db.sqlalchemy.engines [req-c1a5b418-4c82-4049-a4fb-7f984d474ef2 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.696 21216 WARNING oslo_db.sqlalchemy.engines [req-8b35a2c8-0ba0-4eb4-b971-aca32ca83113 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.702 21217 WARNING oslo_db.sqlalchemy.engines [req-49810d28-1ce5-44e8-9ec9-4f9d21d7036a - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.708 21218 WARNING oslo_db.sqlalchemy.engines [req-6bd9d336-5a78-457a-aa6f-6017cd4da94d - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.714 21219 WARNING oslo_db.sqlalchemy.engines [req-d29ef813-72eb-4db5-9857-98ad136784c6 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.720 21220 WARNING oslo_db.sqlalchemy.engines [req-95d0065d-3ad6-4344-86e2-36c62e31948f - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.726 21221 WARNING oslo_db.sqlalchemy.engines [req-46dabd4e-9434-46d8-a452-360c5fd4dcf3 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.733 21222 WARNING oslo_db.sqlalchemy.engines [req-3592f3dc-3016-40f9-8ca8-34c7366157f4 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.739 21223 WARNING oslo_db.sqlalchemy.engines [req-2d6d3688-7c24-4cad-9970-744e64e89319 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.745 21224 WARNING oslo_db.sqlalchemy.engines [req-4d86b822-ba7e-404a-9881-35577a9cc516 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.751 21225 WARNING oslo_db.sqlalchemy.engines [req-9da1f294-d829-43e9-957f-c67d4f03fdde - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.756 21227 WARNING oslo_db.sqlalchemy.engines [req-c6d7103f-f7ab-44d6-8135-9fd06319a747 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.761 21229 WARNING oslo_db.sqlalchemy.engines [req-3711b42d-f363-4e79-ad24-3033e46fd80e - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.768 21230 WARNING oslo_db.sqlalchemy.engines [req-80fddd40-6fe6-4d62-b975-31743f5df02d - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.772 21231 WARNING oslo_db.sqlalchemy.engines [req-20d6ab9b-4fe9-425b-a10c-fbd2ca215cc7 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.778 21232 WARNING oslo_db.sqlalchemy.engines [req-b23db19b-d31e-40b3-8358-d441e3611ae7 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.783 21233 WARNING oslo_db.sqlalchemy.engines [req-0e81c9b8-9124-4bd8-8fd6-d0a076c9a6a4 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.788 21234 WARNING oslo_db.sqlalchemy.engines [req-69417794-12f3-40b6-a52b-0d2a0ae26b67 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.793 21235 WARNING oslo_db.sqlalchemy.engines [req-a33b9cea-5767-4b3a-9a44-1dc0ed03c0e0 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.796 21236 WARNING oslo_db.sqlalchemy.engines [req-4a0b916e-e34e-45d1-b697-08a04c6db7da - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.803 21237 WARNING oslo_db.sqlalchemy.engines [req-bf49a597-ffb1-412b-82cb-52e603bff01e - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.807 21238 WARNING oslo_db.sqlalchemy.engines [req-ef7e2f76-ad95-46a0-8f28-044d729f9baa - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.813 21239 WARNING oslo_db.sqlalchemy.engines [req-0aa8831d-4d0d-4101-b1df-963b2409b143 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.817 21240 WARNING oslo_db.sqlalchemy.engines [req-2f2cb658-6bce-4c64-ad92-384a49c24554 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.821 21241 WARNING oslo_db.sqlalchemy.engines [req-7f98be15-a2e5-4fa4-8d14-83eecb82a769 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.828 21242 WARNING oslo_db.sqlalchemy.engines [req-a7fdfff8-7843-48c6-9d2b-c0fa83f33fb9 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.832 21243 WARNING oslo_db.sqlalchemy.engines [req-703039ff-4261-40fb-8ce9-ba24b24c03fe - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.837 21244 WARNING oslo_db.sqlalchemy.engines [req-a4586966-7dae-4863-b2bb-6da48c95b8f4 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.842 21245 WARNING oslo_db.sqlalchemy.engines [req-78258274-9b52-4199-9a60-17292e7cd77e - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.848 21246 WARNING oslo_db.sqlalchemy.engines [req-003e9ec8-a50e-46c1-9b43-f98d0b9f2f5c - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.852 21247 WARNING oslo_db.sqlalchemy.engines [req-53ed7982-821e-46ad-9bb8-32c10edd3a20 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.857 21248 WARNING oslo_db.sqlalchemy.engines [req-363eb3c6-e317-4f1a-92e3-965f4f7e400e - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.863 21249 WARNING oslo_db.sqlalchemy.engines [req-079e6ae8-c841-487d-b906-b2b31205fc57 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.868 21250 WARNING oslo_db.sqlalchemy.engines [req-1873eac0-e71d-4f91-940b-df29ac29034a - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.874 21251 WARNING oslo_db.sqlalchemy.engines [req-f7bcf8cc-25d0-4a9f-b7fd-c449977b0376 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.883 21252 WARNING oslo_db.sqlalchemy.engines [req-ac09731e-b4c7-4516-b6ad-8396d970581d - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.887 21253 WARNING oslo_db.sqlalchemy.engines [req-e92a1f35-6cc4-4bc7-a1e2-3b7b3b37b1bc - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.893 21254 WARNING oslo_db.sqlalchemy.engines [req-b11aedd6-2580-4028-b46d-135f23bf2e12 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.897 21255 WARNING oslo_db.sqlalchemy.engines [req-15912667-0ec0-4a61-bc78-d7ebe721ecd8 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.903 21256 WARNING oslo_db.sqlalchemy.engines [req-1708383a-aa5c-491a-ad9a-439404d370ab - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.908 21257 WARNING oslo_db.sqlalchemy.engines [req-bcd0ee64-b57d-4761-b36c-c35fd1c4c3c6 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.914 21258 WARNING oslo_db.sqlalchemy.engines [req-f09c8003-606e-4637-8c6f-81194020c3ba - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.921 21259 WARNING oslo_db.sqlalchemy.engines [req-f862be45-d0f4-4d8a-9d6d-b171bca351d6 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.925 21260 WARNING oslo_db.sqlalchemy.engines [req-a5ab6fbf-f7ed-46b7-85db-0612169ce125 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:24:54.934 21261 WARNING oslo_db.sqlalchemy.engines [req-29db3350-507f-4bef-90d1-45948092ce94 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 100  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 101  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 102  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 103  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 104  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 105  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 106  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 107  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 108  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 109  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 10  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 110  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 111  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 112  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 113  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 114  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 115  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 116  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 117  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 118  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 119  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 11  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 120  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 121  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 122  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 123  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 124  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 125  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 127  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 128  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 129  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 12  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 130  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 131  user: 'keystone'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 132  user: 'keystone'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 133  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 134  user: 'keystone'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 135  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 136  user: 'keystone'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 137  user: 'keystone'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 138  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 139  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 13  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 140  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 141  user: 'glance'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 142  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 143  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 146  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 148  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 14  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 151  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 152  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 153  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 154  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 155  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 158  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 15  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 16  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 17  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 18  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 19  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 1  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 20  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 21  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 22  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 23  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 24  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 25  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 26  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 27  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 28  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 29  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 2  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 30  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 31  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 32  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 33  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 34  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 35  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 36  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 37  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 38  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 39  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 40  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 41  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 42  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 43  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 44  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 45  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 46  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 47  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 48  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 49  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 4  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 50  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 51  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 52  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 53  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 54  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 55  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 56  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 57  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 58  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 59  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 5  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 60  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 61  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 62  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 63  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 64  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 65  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 66  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 67  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 68  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 69  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 6  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 70  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 71  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 72  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 73  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 74  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 75  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 76  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 77  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 78  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 79  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 7  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 80  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 81  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 82  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 83  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 84  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 85  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 86  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 87  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 88  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 89  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 8  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 90  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 91  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 92  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 93  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 94  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 95  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 96  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 97  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 98  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 99  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:24:56 15093 [Warning] /usr/sbin/mysqld: Forcing close of thread 9  user: 'nova'
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:25:00.396 14587 ERROR oslo.messaging._drivers.impl_rabbit [-] [1f17823f-bb01-4cf2-b983-3c8110868f69] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: timed out. Trying again in 1 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:05.782 15469 ERROR oslo.messaging._drivers.impl_rabbit [-] [476c8c96-9020-4a13-b50d-749aeaba53cb] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: timed out. Trying again in 1 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent 
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     devices_added_updated, ovs_restarted))
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     devices=devices, agent_id=agent_id, host=host)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/neutron/agent/rpc.py", line 143, in get_devices_details_list_and_failed_devices
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/neutron/common/rpc.py", line 138, in call
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/neutron/common/rpc.py", line 157, in call
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/neutron/plugins/ml2/drivers/openvswitch/agent/ovs_neutron_agent.py", line 1500, in treat_devices_added_or_updated
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/neutron/plugins/ml2/drivers/openvswitch/agent/ovs_neutron_agent.py", line 1633, in process_network_ports
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/neutron/plugins/ml2/drivers/openvswitch/agent/ovs_neutron_agent.py", line 2047, in rpc_loop
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 238, in get
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 336, in wait
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 453, in _send
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 464, in send
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py", line 169, in call
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/transport.py", line 97, in _send
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 196, in force_reraise
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 220, in __exit__
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/osprofiler/profiler.py", line 154, in wrapper
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/osprofiler/profiler.py", line 154, in wrapper
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     message = self.waiters.get(msg_id, timeout=timeout)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent MessagingTimeout: Timed out waiting for a reply to message ID ce626c62414c4a1481f3c29d9e0bf91b
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     port_info, ovs_restarted)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [req-50676fd7-3ed3-46a4-a5ac-c28c13e66081 - - - - -] Error while processing VIF ports
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     result = self._waiter.wait(msg_id, timeout)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     retry=retry)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     retry=self.retry)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     return f(*args, **kwargs)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     return f(*args, **kwargs)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     return self._original_context.call(ctxt, method, **kwargs)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     self.conf.host))
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     self.force_reraise()
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     six.reraise(self.type_, self.value, self.tb)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     timeout=timeout, retry=retry)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     time.sleep(wait)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     'to message ID %s' % msg_id)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.719 21021 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.874 21021 ERROR neutron.agent.linux.async_process [-] Error received from [ovsdb-client monitor Interface name,ofport,external_ids --format=json]: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:25:21.875 21021 ERROR neutron.agent.linux.async_process [-] Process [ovsdb-client monitor Interface name,ofport,external_ids --format=json] dies due to the error: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:30.483 15467 ERROR oslo.messaging._drivers.impl_rabbit [-] [09e92da7-fd3d-45fa-9f9b-1fa9f8f53381] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: timed out. Trying again in 1 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:30.638 15468 ERROR oslo.messaging._drivers.impl_rabbit [-] [4c4b45d9-1bc4-4ea1-8f64-9761c92ad728] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: timed out. Trying again in 1 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api 
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     Agent.host == host).one()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     close_with_result=True)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     compat.reraise(exc_type, exc_value, exc_tb)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     conn = bind.contextual_connect()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     connection = self.__pool._invoke_creator(self)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     context, agent_state['agent_type'], agent_state['host'])
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     dbapi_connection = rec.get_connection()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api DBConnectionError: (pymysql.err.OperationalError) (2003, "Can't connect to MySQL server on 'mysql-001.vassilevski.com' ([Errno 111] ECONNREFUSED)")
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     e, dialect, self)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     engine, execution_options)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     execution_options=execution_options)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     fairy = _ConnectionRecord.checkout(pool)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 1401, in _handle_dbapi_exception_noconnection
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 2039, in contextual_connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 2074, in _wrap_pool_connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 2078, in _wrap_pool_connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/default.py", line 385, in connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/strategies.py", line 97, in connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py", line 2693, in one
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py", line 2736, in __iter__
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py", line 2740, in _connection_from_session
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py", line 2749, in _execute_and_instances
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/session.py", line 334, in _connection_for_bind
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/session.py", line 905, in connection
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/session.py", line 910, in _connection_for_bind
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py", line 376, in connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py", line 482, in checkout
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py", line 485, in checkout
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py", line 563, in get_connection
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py", line 607, in __connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py", line 713, in _checkout
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/util/compat.py", line 200, in raise_from_cause
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/util/langhelpers.py", line 60, in __exit__
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib/python2.7/site-packages/neutron/db/agents_db.py", line 316, in _get_agent_by_type_and_host
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib/python2.7/site-packages/neutron/db/agents_db.py", line 377, in create_or_update_agent
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib/python2.7/site-packages/neutron/db/api.py", line 119, in wrapped
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib/python2.7/site-packages/neutron/db/api.py", line 124, in wrapped
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib/python2.7/site-packages/oslo_db/api.py", line 139, in wrapper
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 196, in force_reraise
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 220, in __exit__
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib/python2.7/site-packages/pymysql/connections.py", line 694, in __init__
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib/python2.7/site-packages/pymysql/connections.py", line 947, in connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api   File "/usr/lib/python2.7/site-packages/pymysql/__init__.py", line 90, in Connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     **kw)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     raise exc
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     rec.checkin()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api [req-0015a0e9-629a-42e6-80da-9a2f61c91a47 - - - - -] DB exceeded retry limit.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     reraise(type(exception), exception, tb=exc_tb)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     ret = list(self)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     return Connection(*args, **kwargs)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     return _ConnectionFairy._checkout(self)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     return dialect.connect(*cargs, **cparams)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     return f(*args, **kwargs)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     return f(*dup_args, **dup_kwargs)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     return fn()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     return self.dbapi.connect(*cargs, **cparams)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     return self._execute_and_instances(context)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     self.connect()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     self.connection = self.__connect()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     self.force_reraise()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     self._wrap_pool_connect(self.pool.connect, None),
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     six.reraise(self.type_, self.value, self.tb)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     traceback.format_exc())
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.131 15467 ERROR oslo_db.api     util.raise_from_cause(newraise, exc_info)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server 
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     Agent.host == host).one()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     close_with_result=True)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     compat.reraise(exc_type, exc_value, exc_tb)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     conn = bind.contextual_connect()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     connection = self.__pool._invoke_creator(self)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     context, agent_state)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     context, agent_state['agent_type'], agent_state['host'])
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     dbapi_connection = rec.get_connection()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server DBConnectionError: (pymysql.err.OperationalError) (2003, "Can't connect to MySQL server on 'mysql-001.vassilevski.com' ([Errno 111] ECONNREFUSED)")
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     ectxt.value = e.inner_exc
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     ectxt.value = e.inner_exc
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     e, dialect, self)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     engine, execution_options)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     execution_options=execution_options)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     fairy = _ConnectionRecord.checkout(pool)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 1401, in _handle_dbapi_exception_noconnection
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 2039, in contextual_connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 2074, in _wrap_pool_connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 2078, in _wrap_pool_connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/default.py", line 385, in connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/strategies.py", line 97, in connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py", line 2693, in one
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py", line 2736, in __iter__
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py", line 2740, in _connection_from_session
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py", line 2749, in _execute_and_instances
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/session.py", line 334, in _connection_for_bind
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/session.py", line 905, in connection
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/session.py", line 910, in _connection_for_bind
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py", line 376, in connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py", line 482, in checkout
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py", line 485, in checkout
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py", line 563, in get_connection
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py", line 607, in __connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py", line 713, in _checkout
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/util/compat.py", line 200, in raise_from_cause
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/util/langhelpers.py", line 60, in __exit__
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/neutron/db/agents_db.py", line 316, in _get_agent_by_type_and_host
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/neutron/db/agents_db.py", line 377, in create_or_update_agent
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/neutron/db/agents_db.py", line 485, in report_state
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/neutron/db/api.py", line 119, in wrapped
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/neutron/db/api.py", line 119, in wrapped
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/neutron/db/api.py", line 124, in wrapped
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/neutron/db/api.py", line 124, in wrapped
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/neutron/db/api.py", line 159, in wrapped
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/neutron/db/api.py", line 159, in wrapped
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/neutron/db/api.py", line 84, in wrapped
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/neutron/db/api.py", line 84, in wrapped
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/neutron/db/api.py", line 88, in wrapped
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/neutron/db/api.py", line 88, in wrapped
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/oslo_db/api.py", line 139, in wrapper
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/oslo_db/api.py", line 139, in wrapper
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/oslo_db/api.py", line 151, in wrapper
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/oslo_db/api.py", line 151, in wrapper
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/dispatcher.py", line 121, in _do_dispatch
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/dispatcher.py", line 150, in dispatch
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/server.py", line 133, in _process_incoming
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 196, in force_reraise
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 196, in force_reraise
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 196, in force_reraise
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 196, in force_reraise
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 196, in force_reraise
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 196, in force_reraise
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 220, in __exit__
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 220, in __exit__
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 220, in __exit__
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 220, in __exit__
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 220, in __exit__
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 220, in __exit__
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/pymysql/connections.py", line 694, in __init__
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/pymysql/connections.py", line 947, in connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/pymysql/__init__.py", line 90, in Connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     **kw)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     raise exc
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     rec.checkin()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server [req-0015a0e9-629a-42e6-80da-9a2f61c91a47 - - - - -] Exception during message handling
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     reraise(type(exception), exception, tb=exc_tb)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     res = self.dispatcher.dispatch(message)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     result = func(ctxt, **new_args)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     ret = list(self)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     return Connection(*args, **kwargs)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     return _ConnectionFairy._checkout(self)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     return dialect.connect(*cargs, **cparams)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     return f(*args, **kwargs)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     return f(*args, **kwargs)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     return f(*args, **kwargs)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     return f(*args, **kwargs)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     return f(*dup_args, **dup_kwargs)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     return f(*dup_args, **dup_kwargs)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     return fn()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     return method(*args, **kwargs)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     return method(*args, **kwargs)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     return self.dbapi.connect(*cargs, **cparams)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     return self._do_dispatch(endpoint, method, ctxt, args)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     return self._execute_and_instances(context)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     self.connect()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     self.connection = self.__connect()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     self.force_reraise()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     self.force_reraise()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     self.force_reraise()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     self.force_reraise()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     self.force_reraise()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     self.force_reraise()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     self._wrap_pool_connect(self.pool.connect, None),
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     setattr(e, '_RETRY_EXCEEDED', True)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     setattr(e, '_RETRY_EXCEEDED', True)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     six.reraise(self.type_, self.value, self.tb)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     six.reraise(self.type_, self.value, self.tb)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     six.reraise(self.type_, self.value, self.tb)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     six.reraise(self.type_, self.value, self.tb)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     six.reraise(self.type_, self.value, self.tb)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     six.reraise(self.type_, self.value, self.tb)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     traceback.format_exc())
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     traceback.format_exc())
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:39.134 15467 ERROR oslo_messaging.rpc.server     util.raise_from_cause(newraise, exc_info)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api 
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     Agent.host == host).one()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     close_with_result=True)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     compat.reraise(exc_type, exc_value, exc_tb)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     connection = self.__pool._invoke_creator(self)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     conn = engine.contextual_connect(**kw)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     context, constants.AGENT_TYPE_L3, host)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     context, host, router_ids))
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     dbapi_connection = rec.get_connection()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api DBConnectionError: (pymysql.err.OperationalError) (2003, "Can't connect to MySQL server on 'mysql-001.vassilevski.com' ([Errno 111] ECONNREFUSED)")
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     e, dialect, self)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     execution_options=execution_options)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     fairy = _ConnectionRecord.checkout(pool)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 1401, in _handle_dbapi_exception_noconnection
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 2039, in contextual_connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 2074, in _wrap_pool_connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 2078, in _wrap_pool_connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/default.py", line 385, in connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/strategies.py", line 97, in connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py", line 2693, in one
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py", line 2736, in __iter__
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py", line 2740, in _connection_from_session
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py", line 2749, in _execute_and_instances
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/session.py", line 905, in connection
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/session.py", line 912, in _connection_for_bind
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py", line 376, in connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py", line 482, in checkout
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py", line 485, in checkout
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py", line 563, in get_connection
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py", line 607, in __connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py", line 713, in _checkout
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/util/compat.py", line 200, in raise_from_cause
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib64/python2.7/site-packages/sqlalchemy/util/langhelpers.py", line 60, in __exit__
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib/python2.7/site-packages/neutron/api/rpc/handlers/l3_rpc.py", line 94, in sync_routers
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib/python2.7/site-packages/neutron/db/agents_db.py", line 316, in _get_agent_by_type_and_host
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib/python2.7/site-packages/neutron/db/api.py", line 119, in wrapped
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib/python2.7/site-packages/neutron/db/api.py", line 124, in wrapped
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib/python2.7/site-packages/neutron/db/l3_agentschedulers_db.py", line 378, in list_active_sync_routers_on_active_l3_agent
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib/python2.7/site-packages/oslo_db/api.py", line 139, in wrapper
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 196, in force_reraise
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 220, in __exit__
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib/python2.7/site-packages/pymysql/connections.py", line 694, in __init__
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib/python2.7/site-packages/pymysql/connections.py", line 947, in connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api   File "/usr/lib/python2.7/site-packages/pymysql/__init__.py", line 90, in Connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     **kw)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     raise exc
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     rec.checkin()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api [req-7805e75e-f51b-4733-8013-b94406ae13ec - - - - -] DB exceeded retry limit.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     reraise(type(exception), exception, tb=exc_tb)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     ret = list(self)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     return Connection(*args, **kwargs)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     return _ConnectionFairy._checkout(self)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     return dialect.connect(*cargs, **cparams)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     return f(*args, **kwargs)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     return f(*dup_args, **dup_kwargs)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     return fn()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     return self.dbapi.connect(*cargs, **cparams)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     return self._execute_and_instances(context)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     self.connect()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     self.connection = self.__connect()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     self.force_reraise()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     self._wrap_pool_connect(self.pool.connect, None),
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     six.reraise(self.type_, self.value, self.tb)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     traceback.format_exc())
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.572 15467 ERROR oslo_db.api     util.raise_from_cause(newraise, exc_info)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server 
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     Agent.host == host).one()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     close_with_result=True)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     compat.reraise(exc_type, exc_value, exc_tb)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     connection = self.__pool._invoke_creator(self)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     conn = engine.contextual_connect(**kw)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     context, constants.AGENT_TYPE_L3, host)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     context, host, router_ids))
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     dbapi_connection = rec.get_connection()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server DBConnectionError: (pymysql.err.OperationalError) (2003, "Can't connect to MySQL server on 'mysql-001.vassilevski.com' ([Errno 111] ECONNREFUSED)")
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     ectxt.value = e.inner_exc
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     e, dialect, self)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     execution_options=execution_options)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     fairy = _ConnectionRecord.checkout(pool)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 1401, in _handle_dbapi_exception_noconnection
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 2039, in contextual_connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 2074, in _wrap_pool_connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 2078, in _wrap_pool_connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/default.py", line 385, in connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/strategies.py", line 97, in connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py", line 2693, in one
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py", line 2736, in __iter__
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py", line 2740, in _connection_from_session
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py", line 2749, in _execute_and_instances
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/session.py", line 905, in connection
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/session.py", line 912, in _connection_for_bind
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py", line 376, in connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py", line 482, in checkout
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py", line 485, in checkout
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py", line 563, in get_connection
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py", line 607, in __connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py", line 713, in _checkout
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/util/compat.py", line 200, in raise_from_cause
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib64/python2.7/site-packages/sqlalchemy/util/langhelpers.py", line 60, in __exit__
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/neutron/api/rpc/handlers/l3_rpc.py", line 94, in sync_routers
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/neutron/db/agents_db.py", line 316, in _get_agent_by_type_and_host
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/neutron/db/api.py", line 119, in wrapped
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/neutron/db/api.py", line 124, in wrapped
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/neutron/db/api.py", line 84, in wrapped
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/neutron/db/api.py", line 88, in wrapped
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/neutron/db/l3_agentschedulers_db.py", line 378, in list_active_sync_routers_on_active_l3_agent
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/oslo_db/api.py", line 139, in wrapper
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/oslo_db/api.py", line 151, in wrapper
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/dispatcher.py", line 121, in _do_dispatch
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/dispatcher.py", line 150, in dispatch
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/server.py", line 133, in _process_incoming
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 196, in force_reraise
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 196, in force_reraise
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 196, in force_reraise
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 220, in __exit__
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 220, in __exit__
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 220, in __exit__
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/pymysql/connections.py", line 694, in __init__
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/pymysql/connections.py", line 947, in connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server   File "/usr/lib/python2.7/site-packages/pymysql/__init__.py", line 90, in Connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     **kw)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     raise exc
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     rec.checkin()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server [req-7805e75e-f51b-4733-8013-b94406ae13ec - - - - -] Exception during message handling
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     reraise(type(exception), exception, tb=exc_tb)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     res = self.dispatcher.dispatch(message)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     result = func(ctxt, **new_args)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     ret = list(self)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     return Connection(*args, **kwargs)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     return _ConnectionFairy._checkout(self)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     return dialect.connect(*cargs, **cparams)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     return f(*args, **kwargs)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     return f(*args, **kwargs)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     return f(*dup_args, **dup_kwargs)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     return fn()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     return self.dbapi.connect(*cargs, **cparams)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     return self._do_dispatch(endpoint, method, ctxt, args)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     return self._execute_and_instances(context)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     self.connect()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     self.connection = self.__connect()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     self.force_reraise()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     self.force_reraise()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     self.force_reraise()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     self._wrap_pool_connect(self.pool.connect, None),
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     setattr(e, '_RETRY_EXCEEDED', True)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     six.reraise(self.type_, self.value, self.tb)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     six.reraise(self.type_, self.value, self.tb)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     six.reraise(self.type_, self.value, self.tb)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     traceback.format_exc())
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:25:49.574 15467 ERROR oslo_messaging.rpc.server     util.raise_from_cause(newraise, exc_info)
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:30:26 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:30:26 24274 [Warning] Buffered warning: Changed limits: max_connections: 214 (requested 500)
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:30:26 24274 [Warning] Buffered warning: Changed limits: max_open_files: 1024 (requested 5000)
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 08:30:26 24274 [Warning] Buffered warning: Changed limits: table_open_cache: 400 (requested 2000)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:30:26.090 23815 WARNING neutron.agent.securitygroups_rpc [-] Driver configuration doesn't match with enable_security_group
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:30:26.214 23837 WARNING stevedore.named [req-a70d9fc1-09d2-4a2b-acd3-c5b07477311c - - - - -] Could not load neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
/root/openstack/logs/2017-02-09/1428/nova-novncproxy.log:2017-02-09 08:30:26.217 23811 WARNING oslo_reports.guru_meditation_report [-] Guru meditation now registers SIGUSR1 and SIGUSR2 by default for backward compatibility. SIGUSR1 will no longer be registered in a future release, so please use SIGUSR2 to generate reports.
/root/openstack/logs/2017-02-09/1428/nova-scheduler.log:2017-02-09 08:30:27.278 23807 WARNING oslo_reports.guru_meditation_report [-] Guru meditation now registers SIGUSR1 and SIGUSR2 by default for backward compatibility. SIGUSR1 will no longer be registered in a future release, so please use SIGUSR2 to generate reports.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 08:30:27.294 23809 WARNING oslo_reports.guru_meditation_report [-] Guru meditation now registers SIGUSR1 and SIGUSR2 by default for backward compatibility. SIGUSR1 will no longer be registered in a future release, so please use SIGUSR2 to generate reports.
/root/openstack/logs/2017-02-09/1428/nova-consoleauth.log:2017-02-09 08:30:27.296 23805 WARNING oslo_reports.guru_meditation_report [-] Guru meditation now registers SIGUSR1 and SIGUSR2 by default for backward compatibility. SIGUSR1 will no longer be registered in a future release, so please use SIGUSR2 to generate reports.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 08:30:27.318 23803 WARNING oslo_reports.guru_meditation_report [-] Guru meditation now registers SIGUSR1 and SIGUSR2 by default for backward compatibility. SIGUSR1 will no longer be registered in a future release, so please use SIGUSR2 to generate reports.
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:30:27.366 23845 WARNING oslo_reports.guru_meditation_report [-] Guru meditation now registers SIGUSR1 and SIGUSR2 by default for backward compatibility. SIGUSR1 will no longer be registered in a future release, so please use SIGUSR2 to generate reports.
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:30:27.757 23845 WARNING os_brick.initiator.connectors.remotefs [req-1396af95-500f-461d-a548-f6c02ff7754d - - - - -] Connection details not present. RemoteFsClient may not initialize properly.
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:30:29.161 23845 WARNING nova.compute.monitors [req-1481de14-bb9a-465a-8e68-b4078336d6c0 - - - - -] Excluding nova.compute.monitors.cpu monitor virt_driver. Not in the list of enabled monitors (CONF.compute_monitors).
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 08:30:29.777 23845 WARNING nova.scheduler.client.report [req-1481de14-bb9a-465a-8e68-b4078336d6c0 - - - - -] No authentication information found for placement API. Optional use of placement API for reporting is now disabled.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:30:31.431 23815 WARNING neutron.agent.securitygroups_rpc [req-b7f04e07-37d0-4b42-a5b3-a6321adeea0f - - - - -] Driver configuration doesn't match with enable_security_group
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:30:31.492 23815 WARNING neutron.api.extensions [req-b7f04e07-37d0-4b42-a5b3-a6321adeea0f - - - - -] Extension dns-integration not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:30:31.501 23815 WARNING neutron.api.extensions [req-b7f04e07-37d0-4b42-a5b3-a6321adeea0f - - - - -] Extension ip_allocation not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:30:31.502 23815 WARNING neutron.api.extensions [req-b7f04e07-37d0-4b42-a5b3-a6321adeea0f - - - - -] Extension l2_adjacency not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:30:31.508 23815 WARNING neutron.api.extensions [req-b7f04e07-37d0-4b42-a5b3-a6321adeea0f - - - - -] Extension metering not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:30:31.518 23815 WARNING neutron.api.extensions [req-b7f04e07-37d0-4b42-a5b3-a6321adeea0f - - - - -] Extension qos not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:30:31.522 23815 WARNING neutron.api.extensions [req-b7f04e07-37d0-4b42-a5b3-a6321adeea0f - - - - -] Extension router-service-type not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:30:31.525 23815 WARNING neutron.api.extensions [req-b7f04e07-37d0-4b42-a5b3-a6321adeea0f - - - - -] Extension segment not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:30:31.531 23815 WARNING neutron.api.extensions [req-b7f04e07-37d0-4b42-a5b3-a6321adeea0f - - - - -] Extension trunk not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:30:31.532 23815 WARNING neutron.api.extensions [req-b7f04e07-37d0-4b42-a5b3-a6321adeea0f - - - - -] Extension trunk-details not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:30:31.533 23815 WARNING neutron.api.extensions [req-b7f04e07-37d0-4b42-a5b3-a6321adeea0f - - - - -] Extension vlan-transparent not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:30:32.560 24695 WARNING neutron.api.rpc.agentnotifiers.dhcp_rpc_agent_api [req-a70d9fc1-09d2-4a2b-acd3-c5b07477311c - - - - -] Only 0 of 1 DHCP agents associated with network '584d9ef6-3673-4cd5-beb9-87b3158d953a' are marked as active, so notifications may be sent to inactive agents.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:30:32.703 24695 WARNING neutron.api.rpc.agentnotifiers.dhcp_rpc_agent_api [req-a70d9fc1-09d2-4a2b-acd3-c5b07477311c - - - - -] Only 0 of 1 DHCP agents associated with network '584d9ef6-3673-4cd5-beb9-87b3158d953a' are marked as active, so notifications may be sent to inactive agents.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:30:32.881 24695 WARNING neutron.api.rpc.agentnotifiers.dhcp_rpc_agent_api [req-a70d9fc1-09d2-4a2b-acd3-c5b07477311c - - - - -] Only 0 of 1 DHCP agents associated with network '2eb2a376-5200-4093-8bb9-7eb4c263a96a' are marked as active, so notifications may be sent to inactive agents.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:30:33.007 24695 WARNING neutron.api.rpc.agentnotifiers.dhcp_rpc_agent_api [req-a70d9fc1-09d2-4a2b-acd3-c5b07477311c - - - - -] Only 0 of 1 DHCP agents associated with network '584d9ef6-3673-4cd5-beb9-87b3158d953a' are marked as active, so notifications may be sent to inactive agents.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:30:33.551 24695 WARNING neutron.api.rpc.agentnotifiers.dhcp_rpc_agent_api [req-a70d9fc1-09d2-4a2b-acd3-c5b07477311c - - - - -] Only 0 of 1 DHCP agents associated with network '584d9ef6-3673-4cd5-beb9-87b3158d953a' are marked as active, so notifications may be sent to inactive agents.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:30:33.773 24695 WARNING neutron.api.rpc.agentnotifiers.dhcp_rpc_agent_api [req-a70d9fc1-09d2-4a2b-acd3-c5b07477311c - - - - -] Only 0 of 1 DHCP agents associated with network '584d9ef6-3673-4cd5-beb9-87b3158d953a' are marked as active, so notifications may be sent to inactive agents.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:30:34.157 24695 WARNING neutron.api.rpc.agentnotifiers.dhcp_rpc_agent_api [req-a70d9fc1-09d2-4a2b-acd3-c5b07477311c - - - - -] Only 0 of 1 DHCP agents associated with network '2eb2a376-5200-4093-8bb9-7eb4c263a96a' are marked as active, so notifications may be sent to inactive agents.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:30:34.361 24695 WARNING neutron.api.rpc.agentnotifiers.dhcp_rpc_agent_api [req-a70d9fc1-09d2-4a2b-acd3-c5b07477311c - - - - -] Only 0 of 1 DHCP agents associated with network '584d9ef6-3673-4cd5-beb9-87b3158d953a' are marked as active, so notifications may be sent to inactive agents.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 08:31:25.762 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 4 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/l3-agent.log:2017-02-09 08:31:30.782 23819 ERROR neutron.common.rpc [req-9db29f13-e4af-419a-9e49-b99c22eb6472 - - - - -] Timeout in RPC method get_service_plugin_list. Waiting for 31 seconds before next attempt. If the server is not down, consider increasing the rpc_response_timeout option as Neutron server(s) may be overloaded and unable to respond quickly enough.
/root/openstack/logs/2017-02-09/1428/l3-agent.log:2017-02-09 08:31:30.784 23819 WARNING neutron.common.rpc [req-9db29f13-e4af-419a-9e49-b99c22eb6472 - - - - -] Increasing timeout for get_service_plugin_list calls to 120 seconds. Restart the agent to restore it to the default value.
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 08:31:30.812 23817 ERROR neutron.agent.metadata.agent 
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 08:31:30.812 23817 ERROR neutron.agent.metadata.agent [-] Failed reporting state!
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 08:31:30.812 23817 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/metadata/agent.py", line 262, in _report_state
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 08:31:30.812 23817 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/rpc.py", line 88, in report_state
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 08:31:30.812 23817 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 238, in get
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 08:31:30.812 23817 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 336, in wait
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 08:31:30.812 23817 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 453, in _send
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 08:31:30.812 23817 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 464, in send
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 08:31:30.812 23817 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py", line 169, in call
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 08:31:30.812 23817 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/transport.py", line 97, in _send
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 08:31:30.812 23817 ERROR neutron.agent.metadata.agent     message = self.waiters.get(msg_id, timeout=timeout)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 08:31:30.812 23817 ERROR neutron.agent.metadata.agent MessagingTimeout: Timed out waiting for a reply to message ID 3dc8f280cbeb4146b76df4411111cef0
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 08:31:30.812 23817 ERROR neutron.agent.metadata.agent     result = self._waiter.wait(msg_id, timeout)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 08:31:30.812 23817 ERROR neutron.agent.metadata.agent     retry=retry)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 08:31:30.812 23817 ERROR neutron.agent.metadata.agent     retry=self.retry)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 08:31:30.812 23817 ERROR neutron.agent.metadata.agent     return method(context, 'report_state', **kwargs)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 08:31:30.812 23817 ERROR neutron.agent.metadata.agent     timeout=timeout, retry=retry)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 08:31:30.812 23817 ERROR neutron.agent.metadata.agent     'to message ID %s' % msg_id)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 08:31:30.812 23817 ERROR neutron.agent.metadata.agent Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 08:31:30.812 23817 ERROR neutron.agent.metadata.agent     use_call=self.agent_state.get('start_flag'))
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 08:31:30.823 23817 WARNING oslo.service.loopingcall [-] Function 'neutron.agent.metadata.agent.UnixDomainMetadataProxy._report_state' run outlasted interval by 35.06 sec
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:31:30.825 23816 ERROR neutron.agent.dhcp.agent 
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:31:30.825 23816 ERROR neutron.agent.dhcp.agent     ctx, self.agent_state, True)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:31:30.825 23816 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/dhcp/agent.py", line 687, in _report_state
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:31:30.825 23816 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/rpc.py", line 88, in report_state
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:31:30.825 23816 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 238, in get
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:31:30.825 23816 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 336, in wait
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:31:30.825 23816 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 453, in _send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:31:30.825 23816 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 464, in send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:31:30.825 23816 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py", line 169, in call
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:31:30.825 23816 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/transport.py", line 97, in _send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:31:30.825 23816 ERROR neutron.agent.dhcp.agent     message = self.waiters.get(msg_id, timeout=timeout)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:31:30.825 23816 ERROR neutron.agent.dhcp.agent MessagingTimeout: Timed out waiting for a reply to message ID ef20734bba404eccb50c1b8537ad1107
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:31:30.825 23816 ERROR neutron.agent.dhcp.agent [req-9ef93eff-94d0-40dd-b184-0550f81b70c1 - - - - -] Failed reporting state!
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:31:30.825 23816 ERROR neutron.agent.dhcp.agent     result = self._waiter.wait(msg_id, timeout)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:31:30.825 23816 ERROR neutron.agent.dhcp.agent     retry=retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:31:30.825 23816 ERROR neutron.agent.dhcp.agent     retry=self.retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:31:30.825 23816 ERROR neutron.agent.dhcp.agent     return method(context, 'report_state', **kwargs)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:31:30.825 23816 ERROR neutron.agent.dhcp.agent     timeout=timeout, retry=retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:31:30.825 23816 ERROR neutron.agent.dhcp.agent     'to message ID %s' % msg_id)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:31:30.825 23816 ERROR neutron.agent.dhcp.agent Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:31:30.832 23816 WARNING oslo.service.loopingcall [req-9ef93eff-94d0-40dd-b184-0550f81b70c1 - - - - -] Function 'neutron.agent.dhcp.agent.DhcpAgentWithStateReport._report_state' run outlasted interval by 35.07 sec
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:31:30.835 23816 ERROR neutron.common.rpc [req-3bbec363-a8f1-48bc-b00c-df3ecb585a3c - - - - -] Timeout in RPC method get_active_networks_info. Waiting for 45 seconds before next attempt. If the server is not down, consider increasing the rpc_response_timeout option as Neutron server(s) may be overloaded and unable to respond quickly enough.
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:31:30.835 23816 WARNING neutron.common.rpc [req-3bbec363-a8f1-48bc-b00c-df3ecb585a3c - - - - -] Increasing timeout for get_active_networks_info calls to 120 seconds. Restart the agent to restore it to the default value.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:31:31.335 23837 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent 
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:31:31.335 23837 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Failed reporting state!
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:31:31.335 23837 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/neutron/agent/rpc.py", line 88, in report_state
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:31:31.335 23837 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/neutron/plugins/ml2/drivers/openvswitch/agent/ovs_neutron_agent.py", line 320, in _report_state
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:31:31.335 23837 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 238, in get
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:31:31.335 23837 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 336, in wait
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:31:31.335 23837 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 453, in _send
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:31:31.335 23837 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 464, in send
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:31:31.335 23837 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py", line 169, in call
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:31:31.335 23837 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/transport.py", line 97, in _send
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:31:31.335 23837 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     message = self.waiters.get(msg_id, timeout=timeout)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:31:31.335 23837 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent MessagingTimeout: Timed out waiting for a reply to message ID 615e9ca11d2640b9849fa3845f0e8867
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:31:31.335 23837 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     result = self._waiter.wait(msg_id, timeout)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:31:31.335 23837 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     retry=retry)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:31:31.335 23837 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     retry=self.retry)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:31:31.335 23837 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     return method(context, 'report_state', **kwargs)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:31:31.335 23837 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     timeout=timeout, retry=retry)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:31:31.335 23837 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     'to message ID %s' % msg_id)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:31:31.335 23837 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:31:31.335 23837 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     True)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 08:31:31.339 23837 WARNING oslo.service.loopingcall [-] Function 'neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent.OVSNeutronAgent._report_state' run outlasted interval by 30.06 sec
/root/openstack/logs/2017-02-09/1428/l3-agent.log:2017-02-09 08:32:02.079 23819 WARNING neutron.agent.l3.agent [req-9db29f13-e4af-419a-9e49-b99c22eb6472 - - - - -] l3-agent cannot contact neutron server to retrieve service plugins enabled. Check connectivity to neutron server. Retrying... Detailed message: Timed out waiting for a reply to message ID 870679c2bbb84879964240937e648466.
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:32:16.132 23816 ERROR neutron.agent.dhcp.agent 
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:32:16.132 23816 ERROR neutron.agent.dhcp.agent     active_networks = self.plugin_rpc.get_active_networks_info()
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:32:16.132 23816 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/dhcp/agent.py", line 159, in sync_state
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:32:16.132 23816 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/dhcp/agent.py", line 522, in get_active_networks_info
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:32:16.132 23816 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/common/rpc.py", line 138, in call
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:32:16.132 23816 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/common/rpc.py", line 157, in call
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:32:16.132 23816 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 238, in get
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:32:16.132 23816 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 336, in wait
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:32:16.132 23816 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 453, in _send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:32:16.132 23816 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 464, in send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:32:16.132 23816 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py", line 169, in call
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:32:16.132 23816 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/transport.py", line 97, in _send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:32:16.132 23816 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 196, in force_reraise
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:32:16.132 23816 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 220, in __exit__
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:32:16.132 23816 ERROR neutron.agent.dhcp.agent     host=self.host)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:32:16.132 23816 ERROR neutron.agent.dhcp.agent     message = self.waiters.get(msg_id, timeout=timeout)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:32:16.132 23816 ERROR neutron.agent.dhcp.agent MessagingTimeout: Timed out waiting for a reply to message ID 30f4c44c23ed44d99b71d7f49988c006
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:32:16.132 23816 ERROR neutron.agent.dhcp.agent [req-3bbec363-a8f1-48bc-b00c-df3ecb585a3c - - - - -] Unable to sync network state.
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:32:16.132 23816 ERROR neutron.agent.dhcp.agent     result = self._waiter.wait(msg_id, timeout)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:32:16.132 23816 ERROR neutron.agent.dhcp.agent     retry=retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:32:16.132 23816 ERROR neutron.agent.dhcp.agent     retry=self.retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:32:16.132 23816 ERROR neutron.agent.dhcp.agent     return self._original_context.call(ctxt, method, **kwargs)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:32:16.132 23816 ERROR neutron.agent.dhcp.agent     self.force_reraise()
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:32:16.132 23816 ERROR neutron.agent.dhcp.agent     six.reraise(self.type_, self.value, self.tb)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:32:16.132 23816 ERROR neutron.agent.dhcp.agent     timeout=timeout, retry=retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:32:16.132 23816 ERROR neutron.agent.dhcp.agent     time.sleep(wait)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:32:16.132 23816 ERROR neutron.agent.dhcp.agent     'to message ID %s' % msg_id)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:32:16.132 23816 ERROR neutron.agent.dhcp.agent Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 08:32:16.814 23816 WARNING oslo.service.loopingcall [req-7c392df2-3b63-4151-a010-4f6e3b99a365 - - - - -] Function 'neutron.agent.dhcp.agent.DhcpAgentWithStateReport._report_state' run outlasted interval by 15.98 sec
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 10:48:19.996 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 10:48:57.002 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 10:49:33.994 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 10:50:10.997 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 10:50:48.001 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 10:51:25.006 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 10:52:02.007 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 10:52:39.009 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 10:53:16.010 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 10:53:53.011 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 10:54:30.016 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 10:55:07.017 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 10:55:44.021 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 10:56:21.023 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 10:56:58.024 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 10:57:35.029 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 10:58:12.021 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 10:58:49.022 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 10:59:26.027 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:00:03.031 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:00:40.033 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:01:17.034 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:01:54.035 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:02:31.036 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:03:08.041 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:03:45.031 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:04:22.032 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:04:59.036 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:05:36.037 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:06:13.042 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:06:50.033 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:07:27.034 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:08:04.039 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:08:41.040 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:09:18.042 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:09:55.043 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:10:32.044 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:11:09.057 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:11:46.048 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:12:23.050 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:13:00.055 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:13:37.056 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:14:14.060 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:14:51.061 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:15:28.063 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:16:05.068 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:16:42.061 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:17:19.062 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:17:56.063 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:18:33.063 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:19:10.068 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:19:47.068 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:20:24.069 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:21:01.075 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:21:38.067 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:22:15.068 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:22:52.070 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:23:29.073 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:24:06.079 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:24:43.071 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:25:20.073 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:25:57.074 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:26:34.075 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:27:11.080 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:27:48.073 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:28:25.073 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:29:02.077 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:29:39.080 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:30:16.081 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:30:53.082 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:31:30.087 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:32:07.079 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:32:44.085 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:33:21.091 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:33:58.086 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:34:35.077 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:35:12.079 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:35:49.083 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:36:26.084 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:37:03.089 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:37:40.093 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:38:17.094 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:38:54.098 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:39:31.099 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:40:08.100 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:40:45.106 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:41:22.097 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:41:59.102 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:42:36.096 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:43:13.098 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:43:50.102 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:44:27.103 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:45:04.108 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:45:41.109 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:46:18.110 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:46:55.111 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:47:32.115 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:48:09.120 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:48:46.126 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:49:23.121 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:50:00.122 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:50:37.123 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:51:14.123 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:51:51.124 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:52:28.129 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:53:05.135 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:53:42.131 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:54:19.126 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:54:56.127 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:55:33.127 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:56:10.128 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:56:47.128 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:57:24.133 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:58:01.134 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:58:38.135 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:59:15.141 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 11:59:52.132 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:00:29.136 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:01:06.137 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:01:43.138 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:02:20.143 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:02:57.145 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:03:34.149 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:04:11.154 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:04:48.155 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:05:25.156 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:06:02.157 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:06:39.159 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:07:16.165 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:07:53.161 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:08:30.153 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:09:07.154 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:09:44.159 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:10:21.160 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:10:58.161 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:11:35.162 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:12:12.163 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:12:49.167 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:13:26.168 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:14:03.168 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:14:40.173 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:15:17.173 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:15:54.177 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:16:31.178 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:17:08.179 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:17:45.185 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:18:22.178 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:18:59.182 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:19:36.187 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:20:13.188 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:20:50.193 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:21:27.194 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:22:04.196 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:22:41.202 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:23:18.193 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:23:55.194 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:24:32.194 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:25:09.195 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:25:46.200 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:26:23.201 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:27:00.203 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:27:37.208 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:28:14.214 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:28:51.210 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:29:28.202 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:30:05.203 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:30:42.208 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:31:19.199 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:31:56.205 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:32:33.201 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:33:10.206 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:33:47.202 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:34:24.206 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:35:01.208 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:35:38.212 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:36:15.213 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:36:52.215 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:37:29.219 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:38:06.221 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:38:43.222 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:39:20.224 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:39:57.225 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:40:34.230 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:41:11.232 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:41:48.233 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:42:25.237 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:43:02.238 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:43:39.243 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:44:16.244 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:44:53.245 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:45:30.250 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:46:07.251 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:46:44.253 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:47:21.254 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:47:58.256 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:48:35.260 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:49:12.264 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:49:49.269 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:50:26.273 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:51:03.278 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:51:40.279 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:52:17.280 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:52:54.286 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:53:31.280 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:54:08.280 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:54:45.281 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:55:22.282 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:55:59.287 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:56:36.291 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:57:13.292 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:57:50.293 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:58:27.298 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:59:04.298 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 12:59:41.299 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:00:18.303 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:00:55.304 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:01:32.307 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:02:09.308 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:02:46.313 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:03:23.307 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:04:00.312 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:04:37.312 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:05:14.314 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:05:51.320 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:06:28.326 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:07:05.321 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:07:42.325 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:08:19.326 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:08:56.330 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:09:33.336 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:10:10.328 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:10:47.332 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:11:24.332 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:12:01.333 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:12:38.337 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:13:15.338 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:13:52.339 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:14:29.340 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:15:06.344 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:15:43.348 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:16:20.344 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:16:57.344 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:17:34.345 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:18:11.349 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:18:48.350 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:19:25.351 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:20:02.354 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:20:39.355 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:21:16.360 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:21:53.361 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:22:30.362 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:23:07.366 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:23:44.366 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:24:21.368 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:24:58.369 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:25:17.305 23837 ERROR neutron.agent.linux.async_process [-] Error received from [ovsdb-client monitor Interface name,ofport,external_ids --format=json]: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:25:17.306 23837 ERROR neutron.agent.linux.async_process [-] Process [ovsdb-client monitor Interface name,ofport,external_ids --format=json] dies due to the error: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:25:20.523 20259 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip '192.168.0.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:25:21.962 20283 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip '192.168.0.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:25:23.295 20305 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip '192.168.0.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:25:24.766 20325 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip '192.168.0.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:25:26.200 20348 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip '192.168.0.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:25:35.380 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:26:12.376 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 13:26:24.242 23845 ERROR oslo.messaging._drivers.impl_rabbit [-] [ae6a4228-1305-4605-8f03-4e9ff8008524] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: timed out. Trying again in 1 seconds. Client port: 36848
/root/openstack/logs/2017-02-09/1428/l3-agent.log:2017-02-09 13:26:32.302 23819 ERROR oslo.messaging._drivers.impl_rabbit [-] [d9aa4750-667f-4a37-a3ba-00acd8eb982d] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: timed out. Trying again in 1 seconds. Client port: 36118
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 13:26:32.855 23817 ERROR oslo.messaging._drivers.impl_rabbit [-] [381aabeb-070d-4021-8655-e534c60b73a3] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: timed out. Trying again in 1 seconds. Client port: 36124
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:26:34.247 24306 ERROR oslo.messaging._drivers.impl_rabbit [-] [d015fdbb-9257-48e2-8b48-f365152821c3] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 35854
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:26:42.620 24350 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-consoleauth.log:2017-02-09 13:26:43.445 23805 ERROR oslo.messaging._drivers.impl_rabbit [-] [7cbf929b-ff00-4f81-8707-be0aaf3be608] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 35856
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:26:43.448 24350 ERROR oslo.messaging._drivers.impl_rabbit [-] [48a80cc4-117e-4fc7-ba24-9264f1a101a3] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 35904
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:26:43.457 24357 ERROR oslo.messaging._drivers.impl_rabbit [-] [d5bd6336-7adc-47a4-822a-5c7d4f44be77] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 35906
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:26:43.464 24312 ERROR oslo.messaging._drivers.impl_rabbit [-] [f2187c93-6993-4d9c-b9da-087725684ada] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 35910
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:26:44.248 24305 ERROR oslo.messaging._drivers.impl_rabbit [-] [47efdf84-8b86-4a1a-9206-57bd8f98503a] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 35838
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 13:26:45.618 23845 ERROR oslo.messaging._drivers.impl_rabbit [-] [f020058f-eee6-4a4b-a881-c32b8210172b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 36114
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:26:46.154 24306 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 13:26:46.371 23817 ERROR oslo.messaging._drivers.impl_rabbit [-] [2d211639-a7d3-48c9-beec-2756d95dfcc2] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 36120
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:26:46.693 24305 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:26:47.063 23816 ERROR oslo.messaging._drivers.impl_rabbit [req-92995bfb-c5ae-4122-ab03-7f6a19adc61a - - - - -] [1f580150-6f1f-4c91-89a2-ec395d0acc53] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: timed out. Trying again in 1 seconds. Client port: 36128
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:26:47.287 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [e97b79f4-09ec-49bd-a477-96792fa6e425] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 36158
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:26:47.368 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [26c7d5ce-673b-4656-870b-54703531d40b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 36166
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:26:47.782 23816 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:26:49.367 24697 WARNING neutron.db.agents_db [req-81bfd8d6-e408-4e42-9838-4ea1aab5e372 - - - - -] Agent healthcheck: found 5 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:26:52.148 24357 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:26:52.575 24312 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:26:54.249 24317 ERROR oslo.messaging._drivers.impl_rabbit [-] [cb03c40f-92b0-4301-95a6-5bb629dc7d92] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 35864
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 13:27:00.421 23845 ERROR oslo.messaging._drivers.impl_rabbit [req-58f1d304-bba0-4c0f-bf5d-fab87d6ba21a - - - - -] [e73aa0d6-6744-4723-b5fe-3351b4b3140d] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: timed out. Trying again in 1 seconds. Client port: 35874
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:27:00.430 24321 ERROR oslo.messaging._drivers.impl_rabbit [-] [85c90a60-c8ee-4fd7-9dde-01d118fe90a2] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 35858
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:27:00.477 24316 ERROR oslo.messaging._drivers.impl_rabbit [-] [9a3a8bf9-9b7f-464e-abb3-4237689623a9] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 35860
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:27:00.699 24319 ERROR oslo.messaging._drivers.impl_rabbit [-] [91fbe2d9-fa9b-4db6-bcd0-d5d8672fbadd] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 35852
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:27:01.649 24311 ERROR oslo.messaging._drivers.impl_rabbit [-] [56aceace-ab4f-4501-86e7-56c6109cf4ce] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 35868
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:27:01.731 24317 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:27:01.810 24321 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:27:01.844 24316 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:27:01.968 24319 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:27:01.999 24311 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/l3-agent.log:2017-02-09 13:27:02.357 23819 ERROR oslo.messaging._drivers.impl_rabbit [-] [348059c5-98ef-4574-a81e-f5b706a10d99] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 36116
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:27:02.580 24337 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:27:04.247 24313 ERROR oslo.messaging._drivers.impl_rabbit [-] [f5eefaac-eecf-4c20-bfbb-66af1e21acae] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 35870
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:27:06.168 24328 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:27:06.215 24335 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:27:06.232 24331 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:27:07.131 24365 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:27:07.143 24313 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:27:07.191 24343 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:27:07.239 24368 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:27:07.332 24307 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:27:07.434 24320 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:27:07.496 24355 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:27:08.918 24696 ERROR oslo.messaging._drivers.impl_rabbit [-] [8d4e5752-ea9c-451b-82ce-89226f3bb2fc] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 36160
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:27:10.428 24315 ERROR oslo.messaging._drivers.impl_rabbit [-] [2ed26246-112c-4d6b-baf5-b348b57472a3] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 35842
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.279 20419 WARNING oslo_db.sqlalchemy.engines [req-9b5cf0e9-9c3e-4a35-b0ce-4a5dfbbc48ee - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.283 20423 WARNING oslo_db.sqlalchemy.engines [req-46ee4d49-c6bf-4c17-adac-b022d428bb8e - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.283 20424 WARNING oslo_db.sqlalchemy.engines [req-cf1ec4fc-d587-45e1-a4e4-e6238d5a2407 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.284 20422 WARNING oslo_db.sqlalchemy.engines [req-1ae52049-4633-48a5-af58-59c3242acc66 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.286 20420 WARNING oslo_db.sqlalchemy.engines [req-89e72d47-2e08-468d-bbdb-2e2bb2a1f7e3 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.291 20429 WARNING oslo_db.sqlalchemy.engines [req-e63d1fb8-0e4c-4ff9-a701-0cabd59816c6 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.294 20421 WARNING oslo_db.sqlalchemy.engines [req-6cf8c1f2-fd03-4c30-89b7-1a9fe8acfd2b - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.303 20431 WARNING oslo_db.sqlalchemy.engines [req-7da7dee4-7560-4dfa-b430-7342076cec32 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.308 20430 WARNING oslo_db.sqlalchemy.engines [req-f0f6f1dc-7cf1-4490-9e6a-0e0441756e90 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.312 20432 WARNING oslo_db.sqlalchemy.engines [req-b3eda99a-0cdb-4dd8-9e37-58e67caa458a - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.320 20435 WARNING oslo_db.sqlalchemy.engines [req-d81d8ded-281b-490b-966f-33e13661a193 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.330 20433 WARNING oslo_db.sqlalchemy.engines [req-f496c558-a5e2-440b-ab78-442627cecb7e - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.330 20436 WARNING oslo_db.sqlalchemy.engines [req-3dc8bebf-c297-4650-b7c2-2d7f0a5154d9 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.331 20434 WARNING oslo_db.sqlalchemy.engines [req-7676b1c0-90a1-4e22-950b-facf7e7e72fa - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.332 20437 WARNING oslo_db.sqlalchemy.engines [req-741cddc7-2810-4e07-8cf4-1433ead5e0a7 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.338 20438 WARNING oslo_db.sqlalchemy.engines [req-7ddb9f54-65ab-4413-ac12-fe09e4b0170b - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.347 20439 WARNING oslo_db.sqlalchemy.engines [req-587bbdb0-199b-47f7-8ce0-03146ed03219 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.357 20440 WARNING oslo_db.sqlalchemy.engines [req-6f74d403-564e-417f-8d88-b5dde8301f1f - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.366 20441 WARNING oslo_db.sqlalchemy.engines [req-629c2e0f-9141-4ace-940b-1270d376218a - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.370 20442 WARNING oslo_db.sqlalchemy.engines [req-55c1aed2-47d4-49dd-926e-f7ebc72b0a8b - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.375 20443 WARNING oslo_db.sqlalchemy.engines [req-8ec3102e-66b6-493c-b9fe-417794a60d4c - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.382 20444 WARNING oslo_db.sqlalchemy.engines [req-3413eab9-d124-4300-890c-e7d2958259dc - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.390 20445 WARNING oslo_db.sqlalchemy.engines [req-a1f5d259-ec1e-43f0-8392-20b0cc8107e4 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.403 20447 WARNING oslo_db.sqlalchemy.engines [req-2d62cd4e-8281-424d-83b6-6062bef4bd01 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.404 20446 WARNING oslo_db.sqlalchemy.engines [req-26415fb7-c5b8-4dbf-9451-300351711cac - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.406 20448 WARNING oslo_db.sqlalchemy.engines [req-875b3fbd-5e9c-493d-9599-132717dbfde1 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.411 20452 WARNING oslo_db.sqlalchemy.engines [req-ad4e9d83-d3b6-45a4-bf9c-0cbf4be431da - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.417 20453 WARNING oslo_db.sqlalchemy.engines [req-4a767b9a-9fea-4f3c-8e8d-a137f0e8f442 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.423 20454 WARNING oslo_db.sqlalchemy.engines [req-77fa9166-5857-4dab-be72-615e26fc2d1d - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.431 20455 WARNING oslo_db.sqlalchemy.engines [req-4ba65a86-9cdb-4ff9-85d9-a938a13accb4 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.435 20456 WARNING oslo_db.sqlalchemy.engines [req-e61eb488-afec-40d1-a5c3-850c7e6b992d - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.440 20457 WARNING oslo_db.sqlalchemy.engines [req-34465c53-45a2-4f7b-adec-a4f43b9b59ba - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.445 20458 WARNING oslo_db.sqlalchemy.engines [req-985b9db5-178b-48b2-842b-ed6756460850 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.450 20459 WARNING oslo_db.sqlalchemy.engines [req-d593adba-098f-4e4f-8d93-2ed7a3374b74 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.455 20460 WARNING oslo_db.sqlalchemy.engines [req-9df75ee5-0489-469d-9d28-954ee9522910 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.461 20461 WARNING oslo_db.sqlalchemy.engines [req-1f030cba-bc86-4e5b-b77a-2f495f322e32 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.466 20462 WARNING oslo_db.sqlalchemy.engines [req-7cc7561d-1a73-4851-ab4c-18499ba12755 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.471 20463 WARNING oslo_db.sqlalchemy.engines [req-41ea34fe-99b2-4020-a68b-fb2a59bc34aa - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.476 20464 WARNING oslo_db.sqlalchemy.engines [req-059840b6-1101-4a6e-a146-cba290fc9778 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.481 20465 WARNING oslo_db.sqlalchemy.engines [req-4e802d83-9dc4-435b-baf2-09d74110f285 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.487 20466 WARNING oslo_db.sqlalchemy.engines [req-bdf236da-42b7-446f-adc8-43f95c04f6bd - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.496 20467 WARNING oslo_db.sqlalchemy.engines [req-99e54044-5bfa-43cc-b3f0-128f7528cf85 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.503 20468 WARNING oslo_db.sqlalchemy.engines [req-4db102dc-4986-4f3d-bcfb-d11cb263becf - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.506 20469 WARNING oslo_db.sqlalchemy.engines [req-f5b77879-8652-4683-a3a5-c4d0f4d0332b - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.510 20470 WARNING oslo_db.sqlalchemy.engines [req-2f9610b7-cf57-4940-93cf-d27444826631 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.515 20472 WARNING oslo_db.sqlalchemy.engines [req-7531c6b3-059b-40e8-94b6-d6d213810df7 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.521 20473 WARNING oslo_db.sqlalchemy.engines [req-3052c7bf-132b-4b8a-912e-d9b538c588c7 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.526 20474 WARNING oslo_db.sqlalchemy.engines [req-968ca178-51e1-412e-a2d3-46062849273c - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.530 20475 WARNING oslo_db.sqlalchemy.engines [req-ffde5685-ae14-4bb2-87e1-ba7c17511e29 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.535 20477 WARNING oslo_db.sqlalchemy.engines [req-b0d898b1-b1df-4e59-8a7a-0e7fbae27521 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.538 20478 WARNING oslo_db.sqlalchemy.engines [req-77cdfb99-2beb-48e9-a859-52f1ff3670b1 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.545 20479 WARNING oslo_db.sqlalchemy.engines [req-86bab8ca-9bd0-4761-bd6c-d7b99dc5189b - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.548 20480 WARNING oslo_db.sqlalchemy.engines [req-ccdc7d0a-1cee-455a-8629-577945bccb4c - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.551 20481 WARNING oslo_db.sqlalchemy.engines [req-38204134-9a95-4366-ba0a-17ff5d1426f2 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.556 20482 WARNING oslo_db.sqlalchemy.engines [req-4edf8f78-9200-46f1-8a0a-31df1887932c - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.560 20483 WARNING oslo_db.sqlalchemy.engines [req-0815badd-f23e-4a96-9cee-137426796b1f - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.565 20484 WARNING oslo_db.sqlalchemy.engines [req-8dd31bed-db80-49ed-a433-18f7b9c6055f - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.569 20485 WARNING oslo_db.sqlalchemy.engines [req-ea082867-d96c-4de9-8c1c-8add1ad9736b - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.574 20486 WARNING oslo_db.sqlalchemy.engines [req-2e6d584a-e507-4b97-ab4f-8e64bb076f91 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.577 20487 WARNING oslo_db.sqlalchemy.engines [req-8058fdec-9482-445a-8b89-f734f2410bb9 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.584 20488 WARNING oslo_db.sqlalchemy.engines [req-347d4a35-e5a1-49e3-a4d2-d58d8510746e - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.589 20489 WARNING oslo_db.sqlalchemy.engines [req-560d6250-2a2d-442d-b05b-d5551fcedec7 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.592 20490 WARNING oslo_db.sqlalchemy.engines [req-ad56454e-542a-4108-8938-454052a4b9d2 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.597 20491 WARNING oslo_db.sqlalchemy.engines [req-030242b2-c5de-4c42-8a73-2baa69f41e87 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.600 20492 WARNING oslo_db.sqlalchemy.engines [req-59dba06f-e172-4268-921f-9c26ab95a6c8 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.605 20493 WARNING oslo_db.sqlalchemy.engines [req-91cbc52a-5cae-4577-bd50-1f7f94c80151 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.610 20494 WARNING oslo_db.sqlalchemy.engines [req-327e9f9b-bc9a-4027-a6f1-7c81380318c9 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.613 20495 WARNING oslo_db.sqlalchemy.engines [req-ab6f2307-11ec-4aff-895d-bf3a3b0ae197 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.620 20496 WARNING oslo_db.sqlalchemy.engines [req-3b03c3c9-a998-4d49-92e5-b6d359db9e23 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.624 20497 WARNING oslo_db.sqlalchemy.engines [req-101319a5-1b92-47e7-b5b1-1640e2359930 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.628 20498 WARNING oslo_db.sqlalchemy.engines [req-f9616dbe-46c4-408d-896e-5d730ff3cf1d - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.631 20499 WARNING oslo_db.sqlalchemy.engines [req-53ada140-ed0c-4787-81d9-70dedfc60371 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.637 20500 WARNING oslo_db.sqlalchemy.engines [req-0baa34dc-c246-43a6-83ea-8aa160aa3d60 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.639 20501 WARNING oslo_db.sqlalchemy.engines [req-18daed5a-aeb3-4bdd-8099-b8b4e3cc963c - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.643 20502 WARNING oslo_db.sqlalchemy.engines [req-97eadc91-de70-4d24-91aa-e2c30037dcb3 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.649 20503 WARNING oslo_db.sqlalchemy.engines [req-0b7c40f7-6eae-4853-8cfa-98ed81e47418 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:27:11.652 20504 WARNING oslo_db.sqlalchemy.engines [req-328619b0-3b19-4a95-a634-204e856a8adf - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 100  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 101  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 102  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 103  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 104  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 105  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 106  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 107  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 108  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 109  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 110  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 111  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 112  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 113  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 114  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 115  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 116  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 117  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 118  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 119  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 120  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 121  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 122  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 123  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 124  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 125  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 145  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 1  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 344  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 345  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 346  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 347  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 348  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 349  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 350  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 351  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 352  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 353  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 354  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 355  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 356  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 357  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 358  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 359  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 360  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 361  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 362  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 363  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 364  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 365  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 366  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 367  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 368  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 369  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 370  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 371  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 372  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 373  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 374  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 375  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 376  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 377  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 378  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 379  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 380  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 381  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 382  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 383  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 384  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 385  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 386  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 387  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 388  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 389  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 390  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 391  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 392  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 393  user: 'keystone'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 394  user: 'keystone'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 395  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 396  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 397  user: 'keystone'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 398  user: 'keystone'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 399  user: 'keystone'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 400  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 401  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 402  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 403  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 404  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 405  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 406  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 407  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 408  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 409  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 410  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 411  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 412  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 413  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 414  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 415  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 416  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 417  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 418  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 419  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 420  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 421  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 422  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 423  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 424  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 425  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 426  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 427  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 428  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 429  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 430  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 431  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 46  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 47  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 48  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 49  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 50  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 51  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 52  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 53  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 54  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 55  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 56  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 57  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 58  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 59  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 60  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 61  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 62  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 63  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 64  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 65  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 66  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 67  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 68  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 69  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 70  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 71  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 72  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 73  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 74  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 75  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 76  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 77  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 78  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 79  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 80  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 81  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 82  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 83  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 84  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 85  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 86  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 87  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 88  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 89  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 90  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 91  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 92  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 93  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 94  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 95  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 96  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 97  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 98  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:27:13 24274 [Warning] /usr/sbin/mysqld: Forcing close of thread 99  user: 'nova'
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 13:27:14.290 23845 ERROR oslo.messaging._drivers.impl_rabbit [-] [044a4011-c875-4e3f-8d3d-d5f7648f2c4e] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 35846
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:27:17.067 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [d9e037ab-f34b-4604-bf76-7b8be0c05aab] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 36164
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:27:17.081 23816 ERROR oslo.messaging._drivers.impl_rabbit [-] [d8797b29-0ae7-42c2-8329-73dcda665eca] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 36122
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:27:17.314 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [7d6be1dc-8433-4ee7-b0e1-c6c437fbc99c] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 36162
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:27:19.533 24695 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:27:20.346 24696 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:27:25.037 24697 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:12.836 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [26c7d5ce-673b-4656-870b-54703531d40b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 104] Connection reset by peer. Trying again in 1 seconds. Client port: 35318
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:13.853 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [26c7d5ce-673b-4656-870b-54703531d40b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 2 seconds. Client port: 35318
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:15.871 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [26c7d5ce-673b-4656-870b-54703531d40b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 4 seconds. Client port: 35318
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.088 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] Failed to consume message from queue: [Errno 104] Connection reset by peer
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.089 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] Unable to connect to AMQP server on mysql-001.vassilevski.com:5672 after None tries: [Errno 104] Connection reset by peer
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.089 24695 ERROR root 
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.089 24695 ERROR root     batch_size=self.batch_size, batch_timeout=self.batch_timeout)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.089 24695 ERROR root     error_callback=_error_callback)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.089 24695 ERROR root   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 211, in poll
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.089 24695 ERROR root   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/base.py", line 203, in _runner
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.089 24695 ERROR root   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/base.py", line 52, in wrapper
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.089 24695 ERROR root   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/impl_rabbit.py", line 1090, in consume
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.089 24695 ERROR root   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/impl_rabbit.py", line 807, in ensure
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.089 24695 ERROR root   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 250, in wrapper
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.089 24695 ERROR root MessageDeliveryFailure: Unable to connect to AMQP server on mysql-001.vassilevski.com:5672 after None tries: [Errno 104] Connection reset by peer
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.089 24695 ERROR root     msg = func(in_self, timeout=timeout_watch.leftover(True))
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.089 24695 ERROR root     raise exceptions.MessageDeliveryFailure(msg)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.089 24695 ERROR root     return infunc(*args, **kwargs)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.089 24695 ERROR root     self.conn.consume(timeout=timeout)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.089 24695 ERROR root Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.089 24695 ERROR root [-] Unexpected exception occurred 1 time(s)... retrying.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.334 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] Failed to consume message from queue: [Errno 104] Connection reset by peer
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.335 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] Unable to connect to AMQP server on mysql-001.vassilevski.com:5672 after None tries: [Errno 104] Connection reset by peer
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.335 24695 ERROR root 
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.335 24695 ERROR root     batch_size=self.batch_size, batch_timeout=self.batch_timeout)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.335 24695 ERROR root     error_callback=_error_callback)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.335 24695 ERROR root   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 211, in poll
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.335 24695 ERROR root   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/base.py", line 203, in _runner
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.335 24695 ERROR root   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/base.py", line 52, in wrapper
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.335 24695 ERROR root   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/impl_rabbit.py", line 1090, in consume
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.335 24695 ERROR root   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/impl_rabbit.py", line 807, in ensure
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.335 24695 ERROR root   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 250, in wrapper
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.335 24695 ERROR root MessageDeliveryFailure: Unable to connect to AMQP server on mysql-001.vassilevski.com:5672 after None tries: [Errno 104] Connection reset by peer
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.335 24695 ERROR root     msg = func(in_self, timeout=timeout_watch.leftover(True))
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.335 24695 ERROR root     raise exceptions.MessageDeliveryFailure(msg)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.335 24695 ERROR root     return infunc(*args, **kwargs)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.335 24695 ERROR root     self.conn.consume(timeout=timeout)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.335 24695 ERROR root Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:18.335 24695 ERROR root [-] Unexpected exception occurred 1 time(s)... retrying.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:19.110 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [d9e037ab-f34b-4604-bf76-7b8be0c05aab] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 1 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:19.887 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [26c7d5ce-673b-4656-870b-54703531d40b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 6 seconds. Client port: 35318
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:20.126 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [d9e037ab-f34b-4604-bf76-7b8be0c05aab] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 2 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:22.144 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [d9e037ab-f34b-4604-bf76-7b8be0c05aab] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 4 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:28:25.206 21351 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip '192.168.0.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:25.903 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [26c7d5ce-673b-4656-870b-54703531d40b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 8 seconds. Client port: 35318
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:26.161 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [d9e037ab-f34b-4604-bf76-7b8be0c05aab] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 6 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:28:26.498 21372 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip '192.168.0.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:28:27.758 21395 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip '192.168.0.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:28:29.036 21416 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip '192.168.0.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:28:30.265 21437 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip '192.168.0.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:32.179 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [d9e037ab-f34b-4604-bf76-7b8be0c05aab] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 8 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:33.921 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [26c7d5ce-673b-4656-870b-54703531d40b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 10 seconds. Client port: 35318
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:40.197 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [d9e037ab-f34b-4604-bf76-7b8be0c05aab] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 10 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:43.938 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [26c7d5ce-673b-4656-870b-54703531d40b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 12 seconds. Client port: 35318
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:49.374 24695 WARNING oslo_messaging.server [-] Possible hang: stop is waiting for another thread to complete
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:50.214 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [d9e037ab-f34b-4604-bf76-7b8be0c05aab] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 12 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:28:55.958 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [26c7d5ce-673b-4656-870b-54703531d40b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 14 seconds. Client port: 35318
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:29:02.233 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [d9e037ab-f34b-4604-bf76-7b8be0c05aab] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 14 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:29:09.978 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [26c7d5ce-673b-4656-870b-54703531d40b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 16 seconds. Client port: 35318
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:29:16.251 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [d9e037ab-f34b-4604-bf76-7b8be0c05aab] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 16 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:29:19.534 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [80e65231-589a-4cd0-8a18-af539987763d] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: timed out. Trying again in 1 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:29:20.551 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [80e65231-589a-4cd0-8a18-af539987763d] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 2 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:29:22.568 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [80e65231-589a-4cd0-8a18-af539987763d] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 4 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:29:26.003 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [26c7d5ce-673b-4656-870b-54703531d40b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 18 seconds. Client port: 35318
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:29:26.585 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [80e65231-589a-4cd0-8a18-af539987763d] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 6 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:29:32.273 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [d9e037ab-f34b-4604-bf76-7b8be0c05aab] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 18 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:29:32.602 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [80e65231-589a-4cd0-8a18-af539987763d] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 8 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:29:40.622 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [80e65231-589a-4cd0-8a18-af539987763d] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 10 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:29:44.029 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [26c7d5ce-673b-4656-870b-54703531d40b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 20 seconds. Client port: 35318
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:29:50.299 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [d9e037ab-f34b-4604-bf76-7b8be0c05aab] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 20 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:29:50.641 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [80e65231-589a-4cd0-8a18-af539987763d] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 12 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:30:02.664 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [80e65231-589a-4cd0-8a18-af539987763d] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 14 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:30:04.058 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [26c7d5ce-673b-4656-870b-54703531d40b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 22 seconds. Client port: 35318
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:30:10.326 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [d9e037ab-f34b-4604-bf76-7b8be0c05aab] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 22 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:30:16.688 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [80e65231-589a-4cd0-8a18-af539987763d] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 16 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:30:26.086 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [26c7d5ce-673b-4656-870b-54703531d40b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 24 seconds. Client port: 35318
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:30:32.351 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [d9e037ab-f34b-4604-bf76-7b8be0c05aab] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 24 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:30:32.711 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [80e65231-589a-4cd0-8a18-af539987763d] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 18 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:30:50.118 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [26c7d5ce-673b-4656-870b-54703531d40b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 26 seconds. Client port: 35318
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:30:50.737 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [80e65231-589a-4cd0-8a18-af539987763d] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 20 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:30:56.377 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [d9e037ab-f34b-4604-bf76-7b8be0c05aab] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 26 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:31:10.764 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [80e65231-589a-4cd0-8a18-af539987763d] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 22 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:31:16.148 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [26c7d5ce-673b-4656-870b-54703531d40b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 28 seconds. Client port: 35318
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:31:22.404 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [d9e037ab-f34b-4604-bf76-7b8be0c05aab] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 28 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:31:32.791 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [80e65231-589a-4cd0-8a18-af539987763d] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 24 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:31:44.179 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [26c7d5ce-673b-4656-870b-54703531d40b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 30 seconds. Client port: 35318
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:31:50.433 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [d9e037ab-f34b-4604-bf76-7b8be0c05aab] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 30 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:31:56.821 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [80e65231-589a-4cd0-8a18-af539987763d] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 26 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:32:14.211 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [26c7d5ce-673b-4656-870b-54703531d40b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 32 seconds. Client port: 35318
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:32:20.462 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [d9e037ab-f34b-4604-bf76-7b8be0c05aab] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 32 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:32:22.850 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [80e65231-589a-4cd0-8a18-af539987763d] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 28 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:32:46.244 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [26c7d5ce-673b-4656-870b-54703531d40b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 32 seconds. Client port: 35318
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:32:50.884 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [80e65231-589a-4cd0-8a18-af539987763d] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 30 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:32:52.489 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [d9e037ab-f34b-4604-bf76-7b8be0c05aab] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 32 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:33:18.278 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [26c7d5ce-673b-4656-870b-54703531d40b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 32 seconds. Client port: 35318
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:33:20.916 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [80e65231-589a-4cd0-8a18-af539987763d] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 32 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:33:24.516 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [d9e037ab-f34b-4604-bf76-7b8be0c05aab] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 32 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:33:50.310 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [26c7d5ce-673b-4656-870b-54703531d40b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 32 seconds. Client port: 35318
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:33:52.950 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [80e65231-589a-4cd0-8a18-af539987763d] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 32 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:33:56.544 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [d9e037ab-f34b-4604-bf76-7b8be0c05aab] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 32 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:34:22.343 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [26c7d5ce-673b-4656-870b-54703531d40b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 32 seconds. Client port: 35318
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:34:24.983 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [80e65231-589a-4cd0-8a18-af539987763d] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 32 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:34:28.570 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [d9e037ab-f34b-4604-bf76-7b8be0c05aab] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 32 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:34:33.626 22146 WARNING stevedore.named [req-5bc02c38-5c42-479c-8ce6-22b143ad8e2b - - - - -] Could not load neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:34:33.663 22146 ERROR oslo.messaging._drivers.impl_rabbit [-] [535cfe56-f641-4e3e-8040-1708f7d9c57f] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 1 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:34:33.663 22146 ERROR oslo.messaging._drivers.impl_rabbit [req-5bc02c38-5c42-479c-8ce6-22b143ad8e2b - - - - -] [8bafe5c5-aa1f-45e2-9d5d-457dffe0c1d3] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 1 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:34:34.681 22146 ERROR oslo.messaging._drivers.impl_rabbit [req-5bc02c38-5c42-479c-8ce6-22b143ad8e2b - - - - -] [8bafe5c5-aa1f-45e2-9d5d-457dffe0c1d3] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 2 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:34:34.682 22146 ERROR oslo.messaging._drivers.impl_rabbit [-] [535cfe56-f641-4e3e-8040-1708f7d9c57f] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 2 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:34:36.703 22146 ERROR oslo.messaging._drivers.impl_rabbit [-] [535cfe56-f641-4e3e-8040-1708f7d9c57f] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 4 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:34:36.703 22146 ERROR oslo.messaging._drivers.impl_rabbit [req-5bc02c38-5c42-479c-8ce6-22b143ad8e2b - - - - -] [8bafe5c5-aa1f-45e2-9d5d-457dffe0c1d3] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 4 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:34:40.734 22146 ERROR oslo.messaging._drivers.impl_rabbit [req-5bc02c38-5c42-479c-8ce6-22b143ad8e2b - - - - -] [8bafe5c5-aa1f-45e2-9d5d-457dffe0c1d3] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 6 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:34:40.735 22146 ERROR oslo.messaging._drivers.impl_rabbit [-] [535cfe56-f641-4e3e-8040-1708f7d9c57f] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 6 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:34:46.765 22146 ERROR oslo.messaging._drivers.impl_rabbit [-] [535cfe56-f641-4e3e-8040-1708f7d9c57f] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 8 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:34:46.765 22146 ERROR oslo.messaging._drivers.impl_rabbit [req-5bc02c38-5c42-479c-8ce6-22b143ad8e2b - - - - -] [8bafe5c5-aa1f-45e2-9d5d-457dffe0c1d3] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 8 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:34:54.376 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [26c7d5ce-673b-4656-870b-54703531d40b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 32 seconds. Client port: 35318
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:34:54.794 22146 ERROR oslo.messaging._drivers.impl_rabbit [-] [535cfe56-f641-4e3e-8040-1708f7d9c57f] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 10 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:34:54.795 22146 ERROR oslo.messaging._drivers.impl_rabbit [req-5bc02c38-5c42-479c-8ce6-22b143ad8e2b - - - - -] [8bafe5c5-aa1f-45e2-9d5d-457dffe0c1d3] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 10 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:34:57.014 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [80e65231-589a-4cd0-8a18-af539987763d] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 32 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:35:00.598 24695 ERROR oslo.messaging._drivers.impl_rabbit [-] [d9e037ab-f34b-4604-bf76-7b8be0c05aab] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 32 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:35:04.828 22146 ERROR oslo.messaging._drivers.impl_rabbit [req-5bc02c38-5c42-479c-8ce6-22b143ad8e2b - - - - -] [8bafe5c5-aa1f-45e2-9d5d-457dffe0c1d3] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 12 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:35:04.829 22146 ERROR oslo.messaging._drivers.impl_rabbit [-] [535cfe56-f641-4e3e-8040-1708f7d9c57f] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 12 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/nova-novncproxy.log:2017-02-09 13:35:08.276 23020 WARNING oslo_reports.guru_meditation_report [-] Guru meditation now registers SIGUSR1 and SIGUSR2 by default for backward compatibility. SIGUSR1 will no longer be registered in a future release, so please use SIGUSR2 to generate reports.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:35:08.534 23023 WARNING neutron.agent.securitygroups_rpc [-] Driver configuration doesn't match with enable_security_group
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:35:08.664 23023 WARNING oslo_db.sqlalchemy.engines [-] SQL connection failed. 10 attempts left.
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:35:09 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:35:09 23493 [Warning] Buffered warning: Changed limits: max_connections: 214 (requested 500)
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:35:09 23493 [Warning] Buffered warning: Changed limits: max_open_files: 1024 (requested 5000)
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:35:09 23493 [Warning] Buffered warning: Changed limits: table_open_cache: 400 (requested 2000)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 13:35:09.244 23062 WARNING oslo_reports.guru_meditation_report [-] Guru meditation now registers SIGUSR1 and SIGUSR2 by default for backward compatibility. SIGUSR1 will no longer be registered in a future release, so please use SIGUSR2 to generate reports.
/root/openstack/logs/2017-02-09/1428/nova-scheduler.log:2017-02-09 13:35:09.343 23016 WARNING oslo_reports.guru_meditation_report [-] Guru meditation now registers SIGUSR1 and SIGUSR2 by default for backward compatibility. SIGUSR1 will no longer be registered in a future release, so please use SIGUSR2 to generate reports.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 13:35:09.346 23011 WARNING oslo_reports.guru_meditation_report [-] Guru meditation now registers SIGUSR1 and SIGUSR2 by default for backward compatibility. SIGUSR1 will no longer be registered in a future release, so please use SIGUSR2 to generate reports.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.352 23018 WARNING oslo_reports.guru_meditation_report [-] Guru meditation now registers SIGUSR1 and SIGUSR2 by default for backward compatibility. SIGUSR1 will no longer be registered in a future release, so please use SIGUSR2 to generate reports.
/root/openstack/logs/2017-02-09/1428/nova-scheduler.log:2017-02-09 13:35:09.369 23016 WARNING oslo_db.sqlalchemy.engines [req-b19b2ed8-e31c-45ca-925d-135d02e19c0d - - - - -] SQL connection failed. 10 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-consoleauth.log:2017-02-09 13:35:09.389 23013 WARNING oslo_reports.guru_meditation_report [-] Guru meditation now registers SIGUSR1 and SIGUSR2 by default for backward compatibility. SIGUSR1 will no longer be registered in a future release, so please use SIGUSR2 to generate reports.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.398 23514 WARNING oslo_db.sqlalchemy.engines [req-f2955dc2-8a15-4bbf-809c-394ccc443a89 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.404 23516 WARNING oslo_db.sqlalchemy.engines [req-d39db35b-7069-4052-ab65-7c487838f2fc - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.408 23515 WARNING oslo_db.sqlalchemy.engines [req-92f85e95-c243-49a8-8cd0-e2382516815b - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.414 23512 WARNING oslo_db.sqlalchemy.engines [req-47ebddf1-fbde-4847-9c4c-00198d8b476a - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.415 23519 WARNING oslo_db.sqlalchemy.engines [req-2d306954-f98b-4e7d-8670-3af0e80ef5af - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.416 23513 WARNING oslo_db.sqlalchemy.engines [req-a7bf9b29-8d2c-44e0-8e19-f0d7cd9c3622 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.423 23517 WARNING oslo_db.sqlalchemy.engines [req-e2be238e-52fe-4a36-b4f3-b791cbed2a84 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.423 23526 WARNING oslo_db.sqlalchemy.engines [req-e1ca44f4-e89d-4e08-9087-2b9481df7805 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.425 23520 WARNING oslo_db.sqlalchemy.engines [req-4a0aeeb2-9c8d-4dca-a1b1-4bd471203980 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.425 23525 WARNING oslo_db.sqlalchemy.engines [req-88fc6c3b-8c7b-4bde-a348-b73178c15f47 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.426 23518 WARNING oslo_db.sqlalchemy.engines [req-651edaf3-3b0f-4fab-87cc-99602deaeee4 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.427 23521 WARNING oslo_db.sqlalchemy.engines [req-d01ee1f6-cb76-4a84-b74a-6fee4c57b66e - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.429 23523 WARNING oslo_db.sqlalchemy.engines [req-bcc03429-5b71-4f9b-8608-9d9ddf1f68ad - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.429 23529 WARNING oslo_db.sqlalchemy.engines [req-dd955539-e8a6-44be-bf33-72fcd271887b - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.430 23522 WARNING oslo_db.sqlalchemy.engines [req-905a4011-39fa-415a-8fa7-084bd90e9ec1 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.430 23527 WARNING oslo_db.sqlalchemy.engines [req-14d7fae3-8fc8-4af2-b271-9d883cd7d3e9 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.431 23524 WARNING oslo_db.sqlalchemy.engines [req-ed2b8ed0-2973-468c-841e-6af7b6325117 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.436 23530 WARNING oslo_db.sqlalchemy.engines [req-fefba68a-650f-403b-9577-f5ddeec91d11 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.442 23528 WARNING oslo_db.sqlalchemy.engines [req-90378b10-506b-48c7-b622-a1b75fa5bcd1 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.445 23531 WARNING oslo_db.sqlalchemy.engines [req-1d3348c1-299e-4c19-b53f-697c9579a5cf - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-consoleauth.log:2017-02-09 13:35:09.449 23013 WARNING oslo_db.sqlalchemy.engines [req-3e0df07a-069b-4c72-be46-6f1a7f55d1af - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.451 23533 WARNING oslo_db.sqlalchemy.engines [req-29b444b8-9148-4c6c-b5c3-e21acf8a054a - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 13:35:09.452 23062 WARNING os_brick.initiator.connectors.remotefs [req-1e67a09b-bde7-4d49-818b-d5ba57940a4f - - - - -] Connection details not present. RemoteFsClient may not initialize properly.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.453 23532 WARNING oslo_db.sqlalchemy.engines [req-90a9f75a-dd8b-4c87-88a5-80f34c871e93 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.456 23534 WARNING oslo_db.sqlalchemy.engines [req-17d7b5ea-146b-4ae8-914a-bfee1c4a1856 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.458 23539 WARNING oslo_db.sqlalchemy.engines [req-bf436a42-1db0-4f67-8eb6-1de051ce8130 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.463 23536 WARNING oslo_db.sqlalchemy.engines [req-ffe9e781-a1f9-44b9-a276-d181faf94a12 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.464 23535 WARNING oslo_db.sqlalchemy.engines [req-c59db004-8421-4e4e-868d-3808d93b522a - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.465 23537 WARNING oslo_db.sqlalchemy.engines [req-fab2ac17-727c-40de-b968-1f34b6472acc - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.466 23542 WARNING oslo_db.sqlalchemy.engines [req-3c6c2815-afe1-4b08-8010-22f5c1aa6a23 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.470 23538 WARNING oslo_db.sqlalchemy.engines [req-477fc22b-7c21-4bab-92cb-e0d3d5819238 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.475 23540 WARNING oslo_db.sqlalchemy.engines [req-0b982b1f-5ba3-4a06-87b2-41040a2d9f1e - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.475 23541 WARNING oslo_db.sqlalchemy.engines [req-2aadef2d-ded6-4f72-9ca6-f41969ebd502 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.477 23544 WARNING oslo_db.sqlalchemy.engines [req-30e050c7-6053-4439-93e8-a48e5cc5aa73 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.478 23543 WARNING oslo_db.sqlalchemy.engines [req-d6f17e96-9578-41b8-b93f-c2c1d20706bf - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.478 23545 WARNING oslo_db.sqlalchemy.engines [req-d1cba241-4a88-4b71-8cb2-1ac6c7e505ee - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.482 23546 WARNING oslo_db.sqlalchemy.engines [req-639a34bd-329c-4c4b-bd35-2cd725644c27 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.484 23548 WARNING oslo_db.sqlalchemy.engines [req-d8b587f4-3eb5-4ad0-aeb0-fd4a9f6eed0c - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.484 23549 WARNING oslo_db.sqlalchemy.engines [req-84d4f2da-704e-44ea-a273-857c5e6d0feb - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.485 23547 WARNING oslo_db.sqlalchemy.engines [req-120eb1d6-bf67-4c31-9bab-d0ca3b32c67d - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.495 23551 WARNING oslo_db.sqlalchemy.engines [req-f342b94f-8f38-4c6c-91ad-dfe06508eecd - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 13:35:09.496 23550 WARNING oslo_db.sqlalchemy.engines [req-9fc47d5f-7e0f-4c44-916c-428d16792757 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:35:18.896 23023 WARNING neutron.agent.securitygroups_rpc [req-5b0d7fdd-6290-4021-932a-9ce5ee72f216 - - - - -] Driver configuration doesn't match with enable_security_group
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:35:18.957 23023 WARNING neutron.api.extensions [req-5b0d7fdd-6290-4021-932a-9ce5ee72f216 - - - - -] Extension dns-integration not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:35:18.964 23023 WARNING neutron.api.extensions [req-5b0d7fdd-6290-4021-932a-9ce5ee72f216 - - - - -] Extension ip_allocation not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:35:18.965 23023 WARNING neutron.api.extensions [req-5b0d7fdd-6290-4021-932a-9ce5ee72f216 - - - - -] Extension l2_adjacency not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:35:18.971 23023 WARNING neutron.api.extensions [req-5b0d7fdd-6290-4021-932a-9ce5ee72f216 - - - - -] Extension metering not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:35:18.981 23023 WARNING neutron.api.extensions [req-5b0d7fdd-6290-4021-932a-9ce5ee72f216 - - - - -] Extension qos not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:35:18.985 23023 WARNING neutron.api.extensions [req-5b0d7fdd-6290-4021-932a-9ce5ee72f216 - - - - -] Extension router-service-type not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:35:18.988 23023 WARNING neutron.api.extensions [req-5b0d7fdd-6290-4021-932a-9ce5ee72f216 - - - - -] Extension segment not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:35:18.995 23023 WARNING neutron.api.extensions [req-5b0d7fdd-6290-4021-932a-9ce5ee72f216 - - - - -] Extension trunk-details not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:35:18.995 23023 WARNING neutron.api.extensions [req-5b0d7fdd-6290-4021-932a-9ce5ee72f216 - - - - -] Extension trunk not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:35:18.996 23023 WARNING neutron.api.extensions [req-5b0d7fdd-6290-4021-932a-9ce5ee72f216 - - - - -] Extension vlan-transparent not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 13:35:19.495 23062 WARNING nova.conductor.api [req-1e67a09b-bde7-4d49-818b-d5ba57940a4f - - - - -] Timed out waiting for nova-conductor.  Is it running? Or did this service start before nova-conductor?  Reattempting establishment of nova-conductor connection...
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 13:35:29.510 23062 WARNING nova.conductor.api [req-1e67a09b-bde7-4d49-818b-d5ba57940a4f - - - - -] Timed out waiting for nova-conductor.  Is it running? Or did this service start before nova-conductor?  Reattempting establishment of nova-conductor connection...
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 13:35:30.804 23062 WARNING nova.compute.monitors [req-473a9c01-be72-45dc-970d-ae85aa1c8a67 - - - - -] Excluding nova.compute.monitors.cpu monitor virt_driver. Not in the list of enabled monitors (CONF.compute_monitors).
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 13:35:31.512 23062 WARNING nova.scheduler.client.report [req-473a9c01-be72-45dc-970d-ae85aa1c8a67 - - - - -] No authentication information found for placement API. Optional use of placement API for reporting is now disabled.
/root/openstack/logs/2017-02-09/1428/l3-agent.log:2017-02-09 13:36:07.793 23026 ERROR neutron.common.rpc [req-db7f5acc-ceb1-4cbe-b160-ce6db42a0b40 - - - - -] Timeout in RPC method get_service_plugin_list. Waiting for 49 seconds before next attempt. If the server is not down, consider increasing the rpc_response_timeout option as Neutron server(s) may be overloaded and unable to respond quickly enough.
/root/openstack/logs/2017-02-09/1428/l3-agent.log:2017-02-09 13:36:07.795 23026 WARNING neutron.common.rpc [req-db7f5acc-ceb1-4cbe-b160-ce6db42a0b40 - - - - -] Increasing timeout for get_service_plugin_list calls to 120 seconds. Restart the agent to restore it to the default value.
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:36:07.831 23024 ERROR neutron.agent.dhcp.agent 
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:36:07.831 23024 ERROR neutron.agent.dhcp.agent     ctx, self.agent_state, True)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:36:07.831 23024 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/dhcp/agent.py", line 687, in _report_state
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:36:07.831 23024 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/rpc.py", line 88, in report_state
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:36:07.831 23024 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 238, in get
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:36:07.831 23024 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 336, in wait
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:36:07.831 23024 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 453, in _send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:36:07.831 23024 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 464, in send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:36:07.831 23024 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py", line 169, in call
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:36:07.831 23024 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/transport.py", line 97, in _send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:36:07.831 23024 ERROR neutron.agent.dhcp.agent     message = self.waiters.get(msg_id, timeout=timeout)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:36:07.831 23024 ERROR neutron.agent.dhcp.agent MessagingTimeout: Timed out waiting for a reply to message ID fcdc6d7887a54554a3380b70ae078ee7
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:36:07.831 23024 ERROR neutron.agent.dhcp.agent [req-a3371952-78ab-4424-abdf-baa666b49e4e - - - - -] Failed reporting state!
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:36:07.831 23024 ERROR neutron.agent.dhcp.agent     result = self._waiter.wait(msg_id, timeout)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:36:07.831 23024 ERROR neutron.agent.dhcp.agent     retry=retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:36:07.831 23024 ERROR neutron.agent.dhcp.agent     retry=self.retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:36:07.831 23024 ERROR neutron.agent.dhcp.agent     return method(context, 'report_state', **kwargs)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:36:07.831 23024 ERROR neutron.agent.dhcp.agent     timeout=timeout, retry=retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:36:07.831 23024 ERROR neutron.agent.dhcp.agent     'to message ID %s' % msg_id)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:36:07.831 23024 ERROR neutron.agent.dhcp.agent Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:36:07.835 23024 WARNING oslo.service.loopingcall [req-a3371952-78ab-4424-abdf-baa666b49e4e - - - - -] Function 'neutron.agent.dhcp.agent.DhcpAgentWithStateReport._report_state' run outlasted interval by 30.05 sec
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:36:07.836 23024 ERROR neutron.common.rpc [req-fb12dc23-8bb7-4b9c-8c5b-fbb4dfb4f0f7 - - - - -] Timeout in RPC method get_active_networks_info. Waiting for 57 seconds before next attempt. If the server is not down, consider increasing the rpc_response_timeout option as Neutron server(s) may be overloaded and unable to respond quickly enough.
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:36:07.837 23024 WARNING neutron.common.rpc [req-fb12dc23-8bb7-4b9c-8c5b-fbb4dfb4f0f7 - - - - -] Increasing timeout for get_active_networks_info calls to 120 seconds. Restart the agent to restore it to the default value.
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 13:36:08.056 23025 ERROR neutron.agent.metadata.agent 
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 13:36:08.056 23025 ERROR neutron.agent.metadata.agent [-] Failed reporting state!
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 13:36:08.056 23025 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/metadata/agent.py", line 262, in _report_state
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 13:36:08.056 23025 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/rpc.py", line 88, in report_state
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 13:36:08.056 23025 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 238, in get
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 13:36:08.056 23025 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 336, in wait
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 13:36:08.056 23025 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 453, in _send
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 13:36:08.056 23025 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 464, in send
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 13:36:08.056 23025 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py", line 169, in call
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 13:36:08.056 23025 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/transport.py", line 97, in _send
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 13:36:08.056 23025 ERROR neutron.agent.metadata.agent     message = self.waiters.get(msg_id, timeout=timeout)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 13:36:08.056 23025 ERROR neutron.agent.metadata.agent MessagingTimeout: Timed out waiting for a reply to message ID 35628bc0f0ed43bfa970c5dc7c21af36
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 13:36:08.056 23025 ERROR neutron.agent.metadata.agent     result = self._waiter.wait(msg_id, timeout)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 13:36:08.056 23025 ERROR neutron.agent.metadata.agent     retry=retry)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 13:36:08.056 23025 ERROR neutron.agent.metadata.agent     retry=self.retry)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 13:36:08.056 23025 ERROR neutron.agent.metadata.agent     return method(context, 'report_state', **kwargs)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 13:36:08.056 23025 ERROR neutron.agent.metadata.agent     timeout=timeout, retry=retry)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 13:36:08.056 23025 ERROR neutron.agent.metadata.agent     'to message ID %s' % msg_id)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 13:36:08.056 23025 ERROR neutron.agent.metadata.agent Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 13:36:08.056 23025 ERROR neutron.agent.metadata.agent     use_call=self.agent_state.get('start_flag'))
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 13:36:08.065 23025 WARNING oslo.service.loopingcall [-] Function 'neutron.agent.metadata.agent.UnixDomainMetadataProxy._report_state' run outlasted interval by 30.05 sec
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:36:16.912 22146 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent 
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:36:16.912 22146 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Failed reporting state!
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:36:16.912 22146 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/neutron/agent/rpc.py", line 88, in report_state
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:36:16.912 22146 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/neutron/plugins/ml2/drivers/openvswitch/agent/ovs_neutron_agent.py", line 320, in _report_state
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:36:16.912 22146 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 238, in get
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:36:16.912 22146 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 336, in wait
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:36:16.912 22146 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 453, in _send
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:36:16.912 22146 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 464, in send
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:36:16.912 22146 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py", line 169, in call
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:36:16.912 22146 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/transport.py", line 97, in _send
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:36:16.912 22146 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     message = self.waiters.get(msg_id, timeout=timeout)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:36:16.912 22146 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent MessagingTimeout: Timed out waiting for a reply to message ID 3152cbaf35ae43e8a52bf56172dddb08
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:36:16.912 22146 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     result = self._waiter.wait(msg_id, timeout)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:36:16.912 22146 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     retry=retry)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:36:16.912 22146 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     retry=self.retry)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:36:16.912 22146 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     return method(context, 'report_state', **kwargs)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:36:16.912 22146 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     timeout=timeout, retry=retry)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:36:16.912 22146 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     'to message ID %s' % msg_id)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:36:16.912 22146 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:36:16.912 22146 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     True)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:36:16.916 22146 WARNING oslo.service.loopingcall [-] Function 'neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent.OVSNeutronAgent._report_state' run outlasted interval by 73.26 sec
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:36:17.337 22146 ERROR neutron.common.rpc [req-08d001e6-aa8c-41f3-b31c-90d880d6aef5 - - - - -] Timeout in RPC method tunnel_sync. Waiting for 24 seconds before next attempt. If the server is not down, consider increasing the rpc_response_timeout option as Neutron server(s) may be overloaded and unable to respond quickly enough.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:36:17.338 22146 WARNING neutron.common.rpc [req-08d001e6-aa8c-41f3-b31c-90d880d6aef5 - - - - -] Increasing timeout for tunnel_sync calls to 120 seconds. Restart the agent to restore it to the default value.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:36:24.849 23876 WARNING neutron.db.agents_db [req-8aa95a60-af19-4891-ace2-2120f163120e - - - - -] Agent healthcheck: found 2 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:36:44.306 22146 ERROR neutron.agent.linux.async_process [-] Error received from [ovsdb-client monitor Interface name,ofport,external_ids --format=json]: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:36:44.307 22146 ERROR neutron.agent.linux.async_process [-] Process [ovsdb-client monitor Interface name,ofport,external_ids --format=json] dies due to the error: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:36:46.735 24532 WARNING stevedore.named [req-f2af72b8-4ecf-47da-8d21-5a2a633beeee - - - - -] Could not load neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
/root/openstack/logs/2017-02-09/1428/l3-agent.log:2017-02-09 13:36:56.521 23026 WARNING neutron.agent.l3.agent [req-db7f5acc-ceb1-4cbe-b160-ce6db42a0b40 - - - - -] l3-agent cannot contact neutron server to retrieve service plugins enabled. Check connectivity to neutron server. Retrying... Detailed message: Timed out waiting for a reply to message ID 4e0167d8fa0040c9a12caf134c0e0fd0.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:37:01.838 23876 WARNING neutron.db.agents_db [req-8aa95a60-af19-4891-ace2-2120f163120e - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:37:04.347 23024 ERROR neutron.agent.dhcp.agent 
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:37:04.347 23024 ERROR neutron.agent.dhcp.agent     active_networks = self.plugin_rpc.get_active_networks_info()
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:37:04.347 23024 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/dhcp/agent.py", line 159, in sync_state
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:37:04.347 23024 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/dhcp/agent.py", line 522, in get_active_networks_info
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:37:04.347 23024 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/common/rpc.py", line 138, in call
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:37:04.347 23024 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/common/rpc.py", line 157, in call
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:37:04.347 23024 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 238, in get
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:37:04.347 23024 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 336, in wait
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:37:04.347 23024 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 453, in _send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:37:04.347 23024 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 464, in send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:37:04.347 23024 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py", line 169, in call
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:37:04.347 23024 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/transport.py", line 97, in _send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:37:04.347 23024 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 196, in force_reraise
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:37:04.347 23024 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 220, in __exit__
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:37:04.347 23024 ERROR neutron.agent.dhcp.agent     host=self.host)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:37:04.347 23024 ERROR neutron.agent.dhcp.agent     message = self.waiters.get(msg_id, timeout=timeout)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:37:04.347 23024 ERROR neutron.agent.dhcp.agent MessagingTimeout: Timed out waiting for a reply to message ID 707ab5328888471ea9ae3be2a73a3667
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:37:04.347 23024 ERROR neutron.agent.dhcp.agent [req-fb12dc23-8bb7-4b9c-8c5b-fbb4dfb4f0f7 - - - - -] Unable to sync network state.
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:37:04.347 23024 ERROR neutron.agent.dhcp.agent     result = self._waiter.wait(msg_id, timeout)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:37:04.347 23024 ERROR neutron.agent.dhcp.agent     retry=retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:37:04.347 23024 ERROR neutron.agent.dhcp.agent     retry=self.retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:37:04.347 23024 ERROR neutron.agent.dhcp.agent     return self._original_context.call(ctxt, method, **kwargs)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:37:04.347 23024 ERROR neutron.agent.dhcp.agent     self.force_reraise()
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:37:04.347 23024 ERROR neutron.agent.dhcp.agent     six.reraise(self.type_, self.value, self.tb)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:37:04.347 23024 ERROR neutron.agent.dhcp.agent     timeout=timeout, retry=retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:37:04.347 23024 ERROR neutron.agent.dhcp.agent     time.sleep(wait)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:37:04.347 23024 ERROR neutron.agent.dhcp.agent     'to message ID %s' % msg_id)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:37:04.347 23024 ERROR neutron.agent.dhcp.agent Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 13:37:04.874 23024 WARNING oslo.service.loopingcall [req-58e1a6c7-05ee-4772-97f5-e909ebc75e15 - - - - -] Function 'neutron.agent.dhcp.agent.DhcpAgentWithStateReport._report_state' run outlasted interval by 27.04 sec
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:37:38.837 23876 WARNING neutron.db.agents_db [req-8aa95a60-af19-4891-ace2-2120f163120e - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:38:15.837 23876 WARNING neutron.db.agents_db [req-8aa95a60-af19-4891-ace2-2120f163120e - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:38:52.835 23876 WARNING neutron.db.agents_db [req-8aa95a60-af19-4891-ace2-2120f163120e - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:38:59.319 24532 ERROR neutron.agent.linux.async_process [-] Error received from [ovsdb-client monitor Interface name,ofport,external_ids --format=json]: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:38:59.321 24532 ERROR neutron.agent.linux.async_process [-] Process [ovsdb-client monitor Interface name,ofport,external_ids --format=json] dies due to the error: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:39:00.944 25055 WARNING stevedore.named [req-6f6d447b-1fca-46c7-afd0-5e2e63641d98 - - - - -] Could not load neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:39:01.434 23874 WARNING neutron.plugins.ml2.drivers.type_tunnel [req-6f6d447b-1fca-46c7-afd0-5e2e63641d98 - - - - -] Endpoint with ip 10.0.0.10 already exists
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:39:05.564 25055 ERROR neutron.agent.linux.async_process [-] Error received from [ovsdb-client monitor Interface name,ofport,external_ids --format=json]: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:39:05.565 25055 ERROR neutron.agent.linux.async_process [-] Process [ovsdb-client monitor Interface name,ofport,external_ids --format=json] dies due to the error: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:39:07.990 25443 WARNING stevedore.named [req-fc883605-fad2-4d92-b9e1-75492bd4936a - - - - -] Could not load neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:39:08.426 23874 WARNING neutron.plugins.ml2.drivers.type_tunnel [req-fc883605-fad2-4d92-b9e1-75492bd4936a - - - - -] Endpoint with ip 10.0.0.10 already exists
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:39:29.834 23876 WARNING neutron.db.agents_db [req-8aa95a60-af19-4891-ace2-2120f163120e - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:40:06.834 23876 WARNING neutron.db.agents_db [req-8aa95a60-af19-4891-ace2-2120f163120e - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:40:43.834 23876 WARNING neutron.db.agents_db [req-8aa95a60-af19-4891-ace2-2120f163120e - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:41:20.833 23876 WARNING neutron.db.agents_db [req-8aa95a60-af19-4891-ace2-2120f163120e - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:41:57.832 23876 WARNING neutron.db.agents_db [req-8aa95a60-af19-4891-ace2-2120f163120e - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:42:34.831 23876 WARNING neutron.db.agents_db [req-8aa95a60-af19-4891-ace2-2120f163120e - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:42:46.652 25443 ERROR neutron.agent.linux.async_process [-] Error received from [ovsdb-client monitor Interface name,ofport,external_ids --format=json]: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:42:46.653 25443 ERROR neutron.agent.linux.async_process [-] Process [ovsdb-client monitor Interface name,ofport,external_ids --format=json] dies due to the error: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:42:48.763 26313 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip '10.0.0.10'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:42:50.324 26337 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip '10.0.0.10'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:42:51.809 26358 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip '10.0.0.10'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:42:53.329 26389 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip '10.0.0.10'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:42:54.820 26411 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip '10.0.0.10'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:11.831 23876 WARNING neutron.db.agents_db [req-8aa95a60-af19-4891-ace2-2120f163120e - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:43:29 23493 [Warning] /usr/sbin/mysqld: Forcing close of thread 106  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:43:29 23493 [Warning] /usr/sbin/mysqld: Forcing close of thread 125  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:43:29 23493 [Warning] /usr/sbin/mysqld: Forcing close of thread 126  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:43:29 23493 [Warning] /usr/sbin/mysqld: Forcing close of thread 127  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:43:29 23493 [Warning] /usr/sbin/mysqld: Forcing close of thread 128  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:43:29 23493 [Warning] /usr/sbin/mysqld: Forcing close of thread 134  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:43:29 23493 [Warning] /usr/sbin/mysqld: Forcing close of thread 135  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:43:29 23493 [Warning] /usr/sbin/mysqld: Forcing close of thread 137  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 13:43:29 23493 [Warning] /usr/sbin/mysqld: Forcing close of thread 83  user: 'nova'
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db 
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     close_with_result=True)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     compat.reraise(exc_type, exc_value, exc_tb)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     compiled_sql, distilled_params
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     connection.scalar(select([1]))
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     connection = self.__pool._invoke_creator(self)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     conn = engine.contextual_connect(**kw)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     conn = self._revalidate_connection()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     dbapi_connection = rec.get_connection()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db DBConnectionError: (pymysql.err.OperationalError) (2003, "Can't connect to MySQL server on 'mysql-001.vassilevski.com' ([Errno 111] ECONNREFUSED)") [SQL: u'SELECT 1']
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     execution_options=execution_options)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     fairy = _ConnectionRecord.checkout(pool)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 1010, in _execute_clauseelement
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 1071, in _execute_context
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 1078, in _execute_context
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 1337, in _handle_dbapi_exception
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 2041, in contextual_connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 2074, in _wrap_pool_connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 2080, in _wrap_pool_connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 2104, in raw_connection
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 393, in _revalidate_connection
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 844, in scalar
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 914, in execute
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py", line 92, in __init__
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/default.py", line 385, in connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib64/python2.7/site-packages/sqlalchemy/engine/strategies.py", line 97, in connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib64/python2.7/site-packages/sqlalchemy/event/attr.py", line 256, in __call__
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py", line 2736, in __iter__
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py", line 2740, in _connection_from_session
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py", line 2749, in _execute_and_instances
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/session.py", line 905, in connection
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib64/python2.7/site-packages/sqlalchemy/orm/session.py", line 912, in _connection_for_bind
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py", line 318, in unique_connection
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py", line 482, in checkout
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py", line 485, in checkout
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py", line 594, in get_connection
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py", line 607, in __connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib64/python2.7/site-packages/sqlalchemy/pool.py", line 713, in _checkout
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib64/python2.7/site-packages/sqlalchemy/sql/elements.py", line 323, in _execute_on_connection
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib64/python2.7/site-packages/sqlalchemy/util/compat.py", line 200, in raise_from_cause
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib64/python2.7/site-packages/sqlalchemy/util/langhelpers.py", line 60, in __exit__
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib/python2.7/site-packages/neutron/db/agentschedulers_db.py", line 349, in _filter_bindings
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib/python2.7/site-packages/neutron/db/agentschedulers_db.py", line 389, in remove_networks_from_down_agents
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib/python2.7/site-packages/oslo_db/sqlalchemy/engines.py", line 80, in _connect_ping_listener
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib/python2.7/site-packages/pymysql/connections.py", line 694, in __init__
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib/python2.7/site-packages/pymysql/connections.py", line 947, in connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db   File "/usr/lib/python2.7/site-packages/pymysql/__init__.py", line 90, in Connect
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     fn(*args, **kw)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     for binding in bindings:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     **kw)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     **kwargs)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     None, None)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     raise exc
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     rec.checkin()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db [req-72328852-0f96-493e-935e-a380bcc3bf89 - - - - -] Exception encountered during network rescheduling
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     reraise(type(exception), exception, tb=exc_tb)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     return Connection(*args, **kwargs)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     return connection._execute_clauseelement(self, multiparams, params)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     return _ConnectionFairy._checkout(self)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     return dialect.connect(*cargs, **cparams)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     return fn()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     return meth(self, multiparams, params)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     return self.dbapi.connect(*cargs, **cparams)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     return self._execute_and_instances(context)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     return self.execute(object, *multiparams, **params).scalar()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     self.connect()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     self.connection = self.__connect()
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     self.__connection = self.engine.raw_connection(_connection=self)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     self.dispatch.engine_connect(self, self.__branch)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     self._filter_bindings(context, down_bindings)]
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     self.pool.unique_connection, _connection)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     util.raise_from_cause(newraise, exc_info)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 13:43:48.775 23876 ERROR neutron.db.agentschedulers_db     util.reraise(*sys.exc_info())
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:46:29.305 27316 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip '10.0.0.10'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:46:30.739 27337 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip '10.0.0.10'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:46:32.026 27358 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip '10.0.0.10'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:46:33.497 27379 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip '10.0.0.10'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:46:34.750 27400 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip '10.0.0.10'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:54:20.829 29055 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip '10.0.0.10'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:54:22.252 29077 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip '10.0.0.10'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:54:23.666 29100 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip '10.0.0.10'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:54:24.995 29121 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip '10.0.0.10'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:54:26.256 29143 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip '10.0.0.10'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:57:47.226 29438 WARNING stevedore.named [req-4a4a078e-54ca-449c-9e45-5b8fd16f0108 - - - - -] Could not load neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:57:52.284 29438 ERROR oslo.messaging._drivers.impl_rabbit [req-4a4a078e-54ca-449c-9e45-5b8fd16f0108 - - - - -] [458d028c-086a-4154-b283-17e77ea0bd7b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 1 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:57:52.299 29438 ERROR oslo.messaging._drivers.impl_rabbit [-] [36fe3490-36c2-472b-a2b4-aaae0d489ddd] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 1 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:57:53.299 29438 ERROR oslo.messaging._drivers.impl_rabbit [req-4a4a078e-54ca-449c-9e45-5b8fd16f0108 - - - - -] [458d028c-086a-4154-b283-17e77ea0bd7b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 2 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:57:53.312 29438 ERROR oslo.messaging._drivers.impl_rabbit [-] [36fe3490-36c2-472b-a2b4-aaae0d489ddd] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 2 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:57:55.316 29438 ERROR oslo.messaging._drivers.impl_rabbit [req-4a4a078e-54ca-449c-9e45-5b8fd16f0108 - - - - -] [458d028c-086a-4154-b283-17e77ea0bd7b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 4 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:57:55.329 29438 ERROR oslo.messaging._drivers.impl_rabbit [-] [36fe3490-36c2-472b-a2b4-aaae0d489ddd] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 4 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:57:59.331 29438 ERROR oslo.messaging._drivers.impl_rabbit [req-4a4a078e-54ca-449c-9e45-5b8fd16f0108 - - - - -] [458d028c-086a-4154-b283-17e77ea0bd7b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 6 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:57:59.345 29438 ERROR oslo.messaging._drivers.impl_rabbit [-] [36fe3490-36c2-472b-a2b4-aaae0d489ddd] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 6 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:58:05.346 29438 ERROR oslo.messaging._drivers.impl_rabbit [req-4a4a078e-54ca-449c-9e45-5b8fd16f0108 - - - - -] [458d028c-086a-4154-b283-17e77ea0bd7b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 8 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:58:05.359 29438 ERROR oslo.messaging._drivers.impl_rabbit [-] [36fe3490-36c2-472b-a2b4-aaae0d489ddd] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 8 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:58:13.364 29438 ERROR oslo.messaging._drivers.impl_rabbit [req-4a4a078e-54ca-449c-9e45-5b8fd16f0108 - - - - -] [458d028c-086a-4154-b283-17e77ea0bd7b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 10 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:58:13.379 29438 ERROR oslo.messaging._drivers.impl_rabbit [-] [36fe3490-36c2-472b-a2b4-aaae0d489ddd] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 10 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:58:23.384 29438 ERROR oslo.messaging._drivers.impl_rabbit [req-4a4a078e-54ca-449c-9e45-5b8fd16f0108 - - - - -] [458d028c-086a-4154-b283-17e77ea0bd7b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 12 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:58:23.396 29438 ERROR oslo.messaging._drivers.impl_rabbit [-] [36fe3490-36c2-472b-a2b4-aaae0d489ddd] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 12 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:58:35.405 29438 ERROR oslo.messaging._drivers.impl_rabbit [req-4a4a078e-54ca-449c-9e45-5b8fd16f0108 - - - - -] [458d028c-086a-4154-b283-17e77ea0bd7b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 14 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:58:35.418 29438 ERROR oslo.messaging._drivers.impl_rabbit [-] [36fe3490-36c2-472b-a2b4-aaae0d489ddd] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: [Errno 111] ECONNREFUSED. Trying again in 14 seconds. Client port: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:59:49.490 29438 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent 
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:59:49.490 29438 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Failed reporting state!
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:59:49.490 29438 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/neutron/agent/rpc.py", line 88, in report_state
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:59:49.490 29438 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/neutron/plugins/ml2/drivers/openvswitch/agent/ovs_neutron_agent.py", line 320, in _report_state
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:59:49.490 29438 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 238, in get
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:59:49.490 29438 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 336, in wait
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:59:49.490 29438 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 453, in _send
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:59:49.490 29438 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 464, in send
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:59:49.490 29438 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py", line 169, in call
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:59:49.490 29438 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/transport.py", line 97, in _send
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:59:49.490 29438 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     message = self.waiters.get(msg_id, timeout=timeout)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:59:49.490 29438 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent MessagingTimeout: Timed out waiting for a reply to message ID c44dcaec697d42caac122c8b21ddc67e
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:59:49.490 29438 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     result = self._waiter.wait(msg_id, timeout)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:59:49.490 29438 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     retry=retry)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:59:49.490 29438 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     retry=self.retry)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:59:49.490 29438 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     return method(context, 'report_state', **kwargs)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:59:49.490 29438 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     timeout=timeout, retry=retry)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:59:49.490 29438 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     'to message ID %s' % msg_id)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:59:49.490 29438 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:59:49.490 29438 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent     True)
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:59:49.494 29438 WARNING oslo.service.loopingcall [-] Function 'neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent.OVSNeutronAgent._report_state' run outlasted interval by 87.21 sec
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:59:50.023 29438 ERROR neutron.common.rpc [req-c22ba9ad-8b07-4328-b3ec-4acd5ffe816a - - - - -] Timeout in RPC method tunnel_sync. Waiting for 44 seconds before next attempt. If the server is not down, consider increasing the rpc_response_timeout option as Neutron server(s) may be overloaded and unable to respond quickly enough.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 13:59:50.025 29438 WARNING neutron.common.rpc [req-c22ba9ad-8b07-4328-b3ec-4acd5ffe816a - - - - -] Increasing timeout for tunnel_sync calls to 120 seconds. Restart the agent to restore it to the default value.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:00:01.733 30522 WARNING neutron.agent.securitygroups_rpc [-] Driver configuration doesn't match with enable_security_group
/root/openstack/logs/2017-02-09/1428/nova-novncproxy.log:2017-02-09 14:00:01.809 30520 WARNING oslo_reports.guru_meditation_report [-] Guru meditation now registers SIGUSR1 and SIGUSR2 by default for backward compatibility. SIGUSR1 will no longer be registered in a future release, so please use SIGUSR2 to generate reports.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:00:01.869 30522 WARNING oslo_db.sqlalchemy.engines [-] SQL connection failed. 10 attempts left.
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:00:02 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:00:02 30990 [Warning] Buffered warning: Changed limits: max_connections: 214 (requested 500)
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:00:02 30990 [Warning] Buffered warning: Changed limits: max_open_files: 1024 (requested 5000)
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:00:02 30990 [Warning] Buffered warning: Changed limits: table_open_cache: 400 (requested 2000)
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:00:02.962 30519 WARNING oslo_reports.guru_meditation_report [-] Guru meditation now registers SIGUSR1 and SIGUSR2 by default for backward compatibility. SIGUSR1 will no longer be registered in a future release, so please use SIGUSR2 to generate reports.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:00:03.043 30513 WARNING oslo_reports.guru_meditation_report [-] Guru meditation now registers SIGUSR1 and SIGUSR2 by default for backward compatibility. SIGUSR1 will no longer be registered in a future release, so please use SIGUSR2 to generate reports.
/root/openstack/logs/2017-02-09/1428/nova-consoleauth.log:2017-02-09 14:00:03.087 30515 WARNING oslo_reports.guru_meditation_report [-] Guru meditation now registers SIGUSR1 and SIGUSR2 by default for backward compatibility. SIGUSR1 will no longer be registered in a future release, so please use SIGUSR2 to generate reports.
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:00:03.317 30555 WARNING oslo_reports.guru_meditation_report [-] Guru meditation now registers SIGUSR1 and SIGUSR2 by default for backward compatibility. SIGUSR1 will no longer be registered in a future release, so please use SIGUSR2 to generate reports.
/root/openstack/logs/2017-02-09/1428/nova-scheduler.log:2017-02-09 14:00:03.445 30518 WARNING oslo_reports.guru_meditation_report [-] Guru meditation now registers SIGUSR1 and SIGUSR2 by default for backward compatibility. SIGUSR1 will no longer be registered in a future release, so please use SIGUSR2 to generate reports.
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:00:03.630 30555 WARNING os_brick.initiator.connectors.remotefs [req-ceb82a85-0388-4385-adcb-4045f66dbf5b - - - - -] Connection details not present. RemoteFsClient may not initialize properly.
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:00:05.113 30555 WARNING nova.compute.monitors [req-1400dd6d-5c65-4d11-a713-fb19409c72ed - - - - -] Excluding nova.compute.monitors.cpu monitor virt_driver. Not in the list of enabled monitors (CONF.compute_monitors).
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:00:05.704 30555 WARNING nova.scheduler.client.report [req-1400dd6d-5c65-4d11-a713-fb19409c72ed - - - - -] No authentication information found for placement API. Optional use of placement API for reporting is now disabled.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:00:12.094 30522 WARNING neutron.agent.securitygroups_rpc [req-10ce9b06-ec76-42f0-889b-7dd79e9fe997 - - - - -] Driver configuration doesn't match with enable_security_group
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:00:12.153 30522 WARNING neutron.api.extensions [req-10ce9b06-ec76-42f0-889b-7dd79e9fe997 - - - - -] Extension dns-integration not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:00:12.161 30522 WARNING neutron.api.extensions [req-10ce9b06-ec76-42f0-889b-7dd79e9fe997 - - - - -] Extension ip_allocation not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:00:12.162 30522 WARNING neutron.api.extensions [req-10ce9b06-ec76-42f0-889b-7dd79e9fe997 - - - - -] Extension l2_adjacency not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:00:12.168 30522 WARNING neutron.api.extensions [req-10ce9b06-ec76-42f0-889b-7dd79e9fe997 - - - - -] Extension metering not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:00:12.178 30522 WARNING neutron.api.extensions [req-10ce9b06-ec76-42f0-889b-7dd79e9fe997 - - - - -] Extension qos not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:00:12.182 30522 WARNING neutron.api.extensions [req-10ce9b06-ec76-42f0-889b-7dd79e9fe997 - - - - -] Extension router-service-type not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:00:12.185 30522 WARNING neutron.api.extensions [req-10ce9b06-ec76-42f0-889b-7dd79e9fe997 - - - - -] Extension segment not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:00:12.191 30522 WARNING neutron.api.extensions [req-10ce9b06-ec76-42f0-889b-7dd79e9fe997 - - - - -] Extension trunk not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:00:12.192 30522 WARNING neutron.api.extensions [req-10ce9b06-ec76-42f0-889b-7dd79e9fe997 - - - - -] Extension trunk-details not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:00:12.193 30522 WARNING neutron.api.extensions [req-10ce9b06-ec76-42f0-889b-7dd79e9fe997 - - - - -] Extension vlan-transparent not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:00:28.662 31368 WARNING stevedore.named [req-59711ddd-d674-46f4-87e5-5bcf662669ba - - - - -] Could not load neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:00:29.872 31354 WARNING neutron.api.rpc.agentnotifiers.dhcp_rpc_agent_api [req-59711ddd-d674-46f4-87e5-5bcf662669ba - - - - -] Only 0 of 1 DHCP agents associated with network '584d9ef6-3673-4cd5-beb9-87b3158d953a' are marked as active, so notifications may be sent to inactive agents.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:00:29.990 31354 WARNING neutron.api.rpc.agentnotifiers.dhcp_rpc_agent_api [req-59711ddd-d674-46f4-87e5-5bcf662669ba - - - - -] Only 0 of 1 DHCP agents associated with network '584d9ef6-3673-4cd5-beb9-87b3158d953a' are marked as active, so notifications may be sent to inactive agents.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:00:30.157 31354 WARNING neutron.api.rpc.agentnotifiers.dhcp_rpc_agent_api [req-59711ddd-d674-46f4-87e5-5bcf662669ba - - - - -] Only 0 of 1 DHCP agents associated with network '2eb2a376-5200-4093-8bb9-7eb4c263a96a' are marked as active, so notifications may be sent to inactive agents.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:00:30.271 31354 WARNING neutron.api.rpc.agentnotifiers.dhcp_rpc_agent_api [req-59711ddd-d674-46f4-87e5-5bcf662669ba - - - - -] Only 0 of 1 DHCP agents associated with network '584d9ef6-3673-4cd5-beb9-87b3158d953a' are marked as active, so notifications may be sent to inactive agents.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:00:30.797 31354 WARNING neutron.api.rpc.agentnotifiers.dhcp_rpc_agent_api [req-59711ddd-d674-46f4-87e5-5bcf662669ba - - - - -] Only 0 of 1 DHCP agents associated with network '584d9ef6-3673-4cd5-beb9-87b3158d953a' are marked as active, so notifications may be sent to inactive agents.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:00:31.026 31354 WARNING neutron.api.rpc.agentnotifiers.dhcp_rpc_agent_api [req-59711ddd-d674-46f4-87e5-5bcf662669ba - - - - -] Only 0 of 1 DHCP agents associated with network '584d9ef6-3673-4cd5-beb9-87b3158d953a' are marked as active, so notifications may be sent to inactive agents.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:00:31.408 31354 WARNING neutron.api.rpc.agentnotifiers.dhcp_rpc_agent_api [req-59711ddd-d674-46f4-87e5-5bcf662669ba - - - - -] Only 0 of 1 DHCP agents associated with network '2eb2a376-5200-4093-8bb9-7eb4c263a96a' are marked as active, so notifications may be sent to inactive agents.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:00:31.592 31354 WARNING neutron.api.rpc.agentnotifiers.dhcp_rpc_agent_api [req-59711ddd-d674-46f4-87e5-5bcf662669ba - - - - -] Only 0 of 1 DHCP agents associated with network '584d9ef6-3673-4cd5-beb9-87b3158d953a' are marked as active, so notifications may be sent to inactive agents.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:00:32.040 31354 WARNING neutron.api.rpc.agentnotifiers.dhcp_rpc_agent_api [req-59711ddd-d674-46f4-87e5-5bcf662669ba - - - - -] Only 0 of 1 DHCP agents associated with network '584d9ef6-3673-4cd5-beb9-87b3158d953a' are marked as active, so notifications may be sent to inactive agents.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:00:32.156 31354 WARNING neutron.api.rpc.agentnotifiers.dhcp_rpc_agent_api [req-59711ddd-d674-46f4-87e5-5bcf662669ba - - - - -] Only 0 of 1 DHCP agents associated with network '584d9ef6-3673-4cd5-beb9-87b3158d953a' are marked as active, so notifications may be sent to inactive agents.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:00:32.324 31354 WARNING neutron.api.rpc.agentnotifiers.dhcp_rpc_agent_api [req-59711ddd-d674-46f4-87e5-5bcf662669ba - - - - -] Only 0 of 1 DHCP agents associated with network '2eb2a376-5200-4093-8bb9-7eb4c263a96a' are marked as active, so notifications may be sent to inactive agents.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:00:32.438 31354 WARNING neutron.api.rpc.agentnotifiers.dhcp_rpc_agent_api [req-59711ddd-d674-46f4-87e5-5bcf662669ba - - - - -] Only 0 of 1 DHCP agents associated with network '584d9ef6-3673-4cd5-beb9-87b3158d953a' are marked as active, so notifications may be sent to inactive agents.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:00:32.999 31354 WARNING neutron.api.rpc.agentnotifiers.dhcp_rpc_agent_api [req-59711ddd-d674-46f4-87e5-5bcf662669ba - - - - -] Only 0 of 1 DHCP agents associated with network '584d9ef6-3673-4cd5-beb9-87b3158d953a' are marked as active, so notifications may be sent to inactive agents.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:00:33.221 31354 WARNING neutron.api.rpc.agentnotifiers.dhcp_rpc_agent_api [req-59711ddd-d674-46f4-87e5-5bcf662669ba - - - - -] Only 0 of 1 DHCP agents associated with network '584d9ef6-3673-4cd5-beb9-87b3158d953a' are marked as active, so notifications may be sent to inactive agents.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:00:33.397 31354 WARNING neutron.api.rpc.agentnotifiers.dhcp_rpc_agent_api [req-59711ddd-d674-46f4-87e5-5bcf662669ba - - - - -] Only 0 of 1 DHCP agents associated with network '2eb2a376-5200-4093-8bb9-7eb4c263a96a' are marked as active, so notifications may be sent to inactive agents.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:00:33.579 31354 WARNING neutron.api.rpc.agentnotifiers.dhcp_rpc_agent_api [req-59711ddd-d674-46f4-87e5-5bcf662669ba - - - - -] Only 0 of 1 DHCP agents associated with network '584d9ef6-3673-4cd5-beb9-87b3158d953a' are marked as active, so notifications may be sent to inactive agents.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:00:58.417 31356 WARNING neutron.db.agents_db [req-2e313556-1454-459a-a85a-fb2c00b00ff2 - - - - -] Agent healthcheck: found 4 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:01:01.359 30524 ERROR neutron.agent.metadata.agent 
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:01:01.359 30524 ERROR neutron.agent.metadata.agent [-] Failed reporting state!
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:01:01.359 30524 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/metadata/agent.py", line 262, in _report_state
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:01:01.359 30524 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/rpc.py", line 88, in report_state
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:01:01.359 30524 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 238, in get
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:01:01.359 30524 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 336, in wait
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:01:01.359 30524 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 453, in _send
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:01:01.359 30524 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 464, in send
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:01:01.359 30524 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py", line 169, in call
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:01:01.359 30524 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/transport.py", line 97, in _send
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:01:01.359 30524 ERROR neutron.agent.metadata.agent     message = self.waiters.get(msg_id, timeout=timeout)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:01:01.359 30524 ERROR neutron.agent.metadata.agent MessagingTimeout: Timed out waiting for a reply to message ID faa6bf45021144419fadd858a2027677
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:01:01.359 30524 ERROR neutron.agent.metadata.agent     result = self._waiter.wait(msg_id, timeout)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:01:01.359 30524 ERROR neutron.agent.metadata.agent     retry=retry)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:01:01.359 30524 ERROR neutron.agent.metadata.agent     retry=self.retry)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:01:01.359 30524 ERROR neutron.agent.metadata.agent     return method(context, 'report_state', **kwargs)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:01:01.359 30524 ERROR neutron.agent.metadata.agent     timeout=timeout, retry=retry)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:01:01.359 30524 ERROR neutron.agent.metadata.agent     'to message ID %s' % msg_id)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:01:01.359 30524 ERROR neutron.agent.metadata.agent Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:01:01.359 30524 ERROR neutron.agent.metadata.agent     use_call=self.agent_state.get('start_flag'))
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:01:01.368 30524 WARNING oslo.service.loopingcall [-] Function 'neutron.agent.metadata.agent.UnixDomainMetadataProxy._report_state' run outlasted interval by 30.06 sec
/root/openstack/logs/2017-02-09/1428/l3-agent.log:2017-02-09 14:01:01.412 30525 ERROR neutron.common.rpc [req-dbc70f47-30b5-401a-9903-2108f4eac0fc - - - - -] Timeout in RPC method get_service_plugin_list. Waiting for 58 seconds before next attempt. If the server is not down, consider increasing the rpc_response_timeout option as Neutron server(s) may be overloaded and unable to respond quickly enough.
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:01.413 30523 ERROR neutron.agent.dhcp.agent 
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:01.413 30523 ERROR neutron.agent.dhcp.agent     ctx, self.agent_state, True)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:01.413 30523 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/dhcp/agent.py", line 687, in _report_state
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:01.413 30523 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/rpc.py", line 88, in report_state
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:01.413 30523 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 238, in get
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:01.413 30523 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 336, in wait
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:01.413 30523 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 453, in _send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:01.413 30523 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 464, in send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:01.413 30523 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py", line 169, in call
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:01.413 30523 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/transport.py", line 97, in _send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:01.413 30523 ERROR neutron.agent.dhcp.agent     message = self.waiters.get(msg_id, timeout=timeout)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:01.413 30523 ERROR neutron.agent.dhcp.agent MessagingTimeout: Timed out waiting for a reply to message ID b745fde7f37e4d0bbc7c1bdedfd23d6d
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:01.413 30523 ERROR neutron.agent.dhcp.agent [req-db353ce0-8a9f-469d-b738-2ea6c513094f - - - - -] Failed reporting state!
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:01.413 30523 ERROR neutron.agent.dhcp.agent     result = self._waiter.wait(msg_id, timeout)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:01.413 30523 ERROR neutron.agent.dhcp.agent     retry=retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:01.413 30523 ERROR neutron.agent.dhcp.agent     retry=self.retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:01.413 30523 ERROR neutron.agent.dhcp.agent     return method(context, 'report_state', **kwargs)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:01.413 30523 ERROR neutron.agent.dhcp.agent     timeout=timeout, retry=retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:01.413 30523 ERROR neutron.agent.dhcp.agent     'to message ID %s' % msg_id)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:01.413 30523 ERROR neutron.agent.dhcp.agent Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/l3-agent.log:2017-02-09 14:01:01.414 30525 WARNING neutron.common.rpc [req-dbc70f47-30b5-401a-9903-2108f4eac0fc - - - - -] Increasing timeout for get_service_plugin_list calls to 120 seconds. Restart the agent to restore it to the default value.
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:01.419 30523 WARNING oslo.service.loopingcall [req-db353ce0-8a9f-469d-b738-2ea6c513094f - - - - -] Function 'neutron.agent.dhcp.agent.DhcpAgentWithStateReport._report_state' run outlasted interval by 30.04 sec
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:01.420 30523 ERROR neutron.common.rpc [req-74aac553-679d-4f86-859f-1d43ff993821 - - - - -] Timeout in RPC method get_active_networks_info. Waiting for 38 seconds before next attempt. If the server is not down, consider increasing the rpc_response_timeout option as Neutron server(s) may be overloaded and unable to respond quickly enough.
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:01.421 30523 WARNING neutron.common.rpc [req-74aac553-679d-4f86-859f-1d43ff993821 - - - - -] Increasing timeout for get_active_networks_info calls to 120 seconds. Restart the agent to restore it to the default value.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:01:35.368 31356 WARNING neutron.db.agents_db [req-2e313556-1454-459a-a85a-fb2c00b00ff2 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:39.475 30523 ERROR neutron.agent.dhcp.agent 
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:39.475 30523 ERROR neutron.agent.dhcp.agent     active_networks = self.plugin_rpc.get_active_networks_info()
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:39.475 30523 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/dhcp/agent.py", line 159, in sync_state
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:39.475 30523 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/dhcp/agent.py", line 522, in get_active_networks_info
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:39.475 30523 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/common/rpc.py", line 138, in call
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:39.475 30523 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/common/rpc.py", line 157, in call
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:39.475 30523 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 238, in get
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:39.475 30523 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 336, in wait
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:39.475 30523 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 453, in _send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:39.475 30523 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 464, in send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:39.475 30523 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py", line 169, in call
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:39.475 30523 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/transport.py", line 97, in _send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:39.475 30523 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 196, in force_reraise
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:39.475 30523 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 220, in __exit__
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:39.475 30523 ERROR neutron.agent.dhcp.agent     host=self.host)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:39.475 30523 ERROR neutron.agent.dhcp.agent     message = self.waiters.get(msg_id, timeout=timeout)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:39.475 30523 ERROR neutron.agent.dhcp.agent MessagingTimeout: Timed out waiting for a reply to message ID ae7b4e7a2ef64982bcd09b342c552568
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:39.475 30523 ERROR neutron.agent.dhcp.agent [req-74aac553-679d-4f86-859f-1d43ff993821 - - - - -] Unable to sync network state.
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:39.475 30523 ERROR neutron.agent.dhcp.agent     result = self._waiter.wait(msg_id, timeout)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:39.475 30523 ERROR neutron.agent.dhcp.agent     retry=retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:39.475 30523 ERROR neutron.agent.dhcp.agent     retry=self.retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:39.475 30523 ERROR neutron.agent.dhcp.agent     return self._original_context.call(ctxt, method, **kwargs)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:39.475 30523 ERROR neutron.agent.dhcp.agent     self.force_reraise()
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:39.475 30523 ERROR neutron.agent.dhcp.agent     six.reraise(self.type_, self.value, self.tb)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:39.475 30523 ERROR neutron.agent.dhcp.agent     timeout=timeout, retry=retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:39.475 30523 ERROR neutron.agent.dhcp.agent     time.sleep(wait)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:39.475 30523 ERROR neutron.agent.dhcp.agent     'to message ID %s' % msg_id)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:39.475 30523 ERROR neutron.agent.dhcp.agent Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:01:40.028 30523 WARNING oslo.service.loopingcall [req-78c004db-57e4-499c-92a7-6b8a6a940174 - - - - -] Function 'neutron.agent.dhcp.agent.DhcpAgentWithStateReport._report_state' run outlasted interval by 8.61 sec
/root/openstack/logs/2017-02-09/1428/l3-agent.log:2017-02-09 14:01:59.246 30525 WARNING neutron.agent.l3.agent [req-dbc70f47-30b5-401a-9903-2108f4eac0fc - - - - -] l3-agent cannot contact neutron server to retrieve service plugins enabled. Check connectivity to neutron server. Retrying... Detailed message: Timed out waiting for a reply to message ID bd49ee138c894769af8f330223cfa014.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:02:49.378 31356 WARNING neutron.db.agents_db [req-2e313556-1454-459a-a85a-fb2c00b00ff2 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:02:56.549 31354 WARNING neutron.plugins.ml2.drivers.type_tunnel [req-bd97f63a-b40a-4cd9-85a9-1fed51ac79f0 - - - - -] Endpoint with ip x.x.x.3 already exists
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:10:50.381 31356 WARNING neutron.db.agents_db [req-2e313556-1454-459a-a85a-fb2c00b00ff2 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:11:27.384 31356 WARNING neutron.db.agents_db [req-2e313556-1454-459a-a85a-fb2c00b00ff2 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:12:04.386 31356 WARNING neutron.db.agents_db [req-2e313556-1454-459a-a85a-fb2c00b00ff2 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:12:09.689 31354 WARNING neutron.plugins.ml2.drivers.type_tunnel [req-535c59ca-f307-4902-853e-769142f82e9d - - - - -] Endpoint with ip x.x.x.3 already exists
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:13:01.938 31368 ERROR neutron.agent.linux.async_process [-] Error received from [ovsdb-client monitor Interface name,ofport,external_ids --format=json]: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:13:01.939 31368 ERROR neutron.agent.linux.async_process [-] Process [ovsdb-client monitor Interface name,ofport,external_ids --format=json] dies due to the error: None
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:13:05.155 33612 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip 'x.x.x.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:13:06.488 33631 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip 'x.x.x.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:13:07.746 33650 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip 'x.x.x.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:13:09.260 33669 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip 'x.x.x.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:13:10.751 33691 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip 'x.x.x.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:09.736 31058 ERROR oslo.messaging._drivers.impl_rabbit [-] [a7e5ca48-0603-4c36-aaca-88c87801385c] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41218
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:09.797 31055 ERROR oslo.messaging._drivers.impl_rabbit [-] [e3d86073-0518-46b6-b5db-0b54581168da] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41220
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:09.850 31075 ERROR oslo.messaging._drivers.impl_rabbit [-] [1c43b6e7-0bf4-4ead-b9b1-5ea09f26072b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41222
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:09.894 31067 ERROR oslo.messaging._drivers.impl_rabbit [-] [aae3ceb5-7a72-4b6b-87de-8bd093a357bb] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41224
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:09.920 31050 ERROR oslo.messaging._drivers.impl_rabbit [-] [def14f03-8c72-447e-97c4-07e5a9e763a1] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41226
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:09.939 31084 ERROR oslo.messaging._drivers.impl_rabbit [-] [550da33f-532e-4ddf-a48f-0ae997c1f9b3] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41228
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:14:10.056 30523 ERROR oslo.messaging._drivers.impl_rabbit [req-ac6bdecc-e1c1-46c3-87cc-f26dbcd8bab9 - - - - -] [a89b04ba-671c-4645-a4d0-b8b4ead93d4f] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: timed out. Trying again in 1 seconds. Client port: 41090
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:10.557 31019 ERROR oslo.messaging._drivers.impl_rabbit [-] [df72546b-0c76-427b-ba97-26b246a3ebb2] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41230
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:10.586 31081 ERROR oslo.messaging._drivers.impl_rabbit [-] [77378bcc-8d36-4b4e-95d6-455b48cf2597] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41232
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:10.619 31082 ERROR oslo.messaging._drivers.impl_rabbit [-] [2f5cb2a3-f7a9-48cd-8fc5-e169c401ede7] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41233
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:10.662 31063 ERROR oslo.messaging._drivers.impl_rabbit [-] [9cc573a9-c6b9-46d3-9032-3e561c5a1744] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41238
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:10.695 31077 ERROR oslo.messaging._drivers.impl_rabbit [-] [fa1d4b37-a8fd-4228-9a7e-d0c63f200a34] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41236
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:10.763 31042 ERROR oslo.messaging._drivers.impl_rabbit [-] [d32efac0-fe98-44a7-9be6-42b79a5d96bc] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41240
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:14:10.981 30555 ERROR oslo.messaging._drivers.impl_rabbit [-] [052cfb4c-b12a-4a0c-847f-9ff41de9b03e] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: timed out. Trying again in 1 seconds. Client port: 41262
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:10.997 31032 ERROR oslo.messaging._drivers.impl_rabbit [-] [f7164075-f742-48aa-a2bf-ebec9ec9a7d8] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41244
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:14:14.402 31356 WARNING neutron.db.agentschedulers_db [req-eab7666e-6ddc-4fc0-9fda-2bba7b7b88f0 - - - - -] No DHCP agents available, skipping rescheduling
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:15.397 31089 ERROR oslo.messaging._drivers.impl_rabbit [-] [24c6a2e6-d39a-4c43-b8b9-e3caca952188] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41242
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:15.453 31079 ERROR oslo.messaging._drivers.impl_rabbit [-] [296409c4-33e5-41fe-9313-c6cbae9f4d40] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41246
/root/openstack/logs/2017-02-09/1428/nova-scheduler.log:2017-02-09 14:14:15.488 30518 ERROR oslo.messaging._drivers.impl_rabbit [-] [0fc976b8-d2ed-4e46-8b8f-fc8751a71323] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41274
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:16.055 31028 ERROR oslo.messaging._drivers.impl_rabbit [-] [403ad90d-baed-4eb5-98c0-1f386222618b] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41248
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:16.099 31073 ERROR oslo.messaging._drivers.impl_rabbit [-] [dee27eef-da61-4d07-aa20-d9bc1f3f5608] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41250
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:16.142 31071 ERROR oslo.messaging._drivers.impl_rabbit [-] [040b7bfd-92c4-47e5-be0c-8c09c20ac0a9] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41254
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:16.194 31076 ERROR oslo.messaging._drivers.impl_rabbit [-] [7fd2019c-1d0b-484a-a4c3-b4554a32ddd5] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41252
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:16.266 31065 ERROR oslo.messaging._drivers.impl_rabbit [-] [b3c6d671-a8c4-4f79-814f-e5fe923fcc83] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41258
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:16.400 31029 ERROR oslo.messaging._drivers.impl_rabbit [-] [19ba1704-f380-4243-9b37-16421aacf971] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41177
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:14:16.416 30555 ERROR oslo_service.periodic_task 
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:14:16.416 30555 ERROR oslo_service.periodic_task     args=args, kwargs=kwargs)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:14:16.416 30555 ERROR oslo_service.periodic_task     args, kwargs)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:14:16.416 30555 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 6445, in update_available_resource
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:14:16.416 30555 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 6463, in _get_compute_nodes_in_db
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:14:16.416 30555 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/nova/conductor/rpcapi.py", line 236, in object_class_action_versions
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:14:16.416 30555 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 238, in get
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:14:16.416 30555 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 336, in wait
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:14:16.416 30555 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 453, in _send
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:14:16.416 30555 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 464, in send
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:14:16.416 30555 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py", line 169, in call
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:14:16.416 30555 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_messaging/transport.py", line 97, in _send
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:14:16.416 30555 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_service/periodic_task.py", line 220, in run_periodic_tasks
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:14:16.416 30555 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_versionedobjects/base.py", line 177, in wrapper
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:14:16.416 30555 ERROR oslo_service.periodic_task     message = self.waiters.get(msg_id, timeout=timeout)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:14:16.416 30555 ERROR oslo_service.periodic_task MessagingTimeout: Timed out waiting for a reply to message ID 5fd1a09116564be3b0aa97bd931d8d69
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:14:16.416 30555 ERROR oslo_service.periodic_task [req-e2a7c263-49d2-42f2-9a9d-d636712b8f0e - - - - -] Error during ComputeManager.update_available_resource
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:14:16.416 30555 ERROR oslo_service.periodic_task     result = self._waiter.wait(msg_id, timeout)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:14:16.416 30555 ERROR oslo_service.periodic_task     retry=retry)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:14:16.416 30555 ERROR oslo_service.periodic_task     retry=self.retry)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:14:16.416 30555 ERROR oslo_service.periodic_task     task(self, context)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:14:16.416 30555 ERROR oslo_service.periodic_task     timeout=timeout, retry=retry)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:14:16.416 30555 ERROR oslo_service.periodic_task     'to message ID %s' % msg_id)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:14:16.416 30555 ERROR oslo_service.periodic_task Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:14:16.416 30555 ERROR oslo_service.periodic_task     use_slave=True)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:14:16.416 30555 ERROR oslo_service.periodic_task     use_slave=use_slave)
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:16.451 31035 ERROR oslo.messaging._drivers.impl_rabbit [-] [eff57b93-ceea-4030-989b-09ddc8668d73] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41176
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:16.527 31047 ERROR oslo.messaging._drivers.impl_rabbit [-] [ea7cc6de-3c50-4b8c-b2de-41ca84959c27] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41180
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:16.865 31031 ERROR oslo.messaging._drivers.impl_rabbit [-] [ed829b46-dcfc-47c9-bd3f-02e3e0737c05] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41186
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:17.074 31033 ERROR oslo.messaging._drivers.impl_rabbit [-] [06b588a9-4256-4972-880c-a2d6aba8cd06] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41182
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:18.490 31018 ERROR oslo.messaging._drivers.impl_rabbit [-] [42b065ab-5351-4f9e-9e52-97a5800d5fb5] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41200
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:18.496 31026 ERROR oslo.messaging._drivers.impl_rabbit [-] [5341b94f-f927-44c1-ae72-6ada56680e87] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41206
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:18.502 31030 ERROR oslo.messaging._drivers.impl_rabbit [-] [e6ab00ae-16f8-4c76-8cc9-66412a8bf9db] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41208
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:18.537 31058 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-consoleauth.log:2017-02-09 14:14:18.695 30515 ERROR oslo.messaging._drivers.impl_rabbit [-] [de33a51c-5476-40f8-8368-ae4dfe1af8dd] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41256
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:18.785 31029 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:18.872 31035 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:18.958 31032 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:18.983 31047 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:19.021 31031 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:19.064 31089 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:19.174 31079 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:19.221 31028 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:19.270 31073 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:19.311 31055 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:19.322 31071 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:19.393 31075 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:19.482 31067 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:19.551 31050 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:19.621 31084 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:19.738 31019 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:19.779 31081 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:20.106 31076 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:20.155 31065 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:20.165 31033 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:14:20.997 30555 ERROR oslo.messaging._drivers.impl_rabbit [-] [9638ccec-7c08-4d7b-bbb2-bbfdd585c75c] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41456
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:14:21.001 31020 ERROR oslo.messaging._drivers.impl_rabbit [-] [20c80fb7-2f48-4b2f-b7b8-bc1b0209e0b6] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41188
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:23.877 33758 WARNING oslo_db.sqlalchemy.engines [req-b95b0d7a-bf1d-427c-a907-ecedc3f1d603 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:23.879 33759 WARNING oslo_db.sqlalchemy.engines [req-ef01a9bc-1e60-420e-8e4c-7b079582bc1b - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:23.886 33764 WARNING oslo_db.sqlalchemy.engines [req-ae901aa0-22c4-4f7e-852c-7c2470c9fcf7 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:23.889 33755 WARNING oslo_db.sqlalchemy.engines [req-8e44aaf1-231b-4157-86cf-c88797c245b2 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:23.906 33760 WARNING oslo_db.sqlalchemy.engines [req-1831b21d-90fd-4590-b7cd-3068192863e3 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:23.911 33766 WARNING oslo_db.sqlalchemy.engines [req-637d26ac-d754-46f2-9458-90ca86ca376c - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:23.912 33767 WARNING oslo_db.sqlalchemy.engines [req-3f645d5b-2628-49b7-a932-aeb5600a9a6d - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:23.920 33765 WARNING oslo_db.sqlalchemy.engines [req-7a120de0-8800-49e2-9ac1-6d4a770f4d23 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:23.920 33768 WARNING oslo_db.sqlalchemy.engines [req-ada62595-5d4d-4db8-a677-8162eac35214 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:23.925 33769 WARNING oslo_db.sqlalchemy.engines [req-ebb0c162-2ec3-4dc8-8510-6520d80c7b19 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:23.927 33756 WARNING oslo_db.sqlalchemy.engines [req-52bb7a78-b955-489b-bd88-9cf9f3fb825f - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:23.927 33757 WARNING oslo_db.sqlalchemy.engines [req-f1f0b484-70fc-4a68-8c1e-44266a2e6172 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:23.929 33770 WARNING oslo_db.sqlalchemy.engines [req-0a99491d-e95f-485b-afb4-0cdb54680657 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:23.947 33771 WARNING oslo_db.sqlalchemy.engines [req-a21e4ac6-3432-46f1-a196-ef7c58a0b24e - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:23.947 33774 WARNING oslo_db.sqlalchemy.engines [req-30d1b945-b682-4a59-b774-afbece1336b8 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:23.950 33772 WARNING oslo_db.sqlalchemy.engines [req-0da1836c-35ae-40c2-bab3-b5323c186bea - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:23.950 33776 WARNING oslo_db.sqlalchemy.engines [req-5ae380ff-03fd-4be7-afff-ef10a11d1038 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:23.952 33779 WARNING oslo_db.sqlalchemy.engines [req-f2db9b7b-ec1c-461a-97c6-cd4f4bb4d74d - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:23.957 33780 WARNING oslo_db.sqlalchemy.engines [req-f5ed3362-4f02-485f-9eb5-b547bc66df6f - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:23.963 33781 WARNING oslo_db.sqlalchemy.engines [req-5fd6830e-aed7-42d4-b7e9-fab785668b85 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:23.967 33782 WARNING oslo_db.sqlalchemy.engines [req-9fa07fa9-b0ce-40be-8344-798bdd3a66b0 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:23.973 33783 WARNING oslo_db.sqlalchemy.engines [req-eaad4cd3-e6bc-4841-bd17-5f55389603cd - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:23.977 33784 WARNING oslo_db.sqlalchemy.engines [req-2b8563d6-4e1f-431e-a359-7335ec55067c - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:23.980 33785 WARNING oslo_db.sqlalchemy.engines [req-66f2a008-45e1-4546-952e-46ce3e11299b - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:23.987 33786 WARNING oslo_db.sqlalchemy.engines [req-2c0e4a78-6b1a-4765-b84c-f91ea812b920 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:23.992 33787 WARNING oslo_db.sqlalchemy.engines [req-7f8237cf-6346-41b9-a357-5e72941eb28a - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:23.996 33788 WARNING oslo_db.sqlalchemy.engines [req-f405dfd6-fc72-4597-ade8-68a277cbe621 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.001 33789 WARNING oslo_db.sqlalchemy.engines [req-1cdfb434-4baf-43e3-8028-0e6a300ea167 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.006 33790 WARNING oslo_db.sqlalchemy.engines [req-2c5c890f-403a-47ac-a0ef-af7cfcfb2aa4 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.008 33791 WARNING oslo_db.sqlalchemy.engines [req-8994527e-f338-4459-ada2-c8550be397bc - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.015 33792 WARNING oslo_db.sqlalchemy.engines [req-497e3379-ec06-4438-af5b-e043e77cb32e - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.019 33793 WARNING oslo_db.sqlalchemy.engines [req-f655bcb4-4663-4402-9e2e-93eb24de09aa - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.025 33794 WARNING oslo_db.sqlalchemy.engines [req-8ffc341a-f3ba-4add-963a-a153f5892dd2 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.029 33795 WARNING oslo_db.sqlalchemy.engines [req-9764e394-0cd5-40de-9ecf-d6a767e1adda - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.033 33796 WARNING oslo_db.sqlalchemy.engines [req-5b765cbd-efac-40b0-83ca-37996abffc1e - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.036 33797 WARNING oslo_db.sqlalchemy.engines [req-91c5199d-382a-4156-9a5a-e9beadb5e35b - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.041 33798 WARNING oslo_db.sqlalchemy.engines [req-ecfbcd50-f6b7-4b1c-84e5-172ae35d4d65 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.047 33799 WARNING oslo_db.sqlalchemy.engines [req-893497d9-3f29-4bd9-9e66-7dc478a10b23 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.053 33800 WARNING oslo_db.sqlalchemy.engines [req-d6c1c100-542a-4440-aa70-38da297f971c - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.057 33801 WARNING oslo_db.sqlalchemy.engines [req-4889af52-9e82-47e3-b37e-eb0fe022b82f - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.061 33802 WARNING oslo_db.sqlalchemy.engines [req-65477a7c-b63d-427b-b54d-b1566a5f8c9e - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.065 33803 WARNING oslo_db.sqlalchemy.engines [req-8abe0c73-18d2-4f73-938f-da59b8e8fb4e - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.068 33804 WARNING oslo_db.sqlalchemy.engines [req-e13631fe-e8c5-4fc5-be29-ff95fa05a264 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.074 33805 WARNING oslo_db.sqlalchemy.engines [req-1d33adf0-d05c-4484-848e-db57b172ef32 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.078 33806 WARNING oslo_db.sqlalchemy.engines [req-f4c1e3ce-dfb1-4b50-89af-72b623b372c2 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.084 33807 WARNING oslo_db.sqlalchemy.engines [req-c2591c43-a8af-4a3b-9c33-7897f9b6e2e1 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.087 33809 WARNING oslo_db.sqlalchemy.engines [req-f3b8c699-12f0-47a2-bb26-0c3e86651ae6 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.091 33810 WARNING oslo_db.sqlalchemy.engines [req-54767c23-d4e1-45a8-b75d-d97d6e572f98 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.097 33811 WARNING oslo_db.sqlalchemy.engines [req-ff31df9d-fa8a-4a7f-b758-3f5958818182 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.097 33812 WARNING oslo_db.sqlalchemy.engines [req-371a7495-65d1-4cff-8c57-58e9f7e9e269 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.104 33813 WARNING oslo_db.sqlalchemy.engines [req-32a8b12e-6de2-49d7-8d60-bc44c5bca4da - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.111 33814 WARNING oslo_db.sqlalchemy.engines [req-3e392e03-5878-47c6-8d74-de7975b5dffc - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.114 33815 WARNING oslo_db.sqlalchemy.engines [req-9a15c886-cd52-45cb-8134-51657ecc536d - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.118 33816 WARNING oslo_db.sqlalchemy.engines [req-0cdf9e55-56dc-4adc-823b-2ae1faaf38e1 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.123 33817 WARNING oslo_db.sqlalchemy.engines [req-53481f1c-fdf8-4fc3-9f28-910bc8efcc71 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.123 33818 WARNING oslo_db.sqlalchemy.engines [req-ab4669fb-16d4-4c82-8138-e83f74262cac - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.130 33819 WARNING oslo_db.sqlalchemy.engines [req-2df9c03c-eca4-413a-8ef4-0594340279a7 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.137 33820 WARNING oslo_db.sqlalchemy.engines [req-0c2b3275-e63a-42ac-a54b-299422b3ecee - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.139 33821 WARNING oslo_db.sqlalchemy.engines [req-4750a315-a453-4b53-be85-66aa2b616035 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.144 33822 WARNING oslo_db.sqlalchemy.engines [req-d3666abe-84d4-44d2-b868-fc64fd4c94e2 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.150 33823 WARNING oslo_db.sqlalchemy.engines [req-da5e346a-9626-45fe-bd11-1b87ddb60620 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.150 33824 WARNING oslo_db.sqlalchemy.engines [req-7ea71e3f-5f80-4bf2-a637-03087ef9a753 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.156 33825 WARNING oslo_db.sqlalchemy.engines [req-ea18cbbf-77d8-4476-804c-c2ab15655cae - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.163 33826 WARNING oslo_db.sqlalchemy.engines [req-1e4aa746-c92d-4771-bf4c-8043d1583a2b - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.166 33827 WARNING oslo_db.sqlalchemy.engines [req-ddf4d6ae-a3a9-4a55-be22-1807f872caf7 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.170 33828 WARNING oslo_db.sqlalchemy.engines [req-b51d382c-d1fa-4886-8d5f-00a5468638a1 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.175 33829 WARNING oslo_db.sqlalchemy.engines [req-8b6ce9ac-4660-4c32-b5f1-75f40a73cdc8 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.176 33830 WARNING oslo_db.sqlalchemy.engines [req-0b034862-bdf2-4eb6-8f13-d5230052ad3a - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.185 33831 WARNING oslo_db.sqlalchemy.engines [req-338f6aa2-491e-40f6-be55-2d407fa8b6ed - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.190 33832 WARNING oslo_db.sqlalchemy.engines [req-3bf0d03c-41f8-4f29-9ac9-ae7a1f47fe1e - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.195 33833 WARNING oslo_db.sqlalchemy.engines [req-47ce41c7-e499-4bfc-afe3-85d5627b5762 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.198 33834 WARNING oslo_db.sqlalchemy.engines [req-7f36035c-e0ba-4eeb-bf5d-575815309db1 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.202 33835 WARNING oslo_db.sqlalchemy.engines [req-db565f27-54b7-4a7d-a9f7-22c4f41a610f - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.209 33836 WARNING oslo_db.sqlalchemy.engines [req-238ab041-a2f2-4424-a386-1c869ae3bd10 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.211 33837 WARNING oslo_db.sqlalchemy.engines [req-e6a1cf59-b409-4147-8224-99833ab78d29 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.214 33838 WARNING oslo_db.sqlalchemy.engines [req-4e2a54a8-eeea-49d8-9b96-d1a132824729 - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:14:24.219 33839 WARNING oslo_db.sqlalchemy.engines [req-43b49fcc-9911-4099-b79c-b4e535d0babb - - - - -] SQL connection failed. -1 attempts left.
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 100  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 101  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 102  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 103  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 104  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 105  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 106  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 107  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 108  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 109  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 10  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 110  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 111  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 112  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 113  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 114  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 115  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 116  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 117  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 118  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 119  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 11  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 120  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 121  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 122  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 123  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 124  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 125  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 127  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 128  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 129  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 12  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 130  user: 'keystone'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 131  user: 'keystone'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 132  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 133  user: 'keystone'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 134  user: 'keystone'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 135  user: 'keystone'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 139  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 13  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 141  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 142  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 143  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 145  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 146  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 147  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 148  user: 'neutron'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 14  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 15  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 16  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 17  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 18  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 19  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 1  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 20  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 21  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 22  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 23  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 24  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 25  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 26  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 27  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 28  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 29  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 2  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 30  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 31  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 32  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 33  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 34  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 35  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 36  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 37  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 38  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 3  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 40  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 41  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 42  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 43  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 44  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 45  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 46  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 47  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 48  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 49  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 4  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 50  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 51  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 52  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 53  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 54  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 55  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 56  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 57  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 58  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 59  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 5  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 60  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 61  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 62  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 63  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 64  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 65  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 66  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 67  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 68  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 69  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 6  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 70  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 71  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 72  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 73  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 74  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 75  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 76  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 77  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 78  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 79  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 7  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 80  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 81  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 82  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 83  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 84  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 85  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 86  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 87  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 88  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 89  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 8  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 90  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 91  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 92  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 93  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 94  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 95  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 96  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 97  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 98  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 99  user: 'nova'
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:14:25 30990 [Warning] /usr/sbin/mysqld: Forcing close of thread 9  user: 'nova'
/root/openstack/logs/2017-02-09/1428/l3-agent.log:2017-02-09 14:14:29.355 30525 ERROR oslo.messaging._drivers.impl_rabbit [-] [70ee6bcd-f503-43ee-9501-e5f8684a1a84] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: timed out. Trying again in 1 seconds. Client port: 41088
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:14:40.084 30523 ERROR oslo.messaging._drivers.impl_rabbit [-] [e1be9280-0b05-40d7-b5a7-0ab2862609cc] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41082
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:14:43.989 31355 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:14:44.352 31354 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/l3-agent.log:2017-02-09 14:14:59.414 30525 ERROR oslo.messaging._drivers.impl_rabbit [-] [efea7aea-ba0d-4c5e-84bf-3c4f7ae1fe2e] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41084
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:15:01.012 30555 ERROR oslo.messaging._drivers.impl_rabbit [-] [95b9be18-d36b-4829-885b-b27e2a4e6f43] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: Too many heartbeats missed. Trying again in 1 seconds. Client port: 41260
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:15:14.377 31356 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:15:16.427 30555 ERROR oslo_service.periodic_task 
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:15:16.427 30555 ERROR oslo_service.periodic_task     args=args, kwargs=kwargs)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:15:16.427 30555 ERROR oslo_service.periodic_task     args, kwargs)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:15:16.427 30555 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 1637, in _sync_scheduler_instance_info
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:15:16.427 30555 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/nova/conductor/rpcapi.py", line 236, in object_class_action_versions
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:15:16.427 30555 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 238, in get
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:15:16.427 30555 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 336, in wait
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:15:16.427 30555 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 453, in _send
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:15:16.427 30555 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 464, in send
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:15:16.427 30555 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py", line 169, in call
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:15:16.427 30555 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_messaging/transport.py", line 97, in _send
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:15:16.427 30555 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_service/periodic_task.py", line 220, in run_periodic_tasks
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:15:16.427 30555 ERROR oslo_service.periodic_task   File "/usr/lib/python2.7/site-packages/oslo_versionedobjects/base.py", line 177, in wrapper
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:15:16.427 30555 ERROR oslo_service.periodic_task     message = self.waiters.get(msg_id, timeout=timeout)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:15:16.427 30555 ERROR oslo_service.periodic_task MessagingTimeout: Timed out waiting for a reply to message ID 6dc439e847b9423185896e291593d615
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:15:16.427 30555 ERROR oslo_service.periodic_task [req-e2a7c263-49d2-42f2-9a9d-d636712b8f0e - - - - -] Error during ComputeManager._sync_scheduler_instance_info
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:15:16.427 30555 ERROR oslo_service.periodic_task     result = self._waiter.wait(msg_id, timeout)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:15:16.427 30555 ERROR oslo_service.periodic_task     retry=retry)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:15:16.427 30555 ERROR oslo_service.periodic_task     retry=self.retry)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:15:16.427 30555 ERROR oslo_service.periodic_task     task(self, context)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:15:16.427 30555 ERROR oslo_service.periodic_task     timeout=timeout, retry=retry)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:15:16.427 30555 ERROR oslo_service.periodic_task     'to message ID %s' % msg_id)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:15:16.427 30555 ERROR oslo_service.periodic_task Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:15:16.427 30555 ERROR oslo_service.periodic_task     use_slave=True)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:15:16.523 30523 INFO oslo.messaging._drivers.impl_rabbit [-] A recoverable connection/channel error occurred, trying to reconnect: Too many heartbeats missed
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:15:55.533 35515 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip 'x.x.x.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:15:57.003 35534 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip 'x.x.x.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:15:58.263 35553 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip 'x.x.x.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:15:59.627 35572 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip 'x.x.x.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:16:00.850 35595 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip 'x.x.x.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:16:29.707 35699 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip 'x.x.x.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:16:30 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:16:30 36183 [Warning] Buffered warning: Changed limits: max_connections: 214 (requested 500)
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:16:30 36183 [Warning] Buffered warning: Changed limits: max_open_files: 1024 (requested 5000)
/root/openstack/logs/2017-02-09/1428/mysqld.log:2017-02-09 14:16:30 36183 [Warning] Buffered warning: Changed limits: table_open_cache: 400 (requested 2000)
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:16:30.013 35678 WARNING neutron.agent.securitygroups_rpc [-] Driver configuration doesn't match with enable_security_group
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:16:30.138 35678 WARNING oslo_db.sqlalchemy.engines [-] SQL connection failed. 10 attempts left.
/root/openstack/logs/2017-02-09/1428/nova-novncproxy.log:2017-02-09 14:16:30.169 35675 WARNING oslo_reports.guru_meditation_report [-] Guru meditation now registers SIGUSR1 and SIGUSR2 by default for backward compatibility. SIGUSR1 will no longer be registered in a future release, so please use SIGUSR2 to generate reports.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:16:30.794 35930 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip 'x.x.x.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/nova-scheduler.log:2017-02-09 14:16:31.117 35671 WARNING oslo_reports.guru_meditation_report [-] Guru meditation now registers SIGUSR1 and SIGUSR2 by default for backward compatibility. SIGUSR1 will no longer be registered in a future release, so please use SIGUSR2 to generate reports.
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:16:31.192 35673 WARNING oslo_reports.guru_meditation_report [-] Guru meditation now registers SIGUSR1 and SIGUSR2 by default for backward compatibility. SIGUSR1 will no longer be registered in a future release, so please use SIGUSR2 to generate reports.
/root/openstack/logs/2017-02-09/1428/nova-api.log:2017-02-09 14:16:31.235 35667 WARNING oslo_reports.guru_meditation_report [-] Guru meditation now registers SIGUSR1 and SIGUSR2 by default for backward compatibility. SIGUSR1 will no longer be registered in a future release, so please use SIGUSR2 to generate reports.
/root/openstack/logs/2017-02-09/1428/nova-consoleauth.log:2017-02-09 14:16:31.272 35669 WARNING oslo_reports.guru_meditation_report [-] Guru meditation now registers SIGUSR1 and SIGUSR2 by default for backward compatibility. SIGUSR1 will no longer be registered in a future release, so please use SIGUSR2 to generate reports.
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:16:31.627 35730 WARNING oslo_reports.guru_meditation_report [-] Guru meditation now registers SIGUSR1 and SIGUSR2 by default for backward compatibility. SIGUSR1 will no longer be registered in a future release, so please use SIGUSR2 to generate reports.
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:16:31.924 35730 WARNING os_brick.initiator.connectors.remotefs [req-198a31ea-f0e4-4806-9190-a5fa5758f416 - - - - -] Connection details not present. RemoteFsClient may not initialize properly.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:16:32.711 36226 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip 'x.x.x.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:16:33.349 35730 WARNING nova.compute.monitors [req-3264a348-cd3d-400a-8902-66b77390d014 - - - - -] Excluding nova.compute.monitors.cpu monitor virt_driver. Not in the list of enabled monitors (CONF.compute_monitors).
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:16:33.988 35730 WARNING nova.scheduler.client.report [req-3264a348-cd3d-400a-8902-66b77390d014 - - - - -] No authentication information found for placement API. Optional use of placement API for reporting is now disabled.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:16:34.149 36473 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip 'x.x.x.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:16:35.533 36556 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip 'x.x.x.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:16:40.353 35678 WARNING neutron.agent.securitygroups_rpc [req-b558b153-7cb6-4d74-9f7a-19709b4ef543 - - - - -] Driver configuration doesn't match with enable_security_group
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:16:40.412 35678 WARNING neutron.api.extensions [req-b558b153-7cb6-4d74-9f7a-19709b4ef543 - - - - -] Extension dns-integration not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:16:40.419 35678 WARNING neutron.api.extensions [req-b558b153-7cb6-4d74-9f7a-19709b4ef543 - - - - -] Extension ip_allocation not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:16:40.420 35678 WARNING neutron.api.extensions [req-b558b153-7cb6-4d74-9f7a-19709b4ef543 - - - - -] Extension l2_adjacency not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:16:40.426 35678 WARNING neutron.api.extensions [req-b558b153-7cb6-4d74-9f7a-19709b4ef543 - - - - -] Extension metering not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:16:40.435 35678 WARNING neutron.api.extensions [req-b558b153-7cb6-4d74-9f7a-19709b4ef543 - - - - -] Extension qos not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:16:40.440 35678 WARNING neutron.api.extensions [req-b558b153-7cb6-4d74-9f7a-19709b4ef543 - - - - -] Extension router-service-type not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:16:40.443 35678 WARNING neutron.api.extensions [req-b558b153-7cb6-4d74-9f7a-19709b4ef543 - - - - -] Extension segment not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:16:40.449 35678 WARNING neutron.api.extensions [req-b558b153-7cb6-4d74-9f7a-19709b4ef543 - - - - -] Extension trunk not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:16:40.450 35678 WARNING neutron.api.extensions [req-b558b153-7cb6-4d74-9f7a-19709b4ef543 - - - - -] Extension trunk-details not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:16:40.450 35678 WARNING neutron.api.extensions [req-b558b153-7cb6-4d74-9f7a-19709b4ef543 - - - - -] Extension vlan-transparent not supported by any of loaded plugins
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:17:05.197 36890 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip 'x.x.x.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:17:06.501 36910 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip 'x.x.x.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:17:07.744 36953 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip 'x.x.x.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:17:09.116 37207 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip 'x.x.x.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:17:10.499 37228 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip 'x.x.x.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:17:25.695 36613 WARNING neutron.db.agents_db [req-56239435-2e26-438c-bb21-56f82ece6e82 - - - - -] Agent healthcheck: found 5 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:17:29.608 35679 ERROR neutron.agent.dhcp.agent 
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:17:29.608 35679 ERROR neutron.agent.dhcp.agent     ctx, self.agent_state, True)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:17:29.608 35679 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/dhcp/agent.py", line 687, in _report_state
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:17:29.608 35679 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/rpc.py", line 88, in report_state
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:17:29.608 35679 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 238, in get
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:17:29.608 35679 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 336, in wait
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:17:29.608 35679 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 453, in _send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:17:29.608 35679 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 464, in send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:17:29.608 35679 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py", line 169, in call
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:17:29.608 35679 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/transport.py", line 97, in _send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:17:29.608 35679 ERROR neutron.agent.dhcp.agent     message = self.waiters.get(msg_id, timeout=timeout)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:17:29.608 35679 ERROR neutron.agent.dhcp.agent MessagingTimeout: Timed out waiting for a reply to message ID ba2be367d2784dda8f0c1c88cdd76e75
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:17:29.608 35679 ERROR neutron.agent.dhcp.agent [req-c7efc9fb-eb62-4c74-bf77-1888acaf9635 - - - - -] Failed reporting state!
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:17:29.608 35679 ERROR neutron.agent.dhcp.agent     result = self._waiter.wait(msg_id, timeout)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:17:29.608 35679 ERROR neutron.agent.dhcp.agent     retry=retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:17:29.608 35679 ERROR neutron.agent.dhcp.agent     retry=self.retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:17:29.608 35679 ERROR neutron.agent.dhcp.agent     return method(context, 'report_state', **kwargs)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:17:29.608 35679 ERROR neutron.agent.dhcp.agent     timeout=timeout, retry=retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:17:29.608 35679 ERROR neutron.agent.dhcp.agent     'to message ID %s' % msg_id)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:17:29.608 35679 ERROR neutron.agent.dhcp.agent Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:17:29.612 35679 WARNING oslo.service.loopingcall [req-c7efc9fb-eb62-4c74-bf77-1888acaf9635 - - - - -] Function 'neutron.agent.dhcp.agent.DhcpAgentWithStateReport._report_state' run outlasted interval by 30.06 sec
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:17:29.614 35679 ERROR neutron.common.rpc [req-13fe2054-55ac-4d2f-ac65-a73ab8f5ab2c - - - - -] Timeout in RPC method get_active_networks_info. Waiting for 33 seconds before next attempt. If the server is not down, consider increasing the rpc_response_timeout option as Neutron server(s) may be overloaded and unable to respond quickly enough.
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:17:29.614 35679 WARNING neutron.common.rpc [req-13fe2054-55ac-4d2f-ac65-a73ab8f5ab2c - - - - -] Increasing timeout for get_active_networks_info calls to 120 seconds. Restart the agent to restore it to the default value.
/root/openstack/logs/2017-02-09/1428/l3-agent.log:2017-02-09 14:17:29.628 35682 ERROR neutron.common.rpc [req-d9a93f9b-4bb6-4a83-ae9a-d69401849132 - - - - -] Timeout in RPC method get_service_plugin_list. Waiting for 47 seconds before next attempt. If the server is not down, consider increasing the rpc_response_timeout option as Neutron server(s) may be overloaded and unable to respond quickly enough.
/root/openstack/logs/2017-02-09/1428/l3-agent.log:2017-02-09 14:17:29.633 35682 WARNING neutron.common.rpc [req-d9a93f9b-4bb6-4a83-ae9a-d69401849132 - - - - -] Increasing timeout for get_service_plugin_list calls to 120 seconds. Restart the agent to restore it to the default value.
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:17:29.666 35680 ERROR neutron.agent.metadata.agent 
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:17:29.666 35680 ERROR neutron.agent.metadata.agent [-] Failed reporting state!
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:17:29.666 35680 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/metadata/agent.py", line 262, in _report_state
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:17:29.666 35680 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/rpc.py", line 88, in report_state
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:17:29.666 35680 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 238, in get
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:17:29.666 35680 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 336, in wait
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:17:29.666 35680 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 453, in _send
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:17:29.666 35680 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 464, in send
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:17:29.666 35680 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py", line 169, in call
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:17:29.666 35680 ERROR neutron.agent.metadata.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/transport.py", line 97, in _send
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:17:29.666 35680 ERROR neutron.agent.metadata.agent     message = self.waiters.get(msg_id, timeout=timeout)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:17:29.666 35680 ERROR neutron.agent.metadata.agent MessagingTimeout: Timed out waiting for a reply to message ID 3b8f6e173d64414d9babbf4f06a31cc9
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:17:29.666 35680 ERROR neutron.agent.metadata.agent     result = self._waiter.wait(msg_id, timeout)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:17:29.666 35680 ERROR neutron.agent.metadata.agent     retry=retry)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:17:29.666 35680 ERROR neutron.agent.metadata.agent     retry=self.retry)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:17:29.666 35680 ERROR neutron.agent.metadata.agent     return method(context, 'report_state', **kwargs)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:17:29.666 35680 ERROR neutron.agent.metadata.agent     timeout=timeout, retry=retry)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:17:29.666 35680 ERROR neutron.agent.metadata.agent     'to message ID %s' % msg_id)
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:17:29.666 35680 ERROR neutron.agent.metadata.agent Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:17:29.666 35680 ERROR neutron.agent.metadata.agent     use_call=self.agent_state.get('start_flag'))
/root/openstack/logs/2017-02-09/1428/metadata-agent.log:2017-02-09 14:17:29.677 35680 WARNING oslo.service.loopingcall [-] Function 'neutron.agent.metadata.agent.UnixDomainMetadataProxy._report_state' run outlasted interval by 30.05 sec
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b] 
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]     args, kwargs)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b] ConnectFailure: Unable to establish connection to http://mysql-001.vassilevski.com:9696/v2.0/ports.json?tenant_id=c674ba8aac9545daa2b9184cd2e984f7&device_id=efca1ee1-f6dd-4aea-b584-775c7b2de29b: HTTPConnectionPool(host='mysql-001.vassilevski.com', port=9696): Max retries exceeded with url: /v2.0/ports.json?tenant_id=c674ba8aac9545daa2b9184cd2e984f7&device_id=efca1ee1-f6dd-4aea-b584-775c7b2de29b (Caused by NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x7ea4150>: Failed to establish a new connection: [Errno 111] ECONNREFUSED',))
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]     data = client.list_ports(**search_opts)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]   File "/usr/lib/python2.7/site-packages/keystoneauth1/adapter.py", line 112, in request
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]   File "/usr/lib/python2.7/site-packages/keystoneauth1/session.py", line 555, in request
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]   File "/usr/lib/python2.7/site-packages/keystoneauth1/session.py", line 605, in _send_request
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]   File "/usr/lib/python2.7/site-packages/neutronclient/client.py", line 299, in request
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]   File "/usr/lib/python2.7/site-packages/neutronclient/client.py", line 311, in do_request
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]   File "/usr/lib/python2.7/site-packages/neutronclient/v2_0/client.py", line 288, in do_request
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]   File "/usr/lib/python2.7/site-packages/neutronclient/v2_0/client.py", line 337, in retry_request
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]   File "/usr/lib/python2.7/site-packages/neutronclient/v2_0/client.py", line 360, in get
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]   File "/usr/lib/python2.7/site-packages/neutronclient/v2_0/client.py", line 375, in list
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]   File "/usr/lib/python2.7/site-packages/neutronclient/v2_0/client.py", line 390, in _pagination
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]   File "/usr/lib/python2.7/site-packages/neutronclient/v2_0/client.py", line 742, in list_ports
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]   File "/usr/lib/python2.7/site-packages/nova/compute/manager.py", line 5766, in _heal_instance_info_cache
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]   File "/usr/lib/python2.7/site-packages/nova/network/base_api.py", line 249, in get_instance_nw_info
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]   File "/usr/lib/python2.7/site-packages/nova/network/neutronv2/api.py", line 1283, in _get_instance_nw_info
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]   File "/usr/lib/python2.7/site-packages/nova/network/neutronv2/api.py", line 2166, in _build_network_info_model
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]   File "/usr/lib/python2.7/site-packages/nova/network/neutronv2/api.py", line 97, in wrapper
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]   File "/usr/lib/python2.7/site-packages/nova/network/neutronv2/api.py", line 97, in wrapper
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]   File "/usr/lib/python2.7/site-packages/nova/network/neutronv2/api.py", line 97, in wrapper
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]   File "/usr/lib/python2.7/site-packages/nova/network/neutronv2/api.py", line 97, in wrapper
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]   File "/usr/lib/python2.7/site-packages/nova/network/neutronv2/api.py", line 97, in wrapper
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]   File "/usr/lib/python2.7/site-packages/positional/__init__.py", line 101, in inner
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]   File "/usr/lib/python2.7/site-packages/wrapt/wrappers.py", line 561, in __call__
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]     for r in self._pagination(collection, path, **params):
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]     headers=headers, params=params)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]     headers=headers, params=params)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]     **_params)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]     preexisting_port_ids)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]     raise exceptions.ConnectFailure(msg)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]     resp, replybody = self.httpclient.do_request(action, method, body=body)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]     resp = send(**kwargs)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]     resp = super(SessionClient, self).request(*args, **kwargs)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]     res = self.get(path, params=params)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]     result = self._get_instance_nw_info(context, instance, **kwargs)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]     ret = obj(*args, **kwargs)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]     ret = obj(*args, **kwargs)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]     ret = obj(*args, **kwargs)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]     ret = obj(*args, **kwargs)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]     ret = obj(*args, **kwargs)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]     return self.request(url, method, **kwargs)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]     return self.session.request(url, method, **kwargs)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]     return wrapped(*args, **kwargs)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b]     self.network_api.get_instance_nw_info(context, instance)
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b] Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/nova-compute.log:2017-02-09 14:17:35.887 35730 ERROR nova.compute.manager [req-0bb6f5b3-7f3a-46a0-ad6b-4ce0d2a6761a - - - - -] [instance: efca1ee1-f6dd-4aea-b584-775c7b2de29b] An error occurred while refreshing the network cache.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:17:59.027 37345 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip 'x.x.x.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:18:00.621 37613 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip 'x.x.x.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:18:01.962 37826 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip 'x.x.x.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:18:02.640 36613 WARNING neutron.db.agents_db [req-56239435-2e26-438c-bb21-56f82ece6e82 - - - - -] Agent healthcheck: found 3 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:18:02.799 35679 ERROR neutron.agent.dhcp.agent 
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:18:02.799 35679 ERROR neutron.agent.dhcp.agent     active_networks = self.plugin_rpc.get_active_networks_info()
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:18:02.799 35679 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/dhcp/agent.py", line 159, in sync_state
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:18:02.799 35679 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/agent/dhcp/agent.py", line 522, in get_active_networks_info
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:18:02.799 35679 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/common/rpc.py", line 138, in call
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:18:02.799 35679 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/neutron/common/rpc.py", line 157, in call
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:18:02.799 35679 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 238, in get
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:18:02.799 35679 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 336, in wait
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:18:02.799 35679 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 453, in _send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:18:02.799 35679 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/_drivers/amqpdriver.py", line 464, in send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:18:02.799 35679 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/rpc/client.py", line 169, in call
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:18:02.799 35679 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_messaging/transport.py", line 97, in _send
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:18:02.799 35679 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 196, in force_reraise
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:18:02.799 35679 ERROR neutron.agent.dhcp.agent   File "/usr/lib/python2.7/site-packages/oslo_utils/excutils.py", line 220, in __exit__
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:18:02.799 35679 ERROR neutron.agent.dhcp.agent     host=self.host)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:18:02.799 35679 ERROR neutron.agent.dhcp.agent     message = self.waiters.get(msg_id, timeout=timeout)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:18:02.799 35679 ERROR neutron.agent.dhcp.agent MessagingTimeout: Timed out waiting for a reply to message ID 7ce062c0bee045ebbb985f2a3bab72cf
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:18:02.799 35679 ERROR neutron.agent.dhcp.agent [req-13fe2054-55ac-4d2f-ac65-a73ab8f5ab2c - - - - -] Unable to sync network state.
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:18:02.799 35679 ERROR neutron.agent.dhcp.agent     result = self._waiter.wait(msg_id, timeout)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:18:02.799 35679 ERROR neutron.agent.dhcp.agent     retry=retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:18:02.799 35679 ERROR neutron.agent.dhcp.agent     retry=self.retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:18:02.799 35679 ERROR neutron.agent.dhcp.agent     return self._original_context.call(ctxt, method, **kwargs)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:18:02.799 35679 ERROR neutron.agent.dhcp.agent     self.force_reraise()
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:18:02.799 35679 ERROR neutron.agent.dhcp.agent     six.reraise(self.type_, self.value, self.tb)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:18:02.799 35679 ERROR neutron.agent.dhcp.agent     timeout=timeout, retry=retry)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:18:02.799 35679 ERROR neutron.agent.dhcp.agent     time.sleep(wait)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:18:02.799 35679 ERROR neutron.agent.dhcp.agent     'to message ID %s' % msg_id)
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:18:02.799 35679 ERROR neutron.agent.dhcp.agent Traceback (most recent call last):
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:18:03.378 37845 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip 'x.x.x.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/dhcp-agent.log:2017-02-09 14:18:03.793 35679 WARNING oslo.service.loopingcall [req-af2ee7ac-711b-4dd7-b54c-ee2b52158945 - - - - -] Function 'neutron.agent.dhcp.agent.DhcpAgentWithStateReport._report_state' run outlasted interval by 4.18 sec
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:18:04.829 37866 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip 'x.x.x.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/l3-agent.log:2017-02-09 14:18:16.674 35682 WARNING neutron.agent.l3.agent [req-d9a93f9b-4bb6-4a83-ae9a-d69401849132 - - - - -] l3-agent cannot contact neutron server to retrieve service plugins enabled. Check connectivity to neutron server. Retrying... Detailed message: Timed out waiting for a reply to message ID 35347d3bb733405dbaacf5bd787c8896.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:18:20.099 37962 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip 'x.x.x.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:18:21.557 37984 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip 'x.x.x.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:18:23.051 38005 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip 'x.x.x.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:18:24.563 38071 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip 'x.x.x.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:18:25.936 38091 ERROR neutron.plugins.ml2.drivers.openvswitch.agent.ovs_neutron_agent [-] Tunneling can't be enabled with invalid local_ip 'x.x.x.2'. IP couldn't be found on this host's interfaces.
/root/openstack/logs/2017-02-09/1428/openvswitch-agent.log:2017-02-09 14:18:34.478 38435 WARNING stevedore.named [req-6c245024-b432-43db-a4ef-6abd39dce606 - - - - -] Could not load neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:18:35.072 36611 WARNING neutron.plugins.ml2.drivers.type_tunnel [req-6c245024-b432-43db-a4ef-6abd39dce606 - - - - -] Endpoint with ip x.x.x.2 already exists
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:18:39.658 36613 WARNING neutron.db.agents_db [req-56239435-2e26-438c-bb21-56f82ece6e82 - - - - -] Agent healthcheck: found 1 dead agents out of 5:
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:19:03.387 36611 ERROR neutron.db.agents_db [req-d59975ca-bd3e-4ccd-81cb-b98f21d63aac - - - - -] Message received from the host: mysql-002.vassilevski.com during the registration of Open vSwitch agent has a timestamp: 2017-02-09T14:13:20.820211. This differs from the current server timestamp: 2017-02-09T14:19:03.387632 by 342.567421 seconds, which is more than the threshold agent downtime: 75.
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:19:03.835 36611 WARNING neutron.plugins.ml2.drivers.type_tunnel [req-d59975ca-bd3e-4ccd-81cb-b98f21d63aac - - - - -] Endpoint with ip x.x.x.3 already exists
/root/openstack/logs/2017-02-09/1428/server.log:2017-02-09 14:20:45.509 36611 WARNING neutron.plugins.ml2.drivers.type_tunnel [req-5631c9e1-3923-45f0-96fd-e868a43505e6 - - - - -] Endpoint with ip x.x.x.3 already exists
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:21:03.937 36229 ERROR oslo.messaging._drivers.impl_rabbit [req-dfc38c2b-b9ee-4f7c-834e-bf90c929b9e3 - - - - -] [a02adfed-9f26-414f-9ad4-8a94824b3105] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: timed out. Trying again in 1 seconds. Client port: 39872
/root/openstack/logs/2017-02-09/1428/nova-conductor.log:2017-02-09 14:21:03.990 36231 ERROR oslo.messaging._drivers.impl_rabbit [req-0eae1c46-bbde-4d38-a964-b96c452e325c - - - - -] [8f934b5f-8160-4026-86de-6fca2a8c8541] AMQP server on mysql-001.vassilevski.com:5672 is unreachable: timed out. Trying again in 1 seconds. Client port: 39874
/root/openstack/logs/2017-02-09/1428/ovsdb-server.log:2017-02-09T13:36:44.485Z|00005|jsonrpc|WARN|tcp:127.0.0.1:46072: receive error: Connection reset by peer
/root/openstack/logs/2017-02-09/1428/ovsdb-server.log:2017-02-09T13:39:05.702Z|00007|jsonrpc|WARN|tcp:127.0.0.1:47236: receive error: Connection reset by peer
